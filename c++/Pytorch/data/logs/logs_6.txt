Training set samples: 6798
Batch size: 32
[Epoch: 1, batch: 42/213] total loss per batch: 1.644
Policy (actual, predicted): 5 2
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.4201e-01, 6.2960e-02, 2.8223e-01, 2.3144e-04, 4.1540e-02, 2.2157e-01,
        2.4946e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.020

[Epoch: 1, batch: 84/213] total loss per batch: 1.540
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0370, 0.0390, 0.1176, 0.2581, 0.1838, 0.0973, 0.2672],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 1, batch: 126/213] total loss per batch: 1.488
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0446, 0.0358, 0.0299, 0.0353, 0.7805, 0.0343, 0.0395],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.224

[Epoch: 1, batch: 168/213] total loss per batch: 1.432
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0150, 0.0268, 0.0024, 0.0127, 0.0061, 0.0115, 0.9254],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 1, batch: 210/213] total loss per batch: 1.422
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.5918, 0.0705, 0.0976, 0.0278, 0.0442, 0.0970, 0.0710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 2, batch: 42/213] total loss per batch: 1.129
Policy (actual, predicted): 5 6
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0147e-01, 2.1964e-02, 2.9028e-01, 1.5405e-04, 1.5115e-02, 2.6716e-01,
        3.0386e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.035

[Epoch: 2, batch: 84/213] total loss per batch: 1.129
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0123, 0.0090, 0.0366, 0.1000, 0.0931, 0.0468, 0.7022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 2, batch: 126/213] total loss per batch: 1.121
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0322, 0.0209, 0.0261, 0.0374, 0.8219, 0.0301, 0.0314],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.277

[Epoch: 2, batch: 168/213] total loss per batch: 1.067
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0108, 0.0273, 0.0016, 0.0059, 0.0048, 0.0069, 0.9428],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.010

[Epoch: 2, batch: 210/213] total loss per batch: 1.051
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8126, 0.0233, 0.0265, 0.0101, 0.0706, 0.0357, 0.0210],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 3, batch: 42/213] total loss per batch: 0.879
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.4059e-02, 8.7116e-03, 1.5362e-01, 6.1228e-05, 9.8220e-03, 6.3093e-01,
        1.5280e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.002

[Epoch: 3, batch: 84/213] total loss per batch: 0.876
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0022, 0.0097, 0.0126, 0.0234, 0.0105, 0.9389],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 3, batch: 126/213] total loss per batch: 0.864
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0384, 0.0250, 0.0241, 0.0504, 0.7959, 0.0274, 0.0387],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.485

[Epoch: 3, batch: 168/213] total loss per batch: 0.837
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0120, 0.0271, 0.0022, 0.0039, 0.0037, 0.0069, 0.9442],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.013

[Epoch: 3, batch: 210/213] total loss per batch: 0.819
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8354, 0.0158, 0.0164, 0.0098, 0.0628, 0.0378, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.031

[Epoch: 4, batch: 42/213] total loss per batch: 0.766
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.3383e-02, 5.3443e-03, 7.9634e-02, 3.7426e-05, 1.3466e-02, 7.7535e-01,
        1.0278e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.020

[Epoch: 4, batch: 84/213] total loss per batch: 0.783
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([1.9859e-03, 6.3450e-04, 6.5859e-03, 2.3256e-02, 8.7690e-03, 1.2006e-02,
        9.4676e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 4, batch: 126/213] total loss per batch: 0.787
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0301, 0.0210, 0.0199, 0.0351, 0.8461, 0.0215, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.522

[Epoch: 4, batch: 168/213] total loss per batch: 0.773
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0069, 0.0236, 0.0013, 0.0029, 0.0026, 0.0050, 0.9576],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.030

[Epoch: 4, batch: 210/213] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8978, 0.0048, 0.0081, 0.0053, 0.0433, 0.0212, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 5, batch: 42/213] total loss per batch: 0.742
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.8662e-02, 6.8044e-03, 4.1878e-02, 3.4003e-05, 1.0136e-02, 8.1961e-01,
        1.0287e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.026

[Epoch: 5, batch: 84/213] total loss per batch: 0.762
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0021, 0.0129, 0.0540, 0.0245, 0.0204, 0.8827],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 5, batch: 126/213] total loss per batch: 0.769
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0254, 0.0175, 0.0200, 0.0311, 0.8522, 0.0228, 0.0310],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.366

[Epoch: 5, batch: 168/213] total loss per batch: 0.755
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0168, 0.0343, 0.0055, 0.0046, 0.0036, 0.0078, 0.9275],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.013

[Epoch: 5, batch: 210/213] total loss per batch: 0.729
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8464, 0.0064, 0.0145, 0.0117, 0.0462, 0.0342, 0.0405],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.031

[Epoch: 6, batch: 42/213] total loss per batch: 0.721
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.1917e-02, 1.7099e-02, 6.0812e-02, 4.9437e-05, 1.3663e-02, 8.2017e-01,
        7.6290e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.018

[Epoch: 6, batch: 84/213] total loss per batch: 0.741
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0013, 0.0052, 0.0401, 0.0128, 0.0117, 0.9253],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 6, batch: 126/213] total loss per batch: 0.753
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0317, 0.0247, 0.0250, 0.0320, 0.8226, 0.0321, 0.0320],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.609

[Epoch: 6, batch: 168/213] total loss per batch: 0.732
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0072, 0.0237, 0.0021, 0.0047, 0.0033, 0.0053, 0.9537],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.022

[Epoch: 6, batch: 210/213] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8286, 0.0059, 0.0114, 0.0090, 0.0605, 0.0294, 0.0552],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 7, batch: 42/213] total loss per batch: 0.704
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.4713e-02, 2.8276e-02, 4.2054e-02, 3.6497e-05, 3.4259e-02, 6.8421e-01,
        1.5645e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.002

[Epoch: 7, batch: 84/213] total loss per batch: 0.724
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0017, 0.0066, 0.0965, 0.0179, 0.0201, 0.8542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 7, batch: 126/213] total loss per batch: 0.738
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0404, 0.0274, 0.0314, 0.0396, 0.7823, 0.0380, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.519

[Epoch: 7, batch: 168/213] total loss per batch: 0.719
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0073, 0.0199, 0.0026, 0.0042, 0.0038, 0.0050, 0.9571],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.020

[Epoch: 7, batch: 210/213] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8264, 0.0044, 0.0045, 0.0099, 0.0716, 0.0347, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 8, batch: 42/213] total loss per batch: 0.690
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.6436e-03, 1.1803e-02, 2.7648e-02, 2.4254e-05, 1.3374e-02, 9.0791e-01,
        3.4594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.020

[Epoch: 8, batch: 84/213] total loss per batch: 0.715
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0042, 0.0014, 0.0055, 0.0570, 0.0132, 0.0097, 0.9088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 8, batch: 126/213] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0289, 0.0237, 0.0281, 0.0322, 0.8141, 0.0374, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.508

[Epoch: 8, batch: 168/213] total loss per batch: 0.711
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0081, 0.0240, 0.0026, 0.0051, 0.0040, 0.0055, 0.9507],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.017

[Epoch: 8, batch: 210/213] total loss per batch: 0.684
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8189, 0.0058, 0.0051, 0.0091, 0.0646, 0.0357, 0.0609],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 9, batch: 42/213] total loss per batch: 0.687
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.3688e-02, 2.9790e-02, 4.1111e-02, 3.3982e-05, 3.2435e-02, 7.6263e-01,
        9.0315e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.005

[Epoch: 9, batch: 84/213] total loss per batch: 0.709
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0018, 0.0014, 0.0032, 0.0609, 0.0068, 0.0081, 0.9178],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 9, batch: 126/213] total loss per batch: 0.721
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0419, 0.0294, 0.0343, 0.0422, 0.7686, 0.0391, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.481

[Epoch: 9, batch: 168/213] total loss per batch: 0.705
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0077, 0.0217, 0.0017, 0.0041, 0.0038, 0.0050, 0.9560],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.019

[Epoch: 9, batch: 210/213] total loss per batch: 0.679
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8627, 0.0032, 0.0047, 0.0078, 0.0484, 0.0272, 0.0462],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 10, batch: 42/213] total loss per batch: 0.683
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.2564e-02, 2.7331e-02, 2.2675e-02, 2.4651e-05, 3.0419e-02, 8.2808e-01,
        7.8903e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.015

[Epoch: 10, batch: 84/213] total loss per batch: 0.705
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0012, 0.0062, 0.0515, 0.0141, 0.0106, 0.9128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 10, batch: 126/213] total loss per batch: 0.717
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0447, 0.0308, 0.0412, 0.0491, 0.7485, 0.0441, 0.0416],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.463

[Epoch: 10, batch: 168/213] total loss per batch: 0.705
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0055, 0.0193, 0.0022, 0.0047, 0.0041, 0.0039, 0.9603],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.007

[Epoch: 10, batch: 210/213] total loss per batch: 0.677
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8086, 0.0060, 0.0055, 0.0136, 0.0666, 0.0409, 0.0588],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 11, batch: 42/213] total loss per batch: 0.680
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.8189e-02, 3.9969e-02, 3.0685e-02, 3.4180e-05, 4.9656e-02, 7.6855e-01,
        9.2920e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.006

[Epoch: 11, batch: 84/213] total loss per batch: 0.703
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0021, 0.0010, 0.0032, 0.0847, 0.0076, 0.0103, 0.8910],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 11, batch: 126/213] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0593, 0.0397, 0.0496, 0.0528, 0.6836, 0.0595, 0.0555],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.533

[Epoch: 11, batch: 168/213] total loss per batch: 0.702
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0049, 0.0143, 0.0016, 0.0046, 0.0029, 0.0047, 0.9669],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.002

[Epoch: 11, batch: 210/213] total loss per batch: 0.673
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8174, 0.0032, 0.0053, 0.0115, 0.0668, 0.0354, 0.0604],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 12, batch: 42/213] total loss per batch: 0.678
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.2743e-03, 2.5088e-02, 2.0254e-02, 1.8956e-05, 3.4420e-02, 8.5961e-01,
        5.1331e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.007

[Epoch: 12, batch: 84/213] total loss per batch: 0.700
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0016, 0.0040, 0.0832, 0.0082, 0.0077, 0.8928],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 12, batch: 126/213] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0290, 0.0339, 0.0426, 0.0443, 0.7713, 0.0404, 0.0385],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.460

[Epoch: 12, batch: 168/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0057, 0.0184, 0.0026, 0.0051, 0.0030, 0.0038, 0.9614],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.000

[Epoch: 12, batch: 210/213] total loss per batch: 0.673
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8237, 0.0023, 0.0054, 0.0115, 0.0641, 0.0398, 0.0531],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 13, batch: 42/213] total loss per batch: 0.676
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.7026e-02, 6.4716e-02, 1.9087e-02, 2.2205e-05, 4.5429e-02, 7.5031e-01,
        1.0342e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.001

[Epoch: 13, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0017, 0.0036, 0.0411, 0.0055, 0.0092, 0.9361],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.029

[Epoch: 13, batch: 126/213] total loss per batch: 0.712
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0303, 0.0393, 0.0425, 0.0477, 0.7527, 0.0499, 0.0376],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.296

[Epoch: 13, batch: 168/213] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0040, 0.0133, 0.0018, 0.0040, 0.0025, 0.0040, 0.9703],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 0.004

[Epoch: 13, batch: 210/213] total loss per batch: 0.673
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8458, 0.0032, 0.0048, 0.0115, 0.0458, 0.0391, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 14, batch: 42/213] total loss per batch: 0.675
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.3150e-03, 2.5388e-02, 1.7940e-02, 2.5092e-05, 4.1700e-02, 8.5125e-01,
        5.7384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.013

[Epoch: 14, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0015, 0.0030, 0.1063, 0.0050, 0.0088, 0.8727],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 14, batch: 126/213] total loss per batch: 0.710
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0695, 0.0587, 0.0915, 0.0647, 0.5796, 0.0683, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.809

[Epoch: 14, batch: 168/213] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0048, 0.0143, 0.0016, 0.0035, 0.0026, 0.0040, 0.9692],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.000

[Epoch: 14, batch: 210/213] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7524, 0.0073, 0.0041, 0.0208, 0.0693, 0.0554, 0.0907],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 15, batch: 42/213] total loss per batch: 0.674
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.4791e-02, 6.4386e-02, 2.1311e-02, 3.0644e-05, 6.0281e-02, 7.4718e-01,
        8.2022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.007

[Epoch: 15, batch: 84/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0024, 0.0055, 0.0654, 0.0109, 0.0097, 0.9026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 15, batch: 126/213] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0379, 0.0403, 0.0362, 0.0418, 0.7609, 0.0414, 0.0415],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.624

[Epoch: 15, batch: 168/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0063, 0.0168, 0.0034, 0.0047, 0.0037, 0.0039, 0.9610],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 15, batch: 210/213] total loss per batch: 0.672
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8326, 0.0024, 0.0039, 0.0109, 0.0501, 0.0377, 0.0624],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 16, batch: 42/213] total loss per batch: 0.674
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.0018e-03, 4.6341e-02, 1.2609e-02, 3.3545e-05, 3.5346e-02, 8.3536e-01,
        6.3304e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.007

[Epoch: 16, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0023, 0.0030, 0.0034, 0.0296, 0.0068, 0.0073, 0.9477],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 16, batch: 126/213] total loss per batch: 0.710
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0408, 0.0387, 0.0500, 0.0514, 0.7458, 0.0337, 0.0396],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.491

[Epoch: 16, batch: 168/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0044, 0.0120, 0.0017, 0.0041, 0.0027, 0.0048, 0.9704],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.002

[Epoch: 16, batch: 210/213] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8071, 0.0041, 0.0032, 0.0116, 0.0666, 0.0365, 0.0709],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 17, batch: 42/213] total loss per batch: 0.674
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.0657e-03, 3.4214e-02, 1.3502e-02, 2.1955e-05, 5.3938e-02, 7.9278e-01,
        9.8481e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.010

[Epoch: 17, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0015, 0.0019, 0.1392, 0.0130, 0.0110, 0.8300],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 17, batch: 126/213] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0486, 0.0408, 0.0511, 0.0509, 0.7031, 0.0606, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.419

[Epoch: 17, batch: 168/213] total loss per batch: 0.701
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0120, 0.0018, 0.0035, 0.0026, 0.0041, 0.9719],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 17, batch: 210/213] total loss per batch: 0.670
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8027, 0.0039, 0.0039, 0.0244, 0.0619, 0.0466, 0.0567],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 18, batch: 42/213] total loss per batch: 0.675
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0293e-02, 3.7917e-02, 1.0341e-02, 3.9125e-05, 3.1914e-02, 8.6230e-01,
        4.7192e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.011

[Epoch: 18, batch: 84/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0033, 0.0029, 0.0357, 0.0029, 0.0058, 0.9459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.004

[Epoch: 18, batch: 126/213] total loss per batch: 0.708
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0384, 0.0411, 0.0482, 0.0482, 0.7361, 0.0419, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.505

[Epoch: 18, batch: 168/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0138, 0.0029, 0.0041, 0.0023, 0.0034, 0.9701],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 18, batch: 210/213] total loss per batch: 0.670
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8119, 0.0030, 0.0055, 0.0144, 0.0631, 0.0377, 0.0644],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 19, batch: 42/213] total loss per batch: 0.673
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.9341e-03, 5.6787e-02, 9.6953e-03, 3.5445e-05, 5.8732e-02, 7.4189e-01,
        1.2393e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.012

[Epoch: 19, batch: 84/213] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0028, 0.0036, 0.1006, 0.0132, 0.0126, 0.8637],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.007

[Epoch: 19, batch: 126/213] total loss per batch: 0.708
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0314, 0.0304, 0.0445, 0.0526, 0.7383, 0.0463, 0.0565],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.438

[Epoch: 19, batch: 168/213] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0079, 0.0021, 0.0038, 0.0022, 0.0032, 0.9778],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 19, batch: 210/213] total loss per batch: 0.667
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8300, 0.0069, 0.0042, 0.0120, 0.0506, 0.0344, 0.0618],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 20, batch: 42/213] total loss per batch: 0.672
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.9797e-03, 2.5838e-02, 1.5624e-02, 2.8216e-05, 3.4718e-02, 8.8373e-01,
        3.2085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.000

[Epoch: 20, batch: 84/213] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0032, 0.0018, 0.0037, 0.0374, 0.0083, 0.0099, 0.9357],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 20, batch: 126/213] total loss per batch: 0.708
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0593, 0.0494, 0.0728, 0.0614, 0.6466, 0.0519, 0.0587],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.651

[Epoch: 20, batch: 168/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0089, 0.0016, 0.0045, 0.0029, 0.0056, 0.9734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 20, batch: 210/213] total loss per batch: 0.666
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8156, 0.0043, 0.0050, 0.0194, 0.0506, 0.0324, 0.0728],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 21, batch: 42/213] total loss per batch: 0.670
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.4743e-02, 6.4458e-02, 8.2074e-03, 2.8769e-05, 5.6401e-02, 7.4873e-01,
        1.0743e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.002

[Epoch: 21, batch: 84/213] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0027, 0.0023, 0.0016, 0.0771, 0.0048, 0.0041, 0.9075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 21, batch: 126/213] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0255, 0.0295, 0.0281, 0.0380, 0.8086, 0.0428, 0.0275],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.190

[Epoch: 21, batch: 168/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0069, 0.0019, 0.0044, 0.0035, 0.0035, 0.9767],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 21, batch: 210/213] total loss per batch: 0.666
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8004, 0.0082, 0.0037, 0.0134, 0.0582, 0.0424, 0.0737],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 22, batch: 42/213] total loss per batch: 0.670
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.7249e-02, 4.8726e-02, 1.3124e-02, 4.2978e-05, 6.9680e-02, 7.8827e-01,
        5.2912e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.022

[Epoch: 22, batch: 84/213] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0031, 0.0029, 0.0745, 0.0052, 0.0082, 0.9033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 22, batch: 126/213] total loss per batch: 0.706
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0591, 0.0436, 0.0472, 0.0617, 0.6852, 0.0515, 0.0518],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.559

[Epoch: 22, batch: 168/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0064, 0.0029, 0.0027, 0.0028, 0.0038, 0.9781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.040

[Epoch: 22, batch: 210/213] total loss per batch: 0.665
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8367, 0.0023, 0.0032, 0.0155, 0.0522, 0.0362, 0.0539],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 23, batch: 42/213] total loss per batch: 0.669
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.7029e-02, 4.4006e-02, 1.2269e-02, 2.0108e-05, 3.7741e-02, 8.2729e-01,
        6.1643e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.022

[Epoch: 23, batch: 84/213] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0041, 0.0023, 0.0027, 0.0567, 0.0079, 0.0086, 0.9176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 23, batch: 126/213] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0423, 0.0393, 0.0398, 0.0447, 0.7466, 0.0470, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.391

[Epoch: 23, batch: 168/213] total loss per batch: 0.692
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0051, 0.0081, 0.0022, 0.0033, 0.0032, 0.0035, 0.9746],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 23, batch: 210/213] total loss per batch: 0.667
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8434, 0.0036, 0.0052, 0.0145, 0.0527, 0.0282, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 24, batch: 42/213] total loss per batch: 0.670
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.0955e-03, 3.8229e-02, 8.8503e-03, 4.2528e-05, 3.8587e-02, 7.8265e-01,
        1.2755e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.010

[Epoch: 24, batch: 84/213] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0049, 0.0027, 0.0014, 0.1488, 0.0025, 0.0071, 0.8326],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 24, batch: 126/213] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0586, 0.0584, 0.0609, 0.0685, 0.6254, 0.0621, 0.0661],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.593

[Epoch: 24, batch: 168/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0070, 0.0018, 0.0021, 0.0025, 0.0029, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 24, batch: 210/213] total loss per batch: 0.668
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7903, 0.0037, 0.0051, 0.0229, 0.0683, 0.0406, 0.0691],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 25, batch: 42/213] total loss per batch: 0.672
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.8689e-02, 5.1878e-02, 1.4416e-02, 3.7894e-05, 5.1533e-02, 8.1676e-01,
        3.6683e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.029

[Epoch: 25, batch: 84/213] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0016, 0.0023, 0.0272, 0.0034, 0.0061, 0.9567],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.003

[Epoch: 25, batch: 126/213] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0395, 0.0430, 0.0381, 0.0471, 0.7566, 0.0412, 0.0345],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.366

[Epoch: 25, batch: 168/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0060, 0.0028, 0.0039, 0.0041, 0.0054, 0.9734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 25, batch: 210/213] total loss per batch: 0.668
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7852, 0.0044, 0.0047, 0.0160, 0.0674, 0.0434, 0.0788],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.002

[Epoch: 26, batch: 42/213] total loss per batch: 0.671
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0840e-02, 4.1727e-02, 7.1862e-03, 4.2543e-05, 4.2183e-02, 7.8048e-01,
        1.1754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.001

[Epoch: 26, batch: 84/213] total loss per batch: 0.694
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0055, 0.0043, 0.0029, 0.0778, 0.0089, 0.0143, 0.8863],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 26, batch: 126/213] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0619, 0.0440, 0.0728, 0.0810, 0.5979, 0.0619, 0.0805],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.662

[Epoch: 26, batch: 168/213] total loss per batch: 0.702
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0047, 0.0063, 0.0038, 0.0026, 0.0056, 0.0053, 0.9717],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 26, batch: 210/213] total loss per batch: 0.679
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7156, 0.0053, 0.0058, 0.0267, 0.1039, 0.0444, 0.0983],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 27, batch: 42/213] total loss per batch: 0.684
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1468e-03, 5.1466e-02, 1.2857e-02, 2.4518e-05, 4.9825e-02, 8.2066e-01,
        6.0021e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.007

[Epoch: 27, batch: 84/213] total loss per batch: 0.708
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0015, 0.0026, 0.0041, 0.0416, 0.0033, 0.0042, 0.9428],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.028

[Epoch: 27, batch: 126/213] total loss per batch: 0.720
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0390, 0.0295, 0.0210, 0.0377, 0.8225, 0.0317, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.235

[Epoch: 27, batch: 168/213] total loss per batch: 0.707
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0041, 0.0057, 0.0039, 0.0038, 0.0026, 0.0019, 0.9780],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.031

[Epoch: 27, batch: 210/213] total loss per batch: 0.680
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7674, 0.0024, 0.0034, 0.0107, 0.0617, 0.0874, 0.0670],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 28, batch: 42/213] total loss per batch: 0.688
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.7965e-03, 7.9517e-02, 1.8495e-02, 3.3052e-05, 9.1364e-02, 7.2998e-01,
        7.1811e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.017

[Epoch: 28, batch: 84/213] total loss per batch: 0.711
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0022, 0.0027, 0.1099, 0.0146, 0.0058, 0.8610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.048

[Epoch: 28, batch: 126/213] total loss per batch: 0.719
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0188, 0.0190, 0.0267, 0.0272, 0.8567, 0.0271, 0.0245],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.378

[Epoch: 28, batch: 168/213] total loss per batch: 0.704
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0057, 0.0050, 0.0042, 0.0042, 0.0055, 0.0065, 0.9689],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.052

[Epoch: 28, batch: 210/213] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.9017, 0.0036, 0.0039, 0.0071, 0.0357, 0.0188, 0.0291],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 29, batch: 42/213] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.2743e-03, 4.1852e-03, 2.1022e-02, 7.5162e-05, 2.0677e-02, 8.2054e-01,
        1.2823e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 -0.012

[Epoch: 29, batch: 84/213] total loss per batch: 0.720
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0013, 0.0033, 0.0034, 0.0562, 0.0046, 0.0074, 0.9237],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 29, batch: 126/213] total loss per batch: 0.722
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0632, 0.0534, 0.0558, 0.0541, 0.6609, 0.0472, 0.0655],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.671

[Epoch: 29, batch: 168/213] total loss per batch: 0.705
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0014, 0.0031, 0.0026, 0.0023, 0.0019, 0.0036, 0.9851],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.035

[Epoch: 29, batch: 210/213] total loss per batch: 0.681
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8128, 0.0045, 0.0049, 0.0167, 0.0696, 0.0248, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 30, batch: 42/213] total loss per batch: 0.686
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.8855e-02, 4.5654e-02, 1.8740e-02, 5.0507e-05, 5.2786e-02, 7.7879e-01,
        7.5121e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.014

[Epoch: 30, batch: 84/213] total loss per batch: 0.710
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0027, 0.0035, 0.0019, 0.0601, 0.0076, 0.0099, 0.9143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.028

[Epoch: 30, batch: 126/213] total loss per batch: 0.717
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0504, 0.0407, 0.0453, 0.0518, 0.7213, 0.0401, 0.0504],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.403

[Epoch: 30, batch: 168/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0035, 0.0017, 0.0022, 0.0027, 0.0035, 0.9838],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.031

[Epoch: 30, batch: 210/213] total loss per batch: 0.672
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8199, 0.0037, 0.0054, 0.0127, 0.0530, 0.0501, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 31, batch: 42/213] total loss per batch: 0.675
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.7097e-02, 3.0505e-02, 7.9728e-03, 3.8530e-05, 5.3424e-02, 8.2411e-01,
        6.6856e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.006

[Epoch: 31, batch: 84/213] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0033, 0.0025, 0.0926, 0.0058, 0.0034, 0.8897],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.031

[Epoch: 31, batch: 126/213] total loss per batch: 0.722
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0758, 0.0460, 0.0741, 0.0505, 0.6526, 0.0427, 0.0584],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.642

[Epoch: 31, batch: 168/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0049, 0.0030, 0.0027, 0.0041, 0.0055, 0.9757],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.043

[Epoch: 31, batch: 210/213] total loss per batch: 0.682
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8336, 0.0034, 0.0054, 0.0187, 0.0556, 0.0343, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 32, batch: 42/213] total loss per batch: 0.689
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.4436e-02, 4.4764e-02, 7.6102e-03, 4.3412e-05, 7.6143e-02, 8.2806e-01,
        2.8940e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 32, batch: 84/213] total loss per batch: 0.710
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0031, 0.0037, 0.0841, 0.0048, 0.0078, 0.8931],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 32, batch: 126/213] total loss per batch: 0.717
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0407, 0.0358, 0.0639, 0.0510, 0.7235, 0.0391, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.637

[Epoch: 32, batch: 168/213] total loss per batch: 0.703
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0065, 0.0043, 0.0041, 0.0026, 0.0058, 0.9739],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 32, batch: 210/213] total loss per batch: 0.678
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7540, 0.0028, 0.0054, 0.0242, 0.0848, 0.0346, 0.0942],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 33, batch: 42/213] total loss per batch: 0.677
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([3.0141e-02, 3.0118e-02, 1.0247e-02, 3.3627e-05, 3.7272e-02, 7.5831e-01,
        1.3387e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 33, batch: 84/213] total loss per batch: 0.701
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0027, 0.0022, 0.0022, 0.0990, 0.0051, 0.0083, 0.8804],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 33, batch: 126/213] total loss per batch: 0.718
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0657, 0.0454, 0.0780, 0.0561, 0.6523, 0.0405, 0.0620],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.464

[Epoch: 33, batch: 168/213] total loss per batch: 0.721
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0047, 0.0011, 0.0023, 0.0017, 0.0031, 0.9843],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.044

[Epoch: 33, batch: 210/213] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8309, 0.0064, 0.0072, 0.0226, 0.0426, 0.0387, 0.0516],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 34, batch: 42/213] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.1993e-02, 6.4428e-02, 8.4719e-03, 3.6985e-05, 7.4383e-02, 7.6880e-01,
        6.1892e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.024

[Epoch: 34, batch: 84/213] total loss per batch: 0.709
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0049, 0.0016, 0.0044, 0.0466, 0.0097, 0.0039, 0.9290],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.035

[Epoch: 34, batch: 126/213] total loss per batch: 0.720
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0487, 0.0319, 0.0309, 0.0513, 0.7530, 0.0426, 0.0417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.300

[Epoch: 34, batch: 168/213] total loss per batch: 0.711
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0051, 0.0044, 0.0019, 0.0020, 0.0039, 0.9793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 34, batch: 210/213] total loss per batch: 0.685
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8148, 0.0037, 0.0064, 0.0271, 0.0737, 0.0395, 0.0348],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 35, batch: 42/213] total loss per batch: 0.679
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.7388e-02, 8.1436e-02, 7.3898e-03, 2.0555e-05, 3.1428e-02, 8.0164e-01,
        6.0700e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 35, batch: 84/213] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0050, 0.0023, 0.0022, 0.1737, 0.0100, 0.0112, 0.7956],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.032

[Epoch: 35, batch: 126/213] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0356, 0.0296, 0.0350, 0.0323, 0.7865, 0.0398, 0.0411],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.382

[Epoch: 35, batch: 168/213] total loss per batch: 0.697
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0059, 0.0025, 0.0019, 0.0017, 0.0045, 0.9810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 35, batch: 210/213] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8695, 0.0027, 0.0034, 0.0150, 0.0435, 0.0289, 0.0371],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 36, batch: 42/213] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.3770e-02, 3.5283e-02, 5.7687e-03, 2.8502e-05, 5.2367e-02, 8.2714e-01,
        6.5637e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.025

[Epoch: 36, batch: 84/213] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0032, 0.0018, 0.0018, 0.0147, 0.0046, 0.0033, 0.9706],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 36, batch: 126/213] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0508, 0.0396, 0.0459, 0.0431, 0.7252, 0.0446, 0.0508],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.478

[Epoch: 36, batch: 168/213] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0062, 0.0032, 0.0026, 0.0020, 0.0044, 0.9789],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 36, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8590, 0.0031, 0.0041, 0.0131, 0.0464, 0.0290, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 37, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0163e-02, 4.1341e-02, 5.8730e-03, 1.9199e-05, 5.8950e-02, 8.1226e-01,
        7.1397e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.022

[Epoch: 37, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0031, 0.0020, 0.0574, 0.0097, 0.0050, 0.9193],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 37, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0511, 0.0409, 0.0534, 0.0522, 0.7032, 0.0464, 0.0529],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.480

[Epoch: 37, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0042, 0.0029, 0.0027, 0.0021, 0.0039, 0.9816],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 37, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8402, 0.0029, 0.0034, 0.0114, 0.0515, 0.0441, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 38, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.2080e-03, 3.8835e-02, 5.8731e-03, 2.0748e-05, 4.4823e-02, 8.1050e-01,
        9.1736e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.026

[Epoch: 38, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0036, 0.0015, 0.0900, 0.0055, 0.0051, 0.8899],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 38, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0508, 0.0429, 0.0485, 0.0490, 0.7083, 0.0498, 0.0507],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 38, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0044, 0.0033, 0.0029, 0.0022, 0.0033, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 38, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8469, 0.0027, 0.0034, 0.0135, 0.0418, 0.0338, 0.0579],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 39, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8768e-03, 4.9304e-02, 4.3678e-03, 1.6167e-05, 4.7019e-02, 8.4128e-01,
        5.2139e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.027

[Epoch: 39, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0028, 0.0019, 0.0629, 0.0059, 0.0053, 0.9174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 39, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0491, 0.0410, 0.0529, 0.0493, 0.7076, 0.0517, 0.0483],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 39, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0039, 0.0034, 0.0029, 0.0021, 0.0034, 0.9814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.038

[Epoch: 39, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8377, 0.0030, 0.0038, 0.0134, 0.0527, 0.0371, 0.0523],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 40, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.4876e-03, 5.4055e-02, 5.0071e-03, 2.0228e-05, 7.4633e-02, 7.4600e-01,
        1.1179e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.029

[Epoch: 40, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0033, 0.0017, 0.0886, 0.0048, 0.0050, 0.8922],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 40, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0499, 0.0449, 0.0482, 0.0501, 0.7088, 0.0494, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 40, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0025, 0.0040, 0.0036, 0.0031, 0.0023, 0.0036, 0.9810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 40, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8084, 0.0028, 0.0032, 0.0159, 0.0546, 0.0396, 0.0755],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 41, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.5596e-03, 2.3597e-02, 3.2915e-03, 1.2021e-05, 2.1481e-02, 9.1867e-01,
        2.8389e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.024

[Epoch: 41, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0025, 0.0019, 0.0577, 0.0051, 0.0040, 0.9251],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 41, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0418, 0.0405, 0.0473, 0.0485, 0.7349, 0.0454, 0.0417],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.482

[Epoch: 41, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0036, 0.0037, 0.0036, 0.0024, 0.0032, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.042

[Epoch: 41, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8244, 0.0035, 0.0040, 0.0179, 0.0548, 0.0358, 0.0596],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 42, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.4094e-03, 6.8308e-02, 4.9364e-03, 1.6002e-05, 6.4437e-02, 7.1936e-01,
        1.3754e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.030

[Epoch: 42, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0035, 0.0020, 0.0751, 0.0034, 0.0046, 0.9077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.027

[Epoch: 42, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0602, 0.0503, 0.0534, 0.0524, 0.6777, 0.0523, 0.0538],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.500

[Epoch: 42, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0043, 0.0039, 0.0036, 0.0024, 0.0042, 0.9785],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 42, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8320, 0.0028, 0.0035, 0.0147, 0.0520, 0.0305, 0.0645],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 43, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.8175e-03, 3.0433e-02, 4.5710e-03, 1.3820e-05, 4.1393e-02, 8.7090e-01,
        4.4871e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.031

[Epoch: 43, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0049, 0.0030, 0.0029, 0.1092, 0.0029, 0.0038, 0.8733],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.024

[Epoch: 43, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0369, 0.0373, 0.0440, 0.0448, 0.7555, 0.0426, 0.0390],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.447

[Epoch: 43, batch: 168/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0037, 0.0029, 0.0029, 0.0019, 0.0027, 0.9833],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 43, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8318, 0.0033, 0.0033, 0.0158, 0.0440, 0.0350, 0.0667],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 44, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1438e-03, 5.4690e-02, 5.0496e-03, 1.5809e-05, 5.3060e-02, 7.8664e-01,
        9.5398e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.031

[Epoch: 44, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0042, 0.0040, 0.0484, 0.0082, 0.0059, 0.9257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 44, batch: 126/213] total loss per batch: 0.697
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0569, 0.0491, 0.0563, 0.0523, 0.6758, 0.0562, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 44, batch: 168/213] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0043, 0.0044, 0.0056, 0.0050, 0.0039, 0.0048, 0.9720],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.040

[Epoch: 44, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8175, 0.0042, 0.0053, 0.0151, 0.0626, 0.0383, 0.0570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 45, batch: 42/213] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.1685e-03, 3.3982e-02, 3.7692e-03, 1.1684e-05, 4.5499e-02, 8.4827e-01,
        6.0295e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 45, batch: 84/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0030, 0.0033, 0.0527, 0.0012, 0.0046, 0.9317],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 45, batch: 126/213] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0405, 0.0444, 0.0399, 0.0375, 0.7448, 0.0492, 0.0438],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.461

[Epoch: 45, batch: 168/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0023, 0.0037, 0.0031, 0.0037, 0.0027, 0.0037, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 45, batch: 210/213] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8224, 0.0052, 0.0037, 0.0216, 0.0499, 0.0329, 0.0643],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 46, batch: 42/213] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.3097e-03, 6.5627e-02, 4.5228e-03, 1.5210e-05, 9.4047e-02, 7.5525e-01,
        7.3226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.028

[Epoch: 46, batch: 84/213] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0040, 0.0049, 0.0042, 0.1193, 0.0020, 0.0083, 0.8573],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 46, batch: 126/213] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0613, 0.0479, 0.0531, 0.0567, 0.6825, 0.0511, 0.0475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.521

[Epoch: 46, batch: 168/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0039, 0.0022, 0.0031, 0.0030, 0.0042, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 46, batch: 210/213] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8167, 0.0027, 0.0026, 0.0143, 0.0450, 0.0379, 0.0808],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 47, batch: 42/213] total loss per batch: 0.666
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0776e-02, 1.4523e-02, 5.9277e-03, 3.4230e-05, 9.0981e-03, 8.2902e-01,
        1.3062e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.021

[Epoch: 47, batch: 84/213] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0047, 0.0053, 0.0418, 0.0035, 0.0092, 0.9323],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 47, batch: 126/213] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0365, 0.0416, 0.0396, 0.0525, 0.7417, 0.0446, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.490

[Epoch: 47, batch: 168/213] total loss per batch: 0.689
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0016, 0.0038, 0.0018, 0.0022, 0.0020, 0.0029, 0.9858],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.038

[Epoch: 47, batch: 210/213] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8463, 0.0034, 0.0041, 0.0143, 0.0602, 0.0272, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 48, batch: 42/213] total loss per batch: 0.668
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([3.1591e-03, 8.5258e-02, 5.0010e-03, 1.8523e-05, 6.5994e-02, 7.8899e-01,
        5.1577e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.024

[Epoch: 48, batch: 84/213] total loss per batch: 0.695
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0027, 0.0022, 0.1877, 0.0088, 0.0077, 0.7883],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.035

[Epoch: 48, batch: 126/213] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0550, 0.0537, 0.0516, 0.0584, 0.6806, 0.0561, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.545

[Epoch: 48, batch: 168/213] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0034, 0.0024, 0.0032, 0.0035, 0.0040, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 48, batch: 210/213] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8612, 0.0121, 0.0101, 0.0261, 0.0192, 0.0254, 0.0460],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 -0.003

[Epoch: 49, batch: 42/213] total loss per batch: 0.683
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.3838e-02, 3.5292e-02, 9.7053e-03, 4.9584e-05, 2.3611e-02, 8.8515e-01,
        3.2359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.021

[Epoch: 49, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([4.2968e-03, 3.5626e-03, 2.1755e-03, 1.3946e-02, 9.0935e-04, 2.7499e-03,
        9.7236e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 49, batch: 126/213] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0219, 0.0248, 0.0260, 0.0319, 0.8198, 0.0351, 0.0404],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.248

[Epoch: 49, batch: 168/213] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0048, 0.0040, 0.0020, 0.0027, 0.0020, 0.9812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 49, batch: 210/213] total loss per batch: 0.673
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8222, 0.0064, 0.0064, 0.0116, 0.0794, 0.0362, 0.0379],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.000

[Epoch: 50, batch: 42/213] total loss per batch: 0.678
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0281e-02, 5.9976e-02, 1.4716e-02, 1.0934e-05, 7.8709e-02, 6.4473e-01,
        1.9158e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.020

[Epoch: 50, batch: 84/213] total loss per batch: 0.699
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0049, 0.0048, 0.0040, 0.0973, 0.0035, 0.0057, 0.8800],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 50, batch: 126/213] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0488, 0.0395, 0.0476, 0.0476, 0.6951, 0.0627, 0.0588],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.401

[Epoch: 50, batch: 168/213] total loss per batch: 0.696
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0115, 0.0018, 0.0021, 0.0032, 0.0033, 0.9752],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 50, batch: 210/213] total loss per batch: 0.671
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8244, 0.0042, 0.0030, 0.0308, 0.0574, 0.0250, 0.0552],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 51, batch: 42/213] total loss per batch: 0.672
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.7449e-03, 3.7658e-02, 5.4352e-03, 2.4330e-05, 2.5904e-02, 8.8844e-01,
        3.9798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.037

[Epoch: 51, batch: 84/213] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0023, 0.0022, 0.0026, 0.0425, 0.0049, 0.0040, 0.9414],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 51, batch: 126/213] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0520, 0.0445, 0.0448, 0.0504, 0.6988, 0.0556, 0.0540],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.560

[Epoch: 51, batch: 168/213] total loss per batch: 0.692
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0036, 0.0025, 0.0021, 0.0029, 0.0039, 0.9824],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 51, batch: 210/213] total loss per batch: 0.663
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7842, 0.0030, 0.0051, 0.0164, 0.0655, 0.0468, 0.0791],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 52, batch: 42/213] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([3.6834e-03, 5.4963e-02, 3.9645e-03, 2.6073e-05, 4.6117e-02, 8.2088e-01,
        7.0368e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.030

[Epoch: 52, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0031, 0.0021, 0.0717, 0.0020, 0.0018, 0.9164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 52, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0399, 0.0420, 0.0493, 0.0491, 0.7156, 0.0565, 0.0477],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.381

[Epoch: 52, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0040, 0.0031, 0.0029, 0.0035, 0.0031, 0.9795],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 52, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8322, 0.0033, 0.0041, 0.0160, 0.0438, 0.0396, 0.0611],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.003

[Epoch: 53, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.4702e-03, 3.8997e-02, 3.7629e-03, 1.7377e-05, 5.6305e-02, 8.2507e-01,
        7.0382e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.040

[Epoch: 53, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0028, 0.0023, 0.0778, 0.0035, 0.0033, 0.9072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 53, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0475, 0.0455, 0.0515, 0.0548, 0.7024, 0.0552, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.498

[Epoch: 53, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0038, 0.0031, 0.0030, 0.0027, 0.0035, 0.9805],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 53, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8371, 0.0029, 0.0032, 0.0161, 0.0569, 0.0328, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 54, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.2393e-03, 5.2005e-02, 4.0461e-03, 1.8908e-05, 4.8933e-02, 8.1042e-01,
        7.9338e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 54, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0035, 0.0026, 0.0793, 0.0034, 0.0035, 0.9042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 54, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0445, 0.0452, 0.0537, 0.0500, 0.7041, 0.0555, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.494

[Epoch: 54, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0036, 0.0032, 0.0029, 0.0032, 0.0031, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 54, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8330, 0.0028, 0.0032, 0.0163, 0.0481, 0.0320, 0.0645],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 55, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.0810e-03, 4.4962e-02, 3.7971e-03, 1.4811e-05, 4.7457e-02, 8.2532e-01,
        7.3370e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 55, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0028, 0.0024, 0.0735, 0.0044, 0.0029, 0.9104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 55, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0461, 0.0439, 0.0489, 0.0477, 0.7133, 0.0524, 0.0476],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 55, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0042, 0.0030, 0.0033, 0.0028, 0.0027, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 55, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8248, 0.0035, 0.0035, 0.0176, 0.0544, 0.0409, 0.0553],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 56, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.7692e-03, 4.9647e-02, 4.0335e-03, 1.5492e-05, 5.9912e-02, 8.0291e-01,
        7.6712e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 56, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0036, 0.0030, 0.0783, 0.0033, 0.0045, 0.9037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 56, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0467, 0.0470, 0.0510, 0.0487, 0.7059, 0.0532, 0.0476],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.469

[Epoch: 56, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0037, 0.0030, 0.0030, 0.0032, 0.0028, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 56, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8124, 0.0031, 0.0039, 0.0189, 0.0637, 0.0369, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 57, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.0966e-03, 4.3428e-02, 3.9062e-03, 1.3879e-05, 3.6898e-02, 8.3438e-01,
        7.5281e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 57, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0040, 0.0029, 0.0021, 0.0719, 0.0033, 0.0038, 0.9121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 57, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0417, 0.0419, 0.0470, 0.0487, 0.7318, 0.0481, 0.0408],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.482

[Epoch: 57, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0038, 0.0033, 0.0030, 0.0029, 0.0030, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 57, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8312, 0.0037, 0.0032, 0.0151, 0.0489, 0.0370, 0.0609],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 58, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.4135e-03, 5.1629e-02, 3.4998e-03, 1.3202e-05, 6.5192e-02, 7.9237e-01,
        8.0884e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 58, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0038, 0.0024, 0.0737, 0.0037, 0.0026, 0.9104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 58, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0578, 0.0530, 0.0627, 0.0565, 0.6515, 0.0581, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 58, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0037, 0.0031, 0.0030, 0.0029, 0.0028, 0.9813],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 58, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8392, 0.0031, 0.0030, 0.0179, 0.0496, 0.0342, 0.0530],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 59, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2805e-03, 4.4750e-02, 4.3508e-03, 1.5400e-05, 3.5508e-02, 8.4591e-01,
        6.3183e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 59, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0045, 0.0036, 0.0035, 0.0912, 0.0048, 0.0052, 0.8872],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 59, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0274, 0.0358, 0.0343, 0.0430, 0.7881, 0.0395, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.463

[Epoch: 59, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0036, 0.0036, 0.0038, 0.0031, 0.0028, 0.0030, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 59, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8421, 0.0033, 0.0037, 0.0147, 0.0476, 0.0358, 0.0528],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 60, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.8081e-03, 3.5053e-02, 3.3979e-03, 1.8020e-05, 5.8151e-02, 7.9886e-01,
        9.7716e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 60, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0039, 0.0037, 0.0027, 0.0585, 0.0048, 0.0051, 0.9213],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 60, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0559, 0.0534, 0.0617, 0.0624, 0.6435, 0.0593, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.520

[Epoch: 60, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0046, 0.0035, 0.0044, 0.0021, 0.0032, 0.9791],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 60, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8348, 0.0038, 0.0038, 0.0182, 0.0525, 0.0306, 0.0563],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 61, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.8333e-03, 6.0293e-02, 3.7772e-03, 1.9340e-05, 5.5300e-02, 7.8758e-01,
        8.4197e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 61, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0054, 0.0023, 0.0034, 0.0746, 0.0036, 0.0038, 0.9069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 61, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0382, 0.0338, 0.0415, 0.0406, 0.7725, 0.0392, 0.0342],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.426

[Epoch: 61, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0049, 0.0055, 0.0048, 0.0038, 0.0048, 0.0041, 0.9721],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 61, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8380, 0.0030, 0.0049, 0.0138, 0.0461, 0.0231, 0.0710],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 62, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.2601e-03, 2.9505e-02, 4.2424e-03, 2.2680e-05, 3.9396e-02, 8.6839e-01,
        5.1186e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 62, batch: 84/213] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0042, 0.0031, 0.0036, 0.0921, 0.0034, 0.0027, 0.8908],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.021

[Epoch: 62, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0516, 0.0463, 0.0573, 0.0600, 0.6904, 0.0458, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.587

[Epoch: 62, batch: 168/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0036, 0.0045, 0.0042, 0.0031, 0.0030, 0.0035, 0.9781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 62, batch: 210/213] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8334, 0.0029, 0.0020, 0.0175, 0.0531, 0.0434, 0.0475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 63, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.1784e-03, 7.1899e-02, 3.8081e-03, 1.6964e-05, 5.7386e-02, 7.4975e-01,
        1.0796e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.042

[Epoch: 63, batch: 84/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0058, 0.0025, 0.0049, 0.0652, 0.0043, 0.0030, 0.9143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 63, batch: 126/213] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0469, 0.0529, 0.0417, 0.0463, 0.7121, 0.0530, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.355

[Epoch: 63, batch: 168/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0025, 0.0037, 0.0033, 0.0023, 0.0027, 0.0040, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 63, batch: 210/213] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8138, 0.0062, 0.0045, 0.0233, 0.0532, 0.0334, 0.0657],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 64, batch: 42/213] total loss per batch: 0.674
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.1887e-02, 2.2862e-02, 4.5464e-03, 2.3173e-05, 3.4318e-02, 8.3296e-01,
        8.3406e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.041

[Epoch: 64, batch: 84/213] total loss per batch: 0.692
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0053, 0.0033, 0.0046, 0.0917, 0.0032, 0.0136, 0.8782],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 64, batch: 126/213] total loss per batch: 0.706
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0336, 0.0333, 0.0447, 0.0480, 0.7390, 0.0528, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.398

[Epoch: 64, batch: 168/213] total loss per batch: 0.689
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0044, 0.0033, 0.0031, 0.0030, 0.0028, 0.9802],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 64, batch: 210/213] total loss per batch: 0.662
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8152, 0.0048, 0.0053, 0.0144, 0.0513, 0.0429, 0.0661],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 65, batch: 42/213] total loss per batch: 0.667
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5184e-03, 6.7091e-02, 4.7605e-03, 1.4400e-05, 4.1373e-02, 8.2168e-01,
        5.8566e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 65, batch: 84/213] total loss per batch: 0.692
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0059, 0.0040, 0.0046, 0.0894, 0.0042, 0.0034, 0.8885],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 65, batch: 126/213] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0501, 0.0387, 0.0570, 0.0491, 0.7192, 0.0544, 0.0315],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.456

[Epoch: 65, batch: 168/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0043, 0.0034, 0.0037, 0.0028, 0.0037, 0.0024, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.036

[Epoch: 65, batch: 210/213] total loss per batch: 0.659
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8162, 0.0047, 0.0031, 0.0184, 0.0643, 0.0357, 0.0577],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 66, batch: 42/213] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.7456e-03, 3.7595e-02, 7.6964e-03, 2.0089e-05, 5.6693e-02, 8.1703e-01,
        7.4220e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.032

[Epoch: 66, batch: 84/213] total loss per batch: 0.688
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0078, 0.0027, 0.0057, 0.0774, 0.0105, 0.0026, 0.8932],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 66, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0492, 0.0461, 0.0574, 0.0584, 0.6844, 0.0588, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.516

[Epoch: 66, batch: 168/213] total loss per batch: 0.690
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0024, 0.0033, 0.0032, 0.0025, 0.0024, 0.0020, 0.9843],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.030

[Epoch: 66, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8477, 0.0029, 0.0027, 0.0164, 0.0431, 0.0300, 0.0573],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 67, batch: 42/213] total loss per batch: 0.666
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([2.5425e-03, 5.6449e-02, 5.6194e-03, 1.1787e-05, 4.9809e-02, 8.0684e-01,
        7.8727e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.029

[Epoch: 67, batch: 84/213] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0032, 0.0025, 0.0596, 0.0026, 0.0040, 0.9257],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.001

[Epoch: 67, batch: 126/213] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0294, 0.0317, 0.0475, 0.0367, 0.7817, 0.0389, 0.0342],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.401

[Epoch: 67, batch: 168/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0027, 0.0031, 0.0027, 0.0025, 0.0024, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.039

[Epoch: 67, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8314, 0.0034, 0.0055, 0.0130, 0.0499, 0.0435, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 68, batch: 42/213] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.6683e-03, 3.2137e-02, 5.1950e-03, 2.9537e-05, 4.4167e-02, 8.3335e-01,
        7.7449e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 68, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0038, 0.0038, 0.0673, 0.0049, 0.0046, 0.9122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 68, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0584, 0.0503, 0.0614, 0.0438, 0.6958, 0.0493, 0.0410],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.542

[Epoch: 68, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0025, 0.0038, 0.0038, 0.0031, 0.0036, 0.0041, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.033

[Epoch: 68, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8294, 0.0040, 0.0036, 0.0146, 0.0516, 0.0337, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 69, batch: 42/213] total loss per batch: 0.666
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.6440e-03, 5.6296e-02, 5.3770e-03, 1.9996e-05, 5.1673e-02, 7.8907e-01,
        8.9924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 69, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0029, 0.0025, 0.0991, 0.0036, 0.0031, 0.8844],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 69, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0420, 0.0428, 0.0512, 0.0464, 0.7192, 0.0553, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.403

[Epoch: 69, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0028, 0.0037, 0.0027, 0.0022, 0.0033, 0.9821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 69, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8686, 0.0025, 0.0023, 0.0162, 0.0393, 0.0230, 0.0481],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 70, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([3.5219e-03, 3.5348e-02, 5.5253e-03, 1.7319e-05, 4.4648e-02, 8.5568e-01,
        5.5255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.046

[Epoch: 70, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0055, 0.0030, 0.0044, 0.0548, 0.0055, 0.0059, 0.9209],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 70, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0524, 0.0485, 0.0609, 0.0536, 0.6816, 0.0479, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.456

[Epoch: 70, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0035, 0.0044, 0.0036, 0.0033, 0.0037, 0.9781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.034

[Epoch: 70, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8516, 0.0035, 0.0028, 0.0155, 0.0395, 0.0312, 0.0559],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 71, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.3749e-03, 4.6429e-02, 3.8897e-03, 1.7962e-05, 5.4589e-02, 7.9217e-01,
        9.4529e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 71, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0045, 0.0033, 0.0020, 0.0753, 0.0029, 0.0039, 0.9081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 71, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0496, 0.0462, 0.0476, 0.0472, 0.7173, 0.0476, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.454

[Epoch: 71, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0032, 0.0035, 0.0032, 0.0033, 0.0034, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 71, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8357, 0.0035, 0.0042, 0.0172, 0.0474, 0.0375, 0.0545],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 72, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.6221e-03, 4.5869e-02, 4.7499e-03, 1.7440e-05, 5.4875e-02, 8.1775e-01,
        6.9118e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 72, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0046, 0.0050, 0.0039, 0.0744, 0.0034, 0.0040, 0.9046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 72, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0439, 0.0449, 0.0484, 0.0514, 0.7251, 0.0430, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.485

[Epoch: 72, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0031, 0.0036, 0.0030, 0.0028, 0.0032, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 72, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8182, 0.0027, 0.0026, 0.0146, 0.0503, 0.0405, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 73, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.0346e-03, 3.8956e-02, 3.4400e-03, 1.5821e-05, 4.0795e-02, 8.4061e-01,
        7.0152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 73, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0051, 0.0035, 0.0037, 0.0915, 0.0045, 0.0057, 0.8860],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 73, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0514, 0.0472, 0.0523, 0.0448, 0.6983, 0.0508, 0.0552],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 73, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0036, 0.0034, 0.0039, 0.0037, 0.0045, 0.9776],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 73, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8369, 0.0036, 0.0036, 0.0146, 0.0450, 0.0394, 0.0570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 74, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.3087e-03, 5.7496e-02, 5.0420e-03, 1.8853e-05, 6.6235e-02, 7.7758e-01,
        8.5318e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 74, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0046, 0.0026, 0.0040, 0.0561, 0.0041, 0.0045, 0.9240],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 74, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0429, 0.0459, 0.0478, 0.0484, 0.7298, 0.0478, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.459

[Epoch: 74, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0026, 0.0030, 0.0027, 0.0028, 0.0033, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 74, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8493, 0.0032, 0.0035, 0.0148, 0.0513, 0.0258, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 75, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.4634e-03, 4.2425e-02, 3.7222e-03, 1.8918e-05, 3.9615e-02, 8.4375e-01,
        6.3002e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.061

[Epoch: 75, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0026, 0.0018, 0.0949, 0.0020, 0.0026, 0.8934],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 75, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0527, 0.0479, 0.0502, 0.0489, 0.6971, 0.0452, 0.0580],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.524

[Epoch: 75, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0033, 0.0031, 0.0027, 0.0034, 0.0037, 0.9810],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 75, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8313, 0.0040, 0.0035, 0.0165, 0.0470, 0.0401, 0.0577],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 76, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5373e-03, 3.6485e-02, 5.4253e-03, 2.2469e-05, 4.0020e-02, 8.0569e-01,
        1.0482e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 76, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0050, 0.0049, 0.0033, 0.0685, 0.0015, 0.0066, 0.9100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.001

[Epoch: 76, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0350, 0.0360, 0.0440, 0.0496, 0.7564, 0.0460, 0.0329],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.430

[Epoch: 76, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0036, 0.0024, 0.0036, 0.0030, 0.0039, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 76, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7853, 0.0039, 0.0040, 0.0180, 0.0591, 0.0492, 0.0805],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 77, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1683e-03, 5.4215e-02, 3.5366e-03, 1.8641e-05, 5.4746e-02, 8.2032e-01,
        6.1992e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 77, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0046, 0.0017, 0.0029, 0.0659, 0.0180, 0.0034, 0.9036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.025

[Epoch: 77, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0700, 0.0561, 0.0571, 0.0628, 0.6370, 0.0561, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 77, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0024, 0.0039, 0.0028, 0.0033, 0.0034, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 77, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8471, 0.0031, 0.0027, 0.0163, 0.0469, 0.0358, 0.0482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 78, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.5146e-03, 4.2463e-02, 6.1615e-03, 3.1788e-05, 5.2315e-02, 8.2251e-01,
        6.8008e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 78, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([2.1889e-03, 3.3226e-03, 2.5520e-03, 8.2240e-02, 3.2243e-04, 4.5319e-03,
        9.0484e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.007

[Epoch: 78, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0470, 0.0513, 0.0443, 0.0432, 0.7154, 0.0562, 0.0426],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.464

[Epoch: 78, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0037, 0.0046, 0.0036, 0.0032, 0.0044, 0.9771],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.034

[Epoch: 78, batch: 210/213] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8393, 0.0031, 0.0038, 0.0168, 0.0472, 0.0395, 0.0503],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 79, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2162e-03, 5.2573e-02, 3.1424e-03, 2.2726e-05, 4.0899e-02, 8.1723e-01,
        7.9919e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 79, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0024, 0.0042, 0.0027, 0.0803, 0.0018, 0.0238, 0.8848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.004

[Epoch: 79, batch: 126/213] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0404, 0.0350, 0.0454, 0.0385, 0.7498, 0.0446, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.465

[Epoch: 79, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0023, 0.0035, 0.0031, 0.0034, 0.0031, 0.0034, 0.9812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 79, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7949, 0.0033, 0.0052, 0.0202, 0.0634, 0.0379, 0.0751],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 80, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.7659e-03, 4.4014e-02, 3.8704e-03, 2.6995e-05, 5.2190e-02, 8.0696e-01,
        8.4172e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 80, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0048, 0.0024, 0.0026, 0.0737, 0.0035, 0.0021, 0.9109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 80, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0595, 0.0583, 0.0634, 0.0515, 0.6527, 0.0543, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.557

[Epoch: 80, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0040, 0.0029, 0.0040, 0.0031, 0.0036, 0.0062, 0.9762],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 80, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8516, 0.0037, 0.0042, 0.0141, 0.0393, 0.0284, 0.0587],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.032

[Epoch: 81, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.1437e-03, 4.6877e-02, 6.3766e-03, 4.4303e-05, 7.6827e-02, 7.9184e-01,
        7.0896e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 81, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0027, 0.0030, 0.0031, 0.0845, 0.0048, 0.0038, 0.8981],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 81, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0366, 0.0374, 0.0377, 0.0347, 0.7776, 0.0409, 0.0352],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.388

[Epoch: 81, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0023, 0.0026, 0.0022, 0.0031, 0.0025, 0.0041, 0.9831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.034

[Epoch: 81, batch: 210/213] total loss per batch: 0.657
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8376, 0.0041, 0.0029, 0.0181, 0.0535, 0.0360, 0.0479],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 82, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.2335e-02, 3.5471e-02, 4.0175e-03, 2.2570e-05, 2.4615e-02, 8.5163e-01,
        7.1907e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 82, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0040, 0.0021, 0.1005, 0.0030, 0.0045, 0.8820],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 82, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0570, 0.0512, 0.0554, 0.0560, 0.6765, 0.0519, 0.0520],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.464

[Epoch: 82, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0031, 0.0046, 0.0031, 0.0032, 0.0043, 0.9775],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 82, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8386, 0.0027, 0.0035, 0.0156, 0.0469, 0.0314, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.004

[Epoch: 83, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.2557e-03, 6.6538e-02, 2.6898e-03, 3.8718e-05, 7.8279e-02, 7.3349e-01,
        1.0971e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.064

[Epoch: 83, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0019, 0.0035, 0.0309, 0.0018, 0.0033, 0.9557],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.001

[Epoch: 83, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0542, 0.0528, 0.0486, 0.0511, 0.6906, 0.0559, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.491

[Epoch: 83, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0025, 0.0030, 0.0021, 0.0034, 0.0024, 0.0036, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 83, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8510, 0.0032, 0.0038, 0.0140, 0.0427, 0.0312, 0.0541],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 84, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.8815e-03, 2.9623e-02, 3.8060e-03, 2.2598e-05, 1.8924e-02, 9.0798e-01,
        3.4766e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.039

[Epoch: 84, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0054, 0.0038, 0.1313, 0.0056, 0.0031, 0.8475],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 84, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0422, 0.0345, 0.0408, 0.0425, 0.7586, 0.0413, 0.0402],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.444

[Epoch: 84, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0047, 0.0029, 0.0029, 0.0028, 0.0025, 0.0041, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 84, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8338, 0.0056, 0.0035, 0.0188, 0.0546, 0.0400, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.033

[Epoch: 85, batch: 42/213] total loss per batch: 0.665
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.4882e-02, 5.1105e-02, 8.9858e-03, 3.5540e-05, 7.2772e-02, 6.9891e-01,
        1.5331e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.065

[Epoch: 85, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0023, 0.0028, 0.0039, 0.0606, 0.0056, 0.0063, 0.9185],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 85, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0429, 0.0483, 0.0520, 0.0543, 0.7023, 0.0512, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.503

[Epoch: 85, batch: 168/213] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0066, 0.0022, 0.0046, 0.0038, 0.0037, 0.9754],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.007

[Epoch: 85, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8238, 0.0031, 0.0040, 0.0205, 0.0520, 0.0273, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 86, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5443e-03, 5.5895e-02, 4.3577e-03, 2.6517e-05, 4.8723e-02, 8.2579e-01,
        5.8662e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 86, batch: 84/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0019, 0.0020, 0.0388, 0.0023, 0.0024, 0.9500],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.009

[Epoch: 86, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0479, 0.0466, 0.0455, 0.0580, 0.7048, 0.0539, 0.0434],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 86, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0060, 0.0061, 0.0045, 0.0047, 0.0043, 0.0041, 0.9703],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.004

[Epoch: 86, batch: 210/213] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8093, 0.0049, 0.0061, 0.0174, 0.0527, 0.0395, 0.0702],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 87, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.1504e-03, 4.1490e-02, 5.2921e-03, 3.1110e-05, 4.1942e-02, 8.3005e-01,
        7.4046e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 87, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0061, 0.0018, 0.1118, 0.0040, 0.0053, 0.8678],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 87, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0528, 0.0465, 0.0446, 0.0470, 0.7128, 0.0505, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.481

[Epoch: 87, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0019, 0.0036, 0.0023, 0.0035, 0.0031, 0.0034, 0.9822],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 87, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8412, 0.0027, 0.0033, 0.0158, 0.0492, 0.0335, 0.0543],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 88, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.4976e-03, 4.7198e-02, 6.7576e-03, 2.5011e-05, 6.2811e-02, 8.0481e-01,
        7.0905e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 88, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0020, 0.0034, 0.0783, 0.0091, 0.0033, 0.9006],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.001

[Epoch: 88, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0504, 0.0462, 0.0455, 0.0463, 0.7107, 0.0524, 0.0484],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 88, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0037, 0.0030, 0.0031, 0.0028, 0.0036, 0.9804],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 88, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8441, 0.0031, 0.0036, 0.0173, 0.0450, 0.0378, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 89, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.1161e-03, 4.7803e-02, 4.2073e-03, 2.4185e-05, 4.0779e-02, 8.2756e-01,
        7.3511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 89, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0024, 0.0023, 0.0699, 0.0021, 0.0075, 0.9125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 89, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0516, 0.0532, 0.0509, 0.0497, 0.6899, 0.0530, 0.0517],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.497

[Epoch: 89, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0037, 0.0031, 0.0032, 0.0026, 0.0040, 0.9802],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 89, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8403, 0.0029, 0.0046, 0.0172, 0.0517, 0.0345, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 90, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.1143e-03, 3.9483e-02, 3.7724e-03, 2.2656e-05, 4.5583e-02, 8.1943e-01,
        8.5592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 90, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0033, 0.0023, 0.0678, 0.0016, 0.0042, 0.9175],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 90, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0405, 0.0437, 0.0450, 0.0507, 0.7308, 0.0464, 0.0429],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.456

[Epoch: 90, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0042, 0.0032, 0.0037, 0.0031, 0.0041, 0.9786],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 90, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8261, 0.0035, 0.0039, 0.0179, 0.0441, 0.0417, 0.0629],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 91, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9288e-03, 5.7214e-02, 3.9385e-03, 2.5538e-05, 5.6949e-02, 7.9595e-01,
        7.8993e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 91, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0034, 0.0027, 0.0826, 0.0030, 0.0017, 0.9037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 91, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0455, 0.0476, 0.0547, 0.0507, 0.7012, 0.0534, 0.0468],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.490

[Epoch: 91, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0041, 0.0033, 0.0034, 0.0029, 0.0041, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 91, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8576, 0.0025, 0.0032, 0.0153, 0.0439, 0.0286, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 92, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2520e-03, 3.8256e-02, 4.7080e-03, 2.4564e-05, 4.1957e-02, 8.4655e-01,
        6.2250e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 92, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0043, 0.0038, 0.0793, 0.0063, 0.0040, 0.8992],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 92, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0431, 0.0442, 0.0506, 0.0465, 0.7178, 0.0489, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.478

[Epoch: 92, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0039, 0.0043, 0.0035, 0.0041, 0.0035, 0.0036, 0.9772],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 92, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8311, 0.0041, 0.0045, 0.0177, 0.0543, 0.0367, 0.0517],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 93, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5469e-03, 6.2918e-02, 3.8118e-03, 2.0816e-05, 5.4977e-02, 7.8912e-01,
        8.1601e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 93, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0059, 0.0042, 0.0046, 0.0907, 0.0032, 0.0054, 0.8859],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 93, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0527, 0.0510, 0.0533, 0.0599, 0.6802, 0.0547, 0.0483],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 93, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0020, 0.0032, 0.0027, 0.0024, 0.0026, 0.0025, 0.9845],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 93, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8049, 0.0033, 0.0035, 0.0183, 0.0545, 0.0416, 0.0739],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 94, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.8490e-03, 3.1694e-02, 4.5385e-03, 2.5389e-05, 5.8715e-02, 8.0781e-01,
        8.9367e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 94, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0045, 0.0037, 0.0033, 0.0701, 0.0026, 0.0045, 0.9114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 94, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0473, 0.0452, 0.0456, 0.0469, 0.7271, 0.0447, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.495

[Epoch: 94, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0037, 0.0037, 0.0039, 0.0040, 0.0041, 0.9764],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 94, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8260, 0.0030, 0.0034, 0.0159, 0.0457, 0.0374, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 95, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.1647e-03, 5.3690e-02, 4.2294e-03, 2.4482e-05, 3.6204e-02, 8.2191e-01,
        7.6775e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.046

[Epoch: 95, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0016, 0.0033, 0.0023, 0.0656, 0.0045, 0.0027, 0.9199],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 95, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0437, 0.0443, 0.0477, 0.0477, 0.7151, 0.0500, 0.0515],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.459

[Epoch: 95, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0023, 0.0041, 0.0027, 0.0034, 0.0029, 0.0035, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 95, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8447, 0.0032, 0.0032, 0.0153, 0.0599, 0.0240, 0.0498],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 96, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.1534e-03, 3.8771e-02, 4.1154e-03, 2.3291e-05, 5.8326e-02, 8.1186e-01,
        7.8748e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 96, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0034, 0.0040, 0.1050, 0.0021, 0.0029, 0.8789],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.019

[Epoch: 96, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0369, 0.0384, 0.0446, 0.0381, 0.7588, 0.0426, 0.0407],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.481

[Epoch: 96, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0050, 0.0046, 0.0035, 0.0043, 0.0033, 0.0042, 0.9751],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 96, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8183, 0.0029, 0.0049, 0.0192, 0.0611, 0.0344, 0.0591],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 97, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.2891e-02, 3.9531e-02, 4.6579e-03, 1.9327e-05, 3.6260e-02, 8.2849e-01,
        7.8150e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 97, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0059, 0.0043, 0.0051, 0.0608, 0.0071, 0.0056, 0.9112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 97, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0510, 0.0553, 0.0573, 0.0537, 0.6800, 0.0503, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.453

[Epoch: 97, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0027, 0.0025, 0.0027, 0.0023, 0.0020, 0.9851],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 97, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8577, 0.0027, 0.0029, 0.0152, 0.0366, 0.0318, 0.0531],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 98, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.5607e-03, 5.6671e-02, 3.0563e-03, 2.2044e-05, 6.0465e-02, 7.9347e-01,
        8.0758e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.035

[Epoch: 98, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0024, 0.0042, 0.0028, 0.0573, 0.0018, 0.0035, 0.9279],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 98, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0457, 0.0492, 0.0545, 0.0578, 0.7014, 0.0469, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.460

[Epoch: 98, batch: 168/213] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0033, 0.0043, 0.0023, 0.0037, 0.0039, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 98, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8254, 0.0031, 0.0049, 0.0168, 0.0437, 0.0404, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 99, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.2962e-03, 4.7449e-02, 4.6769e-03, 2.7653e-05, 5.1529e-02, 8.1085e-01,
        7.7167e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 99, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0033, 0.0024, 0.0743, 0.0039, 0.0025, 0.9103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.004

[Epoch: 99, batch: 126/213] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0409, 0.0455, 0.0440, 0.0474, 0.7195, 0.0531, 0.0495],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 99, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0050, 0.0029, 0.0048, 0.0044, 0.0032, 0.9758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 99, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8021, 0.0028, 0.0034, 0.0176, 0.0581, 0.0361, 0.0799],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 100, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1858e-03, 4.3179e-02, 4.2860e-03, 2.9136e-05, 4.6708e-02, 8.3552e-01,
        6.5092e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 100, batch: 84/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0065, 0.0030, 0.0040, 0.1151, 0.0027, 0.0024, 0.8663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 100, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0476, 0.0483, 0.0464, 0.0467, 0.7159, 0.0497, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.493

[Epoch: 100, batch: 168/213] total loss per batch: 0.684
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0039, 0.0040, 0.0041, 0.0030, 0.0033, 0.9786],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 100, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8502, 0.0040, 0.0029, 0.0140, 0.0426, 0.0352, 0.0510],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 101, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5071e-03, 4.6785e-02, 4.4798e-03, 3.5501e-05, 5.4628e-02, 7.9056e-01,
        9.6010e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 101, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0052, 0.0028, 0.0050, 0.0513, 0.0043, 0.0073, 0.9241],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 101, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0435, 0.0450, 0.0466, 0.0556, 0.7098, 0.0538, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.440

[Epoch: 101, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0025, 0.0030, 0.0036, 0.0023, 0.0035, 0.9818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 101, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8311, 0.0043, 0.0053, 0.0201, 0.0470, 0.0313, 0.0609],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 102, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1520e-03, 4.0962e-02, 4.1460e-03, 2.7626e-05, 4.1543e-02, 8.2898e-01,
        7.9186e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 102, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0025, 0.0038, 0.0786, 0.0034, 0.0042, 0.9039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 102, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0496, 0.0463, 0.0527, 0.0519, 0.7005, 0.0528, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.475

[Epoch: 102, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0029, 0.0034, 0.0033, 0.0026, 0.0032, 0.9816],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 102, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8348, 0.0035, 0.0035, 0.0152, 0.0463, 0.0353, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 103, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5635e-03, 6.7647e-02, 4.2365e-03, 2.7687e-05, 5.1245e-02, 7.9563e-01,
        7.3649e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.043

[Epoch: 103, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0036, 0.0033, 0.0788, 0.0033, 0.0042, 0.9033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 103, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0451, 0.0427, 0.0488, 0.0464, 0.7204, 0.0502, 0.0463],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.475

[Epoch: 103, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0031, 0.0037, 0.0039, 0.0024, 0.0032, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 103, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8393, 0.0031, 0.0031, 0.0170, 0.0507, 0.0282, 0.0588],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 104, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.1669e-03, 3.1226e-02, 3.9588e-03, 2.2797e-05, 4.3892e-02, 8.4144e-01,
        7.3297e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 104, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0029, 0.0037, 0.0757, 0.0039, 0.0043, 0.9057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 104, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0494, 0.0462, 0.0488, 0.0489, 0.7104, 0.0508, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.469

[Epoch: 104, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0034, 0.0038, 0.0036, 0.0033, 0.0031, 0.9796],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 104, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8327, 0.0036, 0.0038, 0.0163, 0.0466, 0.0364, 0.0608],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 105, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.5034e-03, 5.4213e-02, 4.2806e-03, 2.3758e-05, 5.4370e-02, 7.9628e-01,
        8.2325e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 105, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0032, 0.0035, 0.0764, 0.0040, 0.0041, 0.9052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 105, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0492, 0.0483, 0.0499, 0.0501, 0.7048, 0.0507, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.482

[Epoch: 105, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0030, 0.0034, 0.0039, 0.0034, 0.0032, 0.9800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 105, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8371, 0.0033, 0.0034, 0.0170, 0.0466, 0.0340, 0.0587],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 106, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.3109e-03, 4.1239e-02, 3.7277e-03, 1.9885e-05, 5.1714e-02, 8.1982e-01,
        7.7169e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 106, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0032, 0.0030, 0.0744, 0.0025, 0.0035, 0.9100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 106, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0458, 0.0451, 0.0488, 0.0486, 0.7152, 0.0507, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.475

[Epoch: 106, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0038, 0.0039, 0.0034, 0.0038, 0.0032, 0.9788],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 106, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8201, 0.0037, 0.0043, 0.0165, 0.0497, 0.0353, 0.0703],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 107, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8499e-03, 5.1337e-02, 3.8586e-03, 2.3581e-05, 4.6036e-02, 8.1682e-01,
        7.6077e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 107, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0032, 0.0038, 0.0772, 0.0032, 0.0029, 0.9066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 107, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0448, 0.0476, 0.0511, 0.0503, 0.7095, 0.0490, 0.0477],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.480

[Epoch: 107, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0033, 0.0029, 0.0034, 0.0028, 0.0030, 0.9818],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 107, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8445, 0.0033, 0.0036, 0.0162, 0.0469, 0.0331, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 108, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.5266e-03, 4.0189e-02, 4.2114e-03, 1.9688e-05, 5.5697e-02, 8.2197e-01,
        6.9388e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 108, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0034, 0.0036, 0.0839, 0.0050, 0.0046, 0.8959],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 108, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0517, 0.0477, 0.0536, 0.0515, 0.6866, 0.0580, 0.0511],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 108, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0036, 0.0034, 0.0037, 0.0030, 0.0038, 0.0035, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 108, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8455, 0.0031, 0.0032, 0.0149, 0.0491, 0.0310, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 109, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.8787e-03, 4.8053e-02, 3.5750e-03, 2.1935e-05, 4.2955e-02, 8.2280e-01,
        7.7713e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 109, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0026, 0.0040, 0.0581, 0.0029, 0.0028, 0.9267],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 109, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0404, 0.0418, 0.0442, 0.0429, 0.7465, 0.0429, 0.0413],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.468

[Epoch: 109, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0030, 0.0031, 0.0036, 0.0034, 0.0042, 0.9793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 109, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8529, 0.0029, 0.0027, 0.0166, 0.0469, 0.0279, 0.0501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 110, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5157e-03, 4.5836e-02, 4.1945e-03, 2.2616e-05, 6.2485e-02, 7.9729e-01,
        8.2653e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 110, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0034, 0.0022, 0.0856, 0.0022, 0.0028, 0.9005],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 110, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0629, 0.0534, 0.0647, 0.0664, 0.6437, 0.0553, 0.0537],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.451

[Epoch: 110, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0042, 0.0036, 0.0043, 0.0033, 0.0030, 0.0038, 0.9778],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 110, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8570, 0.0031, 0.0034, 0.0124, 0.0410, 0.0258, 0.0573],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 111, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.2392e-03, 4.5279e-02, 3.5959e-03, 2.5961e-05, 3.2435e-02, 8.4452e-01,
        6.8904e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 111, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0052, 0.0039, 0.1081, 0.0039, 0.0045, 0.8712],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.027

[Epoch: 111, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0284, 0.0321, 0.0324, 0.0264, 0.8125, 0.0350, 0.0332],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.490

[Epoch: 111, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0034, 0.0023, 0.0037, 0.0027, 0.0029, 0.9822],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 111, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8341, 0.0032, 0.0030, 0.0172, 0.0521, 0.0409, 0.0497],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 112, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.2786e-03, 4.3848e-02, 3.4715e-03, 1.8528e-05, 5.9750e-02, 8.0288e-01,
        8.1752e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 112, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0052, 0.0031, 0.0074, 0.0624, 0.0056, 0.0055, 0.9109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 112, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0625, 0.0662, 0.0656, 0.0644, 0.6163, 0.0631, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.467

[Epoch: 112, batch: 168/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0045, 0.0031, 0.0047, 0.0035, 0.0049, 0.0032, 0.9761],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 112, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7980, 0.0048, 0.0052, 0.0202, 0.0609, 0.0345, 0.0764],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.044

[Epoch: 113, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.8030e-03, 4.2572e-02, 4.2022e-03, 3.3053e-05, 5.1115e-02, 7.9980e-01,
        9.4473e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 113, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0026, 0.0029, 0.0536, 0.0036, 0.0020, 0.9318],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 113, batch: 126/213] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0408, 0.0429, 0.0420, 0.0432, 0.7410, 0.0468, 0.0433],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.485

[Epoch: 113, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0034, 0.0030, 0.0029, 0.0026, 0.0035, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 113, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8254, 0.0030, 0.0045, 0.0186, 0.0449, 0.0373, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 114, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.1892e-03, 3.6524e-02, 6.0048e-03, 2.1854e-05, 4.4645e-02, 8.2911e-01,
        7.5505e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.044

[Epoch: 114, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0024, 0.0064, 0.0030, 0.0802, 0.0031, 0.0024, 0.9024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 114, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0411, 0.0413, 0.0441, 0.0403, 0.7479, 0.0438, 0.0416],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.520

[Epoch: 114, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0035, 0.0025, 0.0026, 0.0043, 0.0037, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 114, batch: 210/213] total loss per batch: 0.658
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8038, 0.0064, 0.0062, 0.0241, 0.0565, 0.0439, 0.0591],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.032

[Epoch: 115, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.9835e-03, 9.4528e-02, 5.5850e-03, 3.2903e-05, 5.5362e-02, 7.7405e-01,
        6.5463e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.064

[Epoch: 115, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0039, 0.0026, 0.0027, 0.0700, 0.0056, 0.0035, 0.9118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 115, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0425, 0.0413, 0.0419, 0.0455, 0.7271, 0.0489, 0.0528],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.437

[Epoch: 115, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0028, 0.0032, 0.0032, 0.0024, 0.0027, 0.9828],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 115, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8073, 0.0052, 0.0072, 0.0199, 0.0476, 0.0357, 0.0772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 116, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.1244e-03, 3.1866e-02, 4.8667e-03, 2.1910e-05, 4.1206e-02, 7.9939e-01,
        1.1452e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 116, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0032, 0.0022, 0.0776, 0.0021, 0.0040, 0.9075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 116, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0547, 0.0589, 0.0553, 0.0586, 0.6617, 0.0527, 0.0580],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 116, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0026, 0.0042, 0.0027, 0.0028, 0.0035, 0.9809],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 116, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8291, 0.0039, 0.0046, 0.0197, 0.0500, 0.0294, 0.0633],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 117, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.0611e-03, 6.4266e-02, 2.8503e-03, 3.4968e-05, 5.4541e-02, 8.2091e-01,
        5.2339e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 117, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0023, 0.0033, 0.0017, 0.0713, 0.0021, 0.0022, 0.9171],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 117, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0454, 0.0457, 0.0460, 0.0496, 0.7159, 0.0526, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.466

[Epoch: 117, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0045, 0.0035, 0.0035, 0.0030, 0.0039, 0.0038, 0.9778],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 117, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8351, 0.0034, 0.0034, 0.0172, 0.0426, 0.0347, 0.0636],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 118, batch: 42/213] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.3375e-02, 4.1840e-02, 5.3067e-03, 2.0628e-05, 3.6095e-02, 8.3379e-01,
        6.9574e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 118, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0021, 0.0032, 0.0033, 0.0804, 0.0037, 0.0031, 0.9041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.001

[Epoch: 118, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0455, 0.0516, 0.0503, 0.0562, 0.6970, 0.0556, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.471

[Epoch: 118, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0034, 0.0036, 0.0034, 0.0032, 0.0036, 0.9795],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 118, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8489, 0.0025, 0.0026, 0.0138, 0.0460, 0.0292, 0.0570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 119, batch: 42/213] total loss per batch: 0.664
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.0355e-03, 7.7781e-02, 3.2822e-03, 3.7898e-05, 6.1896e-02, 7.2831e-01,
        1.1965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 119, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0032, 0.0020, 0.0760, 0.0053, 0.0033, 0.9077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 119, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0463, 0.0470, 0.0459, 0.0526, 0.7063, 0.0529, 0.0490],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.465

[Epoch: 119, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0027, 0.0035, 0.0031, 0.0041, 0.0027, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.032

[Epoch: 119, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8553, 0.0028, 0.0022, 0.0140, 0.0498, 0.0338, 0.0421],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 120, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1000e-03, 1.9080e-02, 4.6057e-03, 2.6649e-05, 3.8419e-02, 8.8396e-01,
        4.8809e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 120, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0030, 0.0036, 0.0711, 0.0035, 0.0039, 0.9119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 120, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0459, 0.0486, 0.0504, 0.0486, 0.7077, 0.0517, 0.0472],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 120, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0036, 0.0026, 0.0038, 0.0034, 0.0042, 0.0026, 0.9798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 120, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8655, 0.0019, 0.0029, 0.0128, 0.0397, 0.0287, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 121, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.9383e-03, 3.9769e-02, 3.4537e-03, 3.6350e-05, 6.1322e-02, 8.1197e-01,
        7.4511e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 121, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0046, 0.0040, 0.0029, 0.0818, 0.0050, 0.0041, 0.8976],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 121, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0446, 0.0448, 0.0481, 0.0535, 0.7151, 0.0490, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 121, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0028, 0.0035, 0.0034, 0.0046, 0.0029, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.028

[Epoch: 121, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8352, 0.0021, 0.0036, 0.0176, 0.0436, 0.0319, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 122, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.7760e-03, 4.5526e-02, 3.9534e-03, 3.0306e-05, 5.3869e-02, 7.9063e-01,
        9.9217e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 122, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0034, 0.0029, 0.0691, 0.0039, 0.0036, 0.9134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 122, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0427, 0.0469, 0.0495, 0.0489, 0.7150, 0.0504, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 122, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0030, 0.0035, 0.0032, 0.0035, 0.0033, 0.9802],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 122, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8346, 0.0024, 0.0033, 0.0165, 0.0456, 0.0372, 0.0604],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.010

[Epoch: 123, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.6877e-03, 4.3305e-02, 4.0116e-03, 2.4732e-05, 4.5801e-02, 8.3421e-01,
        6.5956e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 123, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0032, 0.0035, 0.0825, 0.0031, 0.0033, 0.9016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 123, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0475, 0.0473, 0.0500, 0.0479, 0.7131, 0.0490, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.491

[Epoch: 123, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0032, 0.0035, 0.0037, 0.0035, 0.0033, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 123, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8400, 0.0025, 0.0041, 0.0158, 0.0453, 0.0337, 0.0586],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 124, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9228e-03, 4.7613e-02, 4.0103e-03, 2.3435e-05, 4.8071e-02, 8.1497e-01,
        7.8385e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 124, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0032, 0.0034, 0.0798, 0.0036, 0.0037, 0.9031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 124, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0465, 0.0462, 0.0508, 0.0501, 0.7108, 0.0496, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 124, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0033, 0.0038, 0.0033, 0.0032, 0.0030, 0.9801],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 124, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8271, 0.0030, 0.0044, 0.0185, 0.0460, 0.0376, 0.0634],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 125, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.1069e-03, 4.7500e-02, 3.8473e-03, 2.2925e-05, 4.8565e-02, 8.1459e-01,
        7.8364e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 125, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0040, 0.0039, 0.0034, 0.0753, 0.0034, 0.0038, 0.9061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 125, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0453, 0.0452, 0.0489, 0.0490, 0.7177, 0.0484, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 125, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0034, 0.0038, 0.0034, 0.0035, 0.0032, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 125, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8157, 0.0028, 0.0037, 0.0186, 0.0528, 0.0405, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.013

[Epoch: 126, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5558e-03, 5.3535e-02, 4.2969e-03, 2.7666e-05, 5.7056e-02, 7.9850e-01,
        7.9024e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 126, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0035, 0.0033, 0.0765, 0.0046, 0.0035, 0.9049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 126, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0482, 0.0488, 0.0524, 0.0509, 0.7033, 0.0493, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 126, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0035, 0.0039, 0.0039, 0.0037, 0.0036, 0.9781],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 126, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8361, 0.0032, 0.0042, 0.0170, 0.0472, 0.0366, 0.0557],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 127, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.5940e-03, 3.8916e-02, 3.9399e-03, 2.5371e-05, 4.4158e-02, 8.4480e-01,
        6.2569e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 127, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0030, 0.0029, 0.0787, 0.0026, 0.0028, 0.9075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 127, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0441, 0.0438, 0.0482, 0.0489, 0.7193, 0.0503, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.491

[Epoch: 127, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0030, 0.0039, 0.0037, 0.0045, 0.0034, 0.9784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 127, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8517, 0.0024, 0.0027, 0.0147, 0.0481, 0.0268, 0.0536],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 128, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.9068e-03, 5.5272e-02, 4.0760e-03, 2.9931e-05, 5.7181e-02, 7.7986e-01,
        9.5675e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 128, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0031, 0.0030, 0.0732, 0.0023, 0.0035, 0.9124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.000

[Epoch: 128, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0457, 0.0459, 0.0481, 0.0503, 0.7058, 0.0512, 0.0531],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.456

[Epoch: 128, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0040, 0.0029, 0.0026, 0.0035, 0.0032, 0.9801],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.027

[Epoch: 128, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8469, 0.0031, 0.0041, 0.0165, 0.0394, 0.0292, 0.0607],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 129, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5652e-03, 4.8450e-02, 4.6230e-03, 3.6373e-05, 4.6525e-02, 8.2933e-01,
        6.3475e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 129, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0040, 0.0032, 0.0046, 0.0720, 0.0027, 0.0045, 0.9089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 129, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0491, 0.0509, 0.0511, 0.0560, 0.7034, 0.0443, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.499

[Epoch: 129, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0028, 0.0033, 0.0032, 0.0035, 0.0033, 0.9812],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 129, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8194, 0.0035, 0.0048, 0.0168, 0.0480, 0.0392, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 130, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.0925e-03, 3.2252e-02, 3.5288e-03, 3.1838e-05, 3.6561e-02, 8.5165e-01,
        7.0881e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 130, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0023, 0.0025, 0.0952, 0.0044, 0.0021, 0.8911],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.005

[Epoch: 130, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0461, 0.0429, 0.0428, 0.0541, 0.7138, 0.0561, 0.0442],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.474

[Epoch: 130, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0039, 0.0034, 0.0035, 0.0039, 0.0040, 0.0036, 0.9778],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 130, batch: 210/213] total loss per batch: 0.655
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8481, 0.0034, 0.0041, 0.0150, 0.0546, 0.0317, 0.0431],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 131, batch: 42/213] total loss per batch: 0.661
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.1227e-02, 5.6545e-02, 3.8141e-03, 3.7267e-05, 6.2225e-02, 7.8672e-01,
        7.9433e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 131, batch: 84/213] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0030, 0.0035, 0.0666, 0.0031, 0.0038, 0.9173],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.006

[Epoch: 131, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0495, 0.0479, 0.0622, 0.0530, 0.6919, 0.0520, 0.0435],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.447

[Epoch: 131, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0036, 0.0042, 0.0035, 0.0036, 0.0030, 0.9787],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 131, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8430, 0.0026, 0.0033, 0.0178, 0.0410, 0.0308, 0.0615],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 132, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.0339e-03, 4.0405e-02, 3.9854e-03, 4.2008e-05, 4.3077e-02, 8.1397e-01,
        9.2487e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.046

[Epoch: 132, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0028, 0.0044, 0.0841, 0.0018, 0.0032, 0.9002],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 132, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0457, 0.0465, 0.0471, 0.0488, 0.7198, 0.0471, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 132, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0032, 0.0029, 0.0025, 0.0024, 0.0026, 0.9831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 132, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7968, 0.0027, 0.0032, 0.0209, 0.0475, 0.0440, 0.0848],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 133, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.7855e-03, 5.0889e-02, 5.4733e-03, 4.2042e-05, 5.4605e-02, 8.2167e-01,
        6.2532e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 133, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0039, 0.0046, 0.0038, 0.0803, 0.0042, 0.0061, 0.8971],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 133, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0507, 0.0457, 0.0501, 0.0477, 0.7105, 0.0478, 0.0476],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.490

[Epoch: 133, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0039, 0.0041, 0.0038, 0.0037, 0.0035, 0.9782],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 133, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8369, 0.0036, 0.0038, 0.0159, 0.0501, 0.0376, 0.0521],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 134, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.7024e-03, 4.0908e-02, 3.7811e-03, 3.6005e-05, 6.4677e-02, 8.0806e-01,
        7.4838e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 134, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0031, 0.0040, 0.0675, 0.0036, 0.0032, 0.9142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 134, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0428, 0.0415, 0.0442, 0.0485, 0.7321, 0.0448, 0.0461],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 134, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0029, 0.0027, 0.0028, 0.0025, 0.0029, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 134, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8322, 0.0036, 0.0038, 0.0182, 0.0504, 0.0305, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.005

[Epoch: 135, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.7477e-03, 4.9587e-02, 4.1301e-03, 3.6425e-05, 3.4649e-02, 8.0676e-01,
        9.7095e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 135, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0030, 0.0021, 0.0683, 0.0030, 0.0034, 0.9168],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 135, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0541, 0.0530, 0.0450, 0.0476, 0.6997, 0.0481, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.503

[Epoch: 135, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0034, 0.0034, 0.0035, 0.0038, 0.0036, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 135, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8017, 0.0037, 0.0043, 0.0189, 0.0497, 0.0456, 0.0761],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.009

[Epoch: 136, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.7500e-03, 4.1382e-02, 3.5748e-03, 3.3548e-05, 4.9771e-02, 8.3826e-01,
        6.1226e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 136, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0031, 0.0029, 0.0741, 0.0028, 0.0036, 0.9108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 136, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0427, 0.0411, 0.0474, 0.0495, 0.7257, 0.0488, 0.0448],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.447

[Epoch: 136, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0024, 0.0028, 0.0030, 0.0033, 0.0025, 0.0027, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 136, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8399, 0.0031, 0.0030, 0.0165, 0.0478, 0.0340, 0.0556],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 137, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.9955e-03, 5.4411e-02, 4.1829e-03, 3.7284e-05, 5.8702e-02, 8.0144e-01,
        7.5236e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 137, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0041, 0.0038, 0.0039, 0.1021, 0.0048, 0.0040, 0.8772],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 137, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0474, 0.0543, 0.0562, 0.0508, 0.6878, 0.0564, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 137, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0036, 0.0037, 0.0035, 0.0035, 0.0031, 0.9793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.010

[Epoch: 137, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8553, 0.0027, 0.0040, 0.0137, 0.0469, 0.0261, 0.0514],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 138, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.5426e-03, 4.3767e-02, 4.4997e-03, 3.3471e-05, 4.0633e-02, 8.1628e-01,
        8.6245e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 138, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0024, 0.0035, 0.0517, 0.0022, 0.0038, 0.9327],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 138, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0457, 0.0396, 0.0433, 0.0508, 0.7252, 0.0511, 0.0443],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 138, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0040, 0.0040, 0.0037, 0.0042, 0.0036, 0.9768],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 138, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8394, 0.0028, 0.0028, 0.0163, 0.0431, 0.0379, 0.0576],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 139, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.5343e-03, 4.4305e-02, 4.4036e-03, 2.7320e-05, 4.9658e-02, 8.2114e-01,
        7.4932e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 139, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0042, 0.0039, 0.0024, 0.0914, 0.0048, 0.0034, 0.8899],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 139, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0519, 0.0479, 0.0521, 0.0527, 0.6985, 0.0520, 0.0450],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 139, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0042, 0.0043, 0.0038, 0.0045, 0.0036, 0.9762],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 139, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8250, 0.0039, 0.0048, 0.0162, 0.0526, 0.0325, 0.0649],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 140, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.5366e-03, 4.7019e-02, 3.4054e-03, 3.8153e-05, 6.1401e-02, 8.1237e-01,
        7.0233e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 140, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0046, 0.0040, 0.0045, 0.0758, 0.0048, 0.0040, 0.9023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 140, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0454, 0.0445, 0.0490, 0.0500, 0.7163, 0.0477, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.468

[Epoch: 140, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0029, 0.0023, 0.0031, 0.0030, 0.0031, 0.9819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.024

[Epoch: 140, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8431, 0.0037, 0.0036, 0.0187, 0.0454, 0.0304, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 141, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.1338e-02, 4.8382e-02, 4.9437e-03, 3.1833e-05, 3.5603e-02, 8.2271e-01,
        7.6995e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 141, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0033, 0.0036, 0.0657, 0.0021, 0.0034, 0.9190],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.002

[Epoch: 141, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0476, 0.0477, 0.0499, 0.0503, 0.6986, 0.0554, 0.0505],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.474

[Epoch: 141, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0030, 0.0026, 0.0030, 0.0029, 0.0025, 0.9834],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 141, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8247, 0.0028, 0.0029, 0.0162, 0.0440, 0.0412, 0.0682],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.006

[Epoch: 142, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.2591e-03, 4.1274e-02, 2.8001e-03, 3.2868e-05, 5.1454e-02, 8.2054e-01,
        7.9638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.045

[Epoch: 142, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0042, 0.0044, 0.0033, 0.0909, 0.0028, 0.0031, 0.8914],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 142, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0446, 0.0421, 0.0471, 0.0460, 0.7292, 0.0471, 0.0439],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.461

[Epoch: 142, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0036, 0.0033, 0.0030, 0.0024, 0.0027, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 142, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8364, 0.0030, 0.0031, 0.0172, 0.0507, 0.0323, 0.0574],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.032

[Epoch: 143, batch: 42/213] total loss per batch: 0.660
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.3227e-03, 5.5921e-02, 4.1965e-03, 5.1165e-05, 6.3884e-02, 7.9702e-01,
        7.3610e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 143, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0026, 0.0046, 0.0705, 0.0063, 0.0035, 0.9098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.023

[Epoch: 143, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0543, 0.0590, 0.0588, 0.0551, 0.6659, 0.0500, 0.0570],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.523

[Epoch: 143, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0031, 0.0031, 0.0031, 0.0031, 0.0038, 0.9806],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 143, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8484, 0.0029, 0.0036, 0.0178, 0.0470, 0.0304, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 144, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.1728e-02, 3.6255e-02, 4.0259e-03, 3.1263e-05, 4.2532e-02, 8.3362e-01,
        7.1804e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 144, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0042, 0.0038, 0.0836, 0.0016, 0.0049, 0.8994],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 144, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0310, 0.0294, 0.0327, 0.0370, 0.7988, 0.0383, 0.0328],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.457

[Epoch: 144, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0027, 0.0025, 0.0027, 0.0033, 0.0024, 0.9831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 144, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8230, 0.0028, 0.0044, 0.0163, 0.0495, 0.0365, 0.0675],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 145, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.7925e-03, 5.2968e-02, 2.8497e-03, 3.8233e-05, 4.2748e-02, 8.1243e-01,
        8.3174e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 145, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0032, 0.0025, 0.0026, 0.0755, 0.0046, 0.0028, 0.9088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 145, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0618, 0.0580, 0.0659, 0.0605, 0.6304, 0.0582, 0.0653],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 145, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0038, 0.0037, 0.0036, 0.0030, 0.0031, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 145, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8058, 0.0040, 0.0034, 0.0232, 0.0521, 0.0394, 0.0721],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.011

[Epoch: 146, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8760e-03, 5.4293e-02, 3.8916e-03, 4.8085e-05, 6.3494e-02, 8.0876e-01,
        6.3636e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 146, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0021, 0.0025, 0.0022, 0.0674, 0.0031, 0.0024, 0.9205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 146, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0338, 0.0380, 0.0408, 0.0445, 0.7492, 0.0488, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.465

[Epoch: 146, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0034, 0.0035, 0.0027, 0.0035, 0.0039, 0.9798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 146, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8397, 0.0032, 0.0032, 0.0172, 0.0507, 0.0334, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 147, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.0478e-03, 3.4320e-02, 4.3543e-03, 3.4474e-05, 3.9888e-02, 8.2492e-01,
        8.9432e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 147, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0033, 0.0026, 0.0728, 0.0020, 0.0035, 0.9130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.002

[Epoch: 147, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0440, 0.0465, 0.0465, 0.0493, 0.7185, 0.0499, 0.0453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.478

[Epoch: 147, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0033, 0.0032, 0.0031, 0.0037, 0.0030, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 147, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8307, 0.0031, 0.0035, 0.0177, 0.0476, 0.0348, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 148, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.0265e-03, 5.5176e-02, 3.7496e-03, 4.6240e-05, 4.3907e-02, 7.9798e-01,
        9.1114e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.047

[Epoch: 148, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0037, 0.0039, 0.0994, 0.0039, 0.0040, 0.8814],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 148, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0521, 0.0526, 0.0510, 0.0511, 0.6942, 0.0518, 0.0474],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 148, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0025, 0.0034, 0.0033, 0.0030, 0.0034, 0.0030, 0.9815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 148, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8309, 0.0032, 0.0044, 0.0163, 0.0482, 0.0335, 0.0635],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 149, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8693e-03, 4.7107e-02, 4.3723e-03, 3.0286e-05, 5.2338e-02, 8.2909e-01,
        6.1195e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 149, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0035, 0.0044, 0.0559, 0.0053, 0.0035, 0.9244],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 149, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0446, 0.0502, 0.0501, 0.0513, 0.7077, 0.0516, 0.0445],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.461

[Epoch: 149, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0037, 0.0039, 0.0037, 0.0037, 0.0041, 0.9771],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 149, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8556, 0.0030, 0.0031, 0.0164, 0.0447, 0.0314, 0.0459],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 150, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.0346e-02, 4.0721e-02, 3.9278e-03, 2.6790e-05, 5.0624e-02, 8.2441e-01,
        6.9941e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.064

[Epoch: 150, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0042, 0.0027, 0.0023, 0.1033, 0.0026, 0.0024, 0.8825],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.002

[Epoch: 150, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0560, 0.0505, 0.0516, 0.0522, 0.6900, 0.0510, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 150, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0038, 0.0036, 0.0035, 0.0037, 0.0030, 0.9792],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 150, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8438, 0.0027, 0.0030, 0.0156, 0.0443, 0.0273, 0.0632],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 151, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.6939e-03, 5.1965e-02, 3.5882e-03, 3.5583e-05, 4.6228e-02, 8.0070e-01,
        9.1787e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.048

[Epoch: 151, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0037, 0.0030, 0.0616, 0.0027, 0.0052, 0.9208],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 151, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0374, 0.0406, 0.0451, 0.0459, 0.7408, 0.0477, 0.0425],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.475

[Epoch: 151, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0034, 0.0029, 0.0031, 0.0024, 0.0032, 0.9819],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 151, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8276, 0.0034, 0.0038, 0.0182, 0.0441, 0.0419, 0.0609],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 152, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5056e-03, 4.9826e-02, 4.3087e-03, 3.3456e-05, 6.0016e-02, 8.0964e-01,
        6.9672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 152, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0047, 0.0038, 0.0748, 0.0032, 0.0037, 0.9060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 152, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0439, 0.0440, 0.0489, 0.0501, 0.7189, 0.0484, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.484

[Epoch: 152, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0034, 0.0031, 0.0030, 0.0025, 0.0029, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 152, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8391, 0.0030, 0.0033, 0.0174, 0.0490, 0.0332, 0.0551],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 153, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.7283e-03, 4.5413e-02, 3.4554e-03, 2.8548e-05, 4.3373e-02, 8.2119e-01,
        7.9815e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 153, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0033, 0.0037, 0.0869, 0.0041, 0.0034, 0.8951],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 153, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0512, 0.0493, 0.0541, 0.0509, 0.6964, 0.0511, 0.0470],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.478

[Epoch: 153, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0034, 0.0033, 0.0033, 0.0037, 0.0031, 0.9796],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 153, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8364, 0.0029, 0.0031, 0.0151, 0.0499, 0.0320, 0.0605],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 154, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2288e-03, 4.7459e-02, 3.9656e-03, 3.4967e-05, 5.1688e-02, 8.1300e-01,
        7.7622e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 154, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0034, 0.0029, 0.0743, 0.0036, 0.0036, 0.9092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 154, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0450, 0.0463, 0.0476, 0.0475, 0.7200, 0.0480, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 154, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0037, 0.0039, 0.0035, 0.0035, 0.0035, 0.9784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 154, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8300, 0.0035, 0.0039, 0.0179, 0.0472, 0.0362, 0.0613],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 155, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.3759e-03, 4.4270e-02, 3.8711e-03, 2.6919e-05, 4.7936e-02, 8.2211e-01,
        7.4413e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 155, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0029, 0.0029, 0.0687, 0.0025, 0.0030, 0.9166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 155, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0440, 0.0451, 0.0485, 0.0497, 0.7183, 0.0475, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.476

[Epoch: 155, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0035, 0.0030, 0.0032, 0.0034, 0.0032, 0.9807],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 155, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8266, 0.0037, 0.0039, 0.0188, 0.0510, 0.0369, 0.0591],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.016

[Epoch: 156, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5695e-03, 4.9355e-02, 3.5428e-03, 2.8045e-05, 5.4345e-02, 8.0736e-01,
        7.8800e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 156, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0033, 0.0030, 0.0026, 0.0785, 0.0031, 0.0031, 0.9063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 156, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0496, 0.0496, 0.0547, 0.0529, 0.6900, 0.0528, 0.0502],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 156, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0028, 0.0030, 0.0031, 0.0026, 0.0031, 0.9828],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.018

[Epoch: 156, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8275, 0.0032, 0.0038, 0.0166, 0.0480, 0.0322, 0.0687],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 157, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9419e-03, 4.7718e-02, 3.6291e-03, 2.4627e-05, 4.3468e-02, 8.2023e-01,
        7.7991e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 157, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0027, 0.0026, 0.0711, 0.0018, 0.0027, 0.9159],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 157, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0472, 0.0469, 0.0492, 0.0511, 0.7068, 0.0523, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.471

[Epoch: 157, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0039, 0.0041, 0.0040, 0.0037, 0.0039, 0.9770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.006

[Epoch: 157, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8508, 0.0029, 0.0032, 0.0148, 0.0468, 0.0314, 0.0501],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 158, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8946e-03, 4.3004e-02, 3.7002e-03, 2.6574e-05, 5.0664e-02, 8.2712e-01,
        6.9590e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 158, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0028, 0.0037, 0.0032, 0.0769, 0.0026, 0.0026, 0.9082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 158, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0462, 0.0439, 0.0470, 0.0458, 0.7215, 0.0512, 0.0444],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.479

[Epoch: 158, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0040, 0.0034, 0.0038, 0.0035, 0.0033, 0.9786],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 158, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8556, 0.0023, 0.0032, 0.0130, 0.0408, 0.0321, 0.0529],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 159, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.4316e-03, 4.4366e-02, 2.7679e-03, 2.4945e-05, 4.4195e-02, 8.1318e-01,
        8.8035e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 159, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0054, 0.0050, 0.0965, 0.0092, 0.0069, 0.8726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 159, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0475, 0.0445, 0.0577, 0.0595, 0.6828, 0.0538, 0.0541],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.447

[Epoch: 159, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0041, 0.0035, 0.0036, 0.0036, 0.0042, 0.0040, 0.9769],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 159, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8336, 0.0034, 0.0027, 0.0146, 0.0449, 0.0335, 0.0673],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.012

[Epoch: 160, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.3812e-03, 5.0220e-02, 4.1498e-03, 3.0349e-05, 6.0879e-02, 8.1395e-01,
        6.5393e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 160, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0025, 0.0033, 0.0647, 0.0020, 0.0031, 0.9200],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 160, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0359, 0.0439, 0.0440, 0.0440, 0.7470, 0.0461, 0.0391],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.494

[Epoch: 160, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0037, 0.0030, 0.0036, 0.0030, 0.0032, 0.9797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 160, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8236, 0.0038, 0.0040, 0.0200, 0.0494, 0.0401, 0.0590],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.017

[Epoch: 161, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5026e-03, 4.5453e-02, 3.0856e-03, 3.2483e-05, 4.6265e-02, 8.1941e-01,
        7.9255e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 161, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0021, 0.0023, 0.0021, 0.0727, 0.0019, 0.0033, 0.9155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.002

[Epoch: 161, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0500, 0.0500, 0.0531, 0.0505, 0.6959, 0.0520, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 161, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0024, 0.0028, 0.0025, 0.0032, 0.0029, 0.0030, 0.9831],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 161, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8283, 0.0034, 0.0046, 0.0204, 0.0476, 0.0339, 0.0617],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 162, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.2040e-03, 4.6052e-02, 4.9949e-03, 3.6481e-05, 5.6560e-02, 7.9459e-01,
        8.8558e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 162, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0033, 0.0033, 0.0748, 0.0040, 0.0032, 0.9077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 162, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0431, 0.0458, 0.0483, 0.0479, 0.7246, 0.0468, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 162, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0031, 0.0037, 0.0030, 0.0036, 0.0030, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 162, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8321, 0.0029, 0.0026, 0.0148, 0.0493, 0.0355, 0.0629],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 163, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.9481e-03, 3.8851e-02, 3.3718e-03, 3.3408e-05, 4.2674e-02, 8.3913e-01,
        6.9990e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 163, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0039, 0.0036, 0.0824, 0.0031, 0.0036, 0.8997],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 163, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0502, 0.0505, 0.0559, 0.0502, 0.6887, 0.0560, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.480

[Epoch: 163, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0041, 0.0038, 0.0037, 0.0040, 0.0036, 0.9771],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 163, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8247, 0.0034, 0.0045, 0.0184, 0.0495, 0.0354, 0.0641],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 164, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9678e-03, 5.4614e-02, 3.3534e-03, 2.9431e-05, 4.8375e-02, 8.1138e-01,
        7.5277e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 164, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0030, 0.0028, 0.0772, 0.0026, 0.0026, 0.9093],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 164, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0492, 0.0448, 0.0513, 0.0484, 0.7079, 0.0507, 0.0477],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.470

[Epoch: 164, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0037, 0.0033, 0.0037, 0.0037, 0.0036, 0.9784],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 164, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8351, 0.0038, 0.0043, 0.0165, 0.0465, 0.0338, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 165, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.7495e-03, 4.6249e-02, 3.5818e-03, 2.4846e-05, 5.1970e-02, 8.1466e-01,
        7.5763e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.058

[Epoch: 165, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0030, 0.0025, 0.0730, 0.0024, 0.0034, 0.9127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 165, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0471, 0.0449, 0.0483, 0.0517, 0.7108, 0.0492, 0.0480],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 165, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0036, 0.0032, 0.0029, 0.0031, 0.0028, 0.0036, 0.9809],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 165, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8353, 0.0030, 0.0034, 0.0164, 0.0447, 0.0363, 0.0608],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 166, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.6076e-03, 4.6088e-02, 3.3958e-03, 3.0716e-05, 4.8201e-02, 8.1626e-01,
        7.9418e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 166, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0039, 0.0038, 0.0873, 0.0038, 0.0036, 0.8937],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 166, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0448, 0.0450, 0.0506, 0.0492, 0.7131, 0.0524, 0.0449],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.478

[Epoch: 166, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0029, 0.0030, 0.0032, 0.0032, 0.0031, 0.9814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 166, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8476, 0.0028, 0.0031, 0.0156, 0.0439, 0.0328, 0.0542],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 167, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8945e-03, 4.5155e-02, 3.5058e-03, 2.5731e-05, 4.8429e-02, 8.1907e-01,
        7.7922e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 167, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0043, 0.0044, 0.0050, 0.0723, 0.0042, 0.0044, 0.9053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 167, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0484, 0.0471, 0.0520, 0.0481, 0.7095, 0.0482, 0.0468],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.491

[Epoch: 167, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0033, 0.0036, 0.0032, 0.0036, 0.0032, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 167, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8481, 0.0029, 0.0031, 0.0140, 0.0457, 0.0314, 0.0548],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 168, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.3221e-03, 4.9949e-02, 3.4804e-03, 2.5854e-05, 5.3492e-02, 8.1266e-01,
        7.4076e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 168, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0034, 0.0033, 0.0033, 0.0791, 0.0030, 0.0030, 0.9049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 168, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0484, 0.0474, 0.0499, 0.0482, 0.7111, 0.0485, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.471

[Epoch: 168, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0044, 0.0042, 0.0037, 0.0041, 0.0041, 0.0042, 0.9753],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 168, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8411, 0.0030, 0.0029, 0.0162, 0.0449, 0.0328, 0.0591],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 169, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.0193e-03, 4.2119e-02, 3.7951e-03, 2.5974e-05, 4.5593e-02, 8.2192e-01,
        7.7528e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 169, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0025, 0.0027, 0.0023, 0.0722, 0.0028, 0.0036, 0.9139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.007

[Epoch: 169, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0442, 0.0462, 0.0513, 0.0497, 0.7158, 0.0475, 0.0455],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.474

[Epoch: 169, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0031, 0.0026, 0.0029, 0.0029, 0.0030, 0.9829],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 169, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8095, 0.0046, 0.0049, 0.0204, 0.0495, 0.0434, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 170, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.4574e-03, 5.3976e-02, 3.3181e-03, 2.5171e-05, 6.0125e-02, 8.0308e-01,
        7.5022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 170, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0028, 0.0026, 0.0796, 0.0037, 0.0034, 0.9053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 170, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0448, 0.0439, 0.0490, 0.0515, 0.7112, 0.0524, 0.0473],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.462

[Epoch: 170, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0027, 0.0029, 0.0029, 0.0024, 0.0033, 0.9829],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 170, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8029, 0.0036, 0.0032, 0.0194, 0.0604, 0.0373, 0.0733],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.025

[Epoch: 171, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.6714e-03, 3.8604e-02, 4.4119e-03, 3.3441e-05, 4.6185e-02, 8.2281e-01,
        8.0285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 171, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0038, 0.0035, 0.0756, 0.0026, 0.0032, 0.9069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.001

[Epoch: 171, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0470, 0.0447, 0.0482, 0.0494, 0.7150, 0.0503, 0.0454],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.481

[Epoch: 171, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0034, 0.0031, 0.0035, 0.0034, 0.0032, 0.0035, 0.9799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.026

[Epoch: 171, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8433, 0.0025, 0.0038, 0.0148, 0.0482, 0.0314, 0.0561],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 172, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9645e-03, 5.1896e-02, 3.4789e-03, 3.4157e-05, 4.4843e-02, 8.1166e-01,
        8.1125e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 172, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0030, 0.0038, 0.0798, 0.0026, 0.0035, 0.9038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.017

[Epoch: 172, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0577, 0.0541, 0.0554, 0.0541, 0.6727, 0.0535, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.533

[Epoch: 172, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0040, 0.0045, 0.0052, 0.0044, 0.0052, 0.0040, 0.9728],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 172, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8576, 0.0035, 0.0038, 0.0135, 0.0430, 0.0304, 0.0483],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 173, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.2837e-03, 3.8045e-02, 4.3367e-03, 3.1229e-05, 4.9675e-02, 8.2342e-01,
        7.7208e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.063

[Epoch: 173, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0026, 0.0027, 0.0030, 0.0778, 0.0039, 0.0027, 0.9073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 173, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0352, 0.0368, 0.0390, 0.0396, 0.7666, 0.0455, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.426

[Epoch: 173, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0034, 0.0029, 0.0033, 0.0030, 0.0028, 0.9814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.038

[Epoch: 173, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8398, 0.0031, 0.0032, 0.0147, 0.0413, 0.0316, 0.0663],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.024

[Epoch: 174, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.3272e-03, 5.5104e-02, 3.5802e-03, 2.7479e-05, 4.7889e-02, 8.1763e-01,
        6.8440e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 174, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0032, 0.0030, 0.0741, 0.0041, 0.0041, 0.9079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.008

[Epoch: 174, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0537, 0.0510, 0.0566, 0.0587, 0.6761, 0.0499, 0.0539],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.464

[Epoch: 174, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0027, 0.0023, 0.0025, 0.0022, 0.0032, 0.9842],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 174, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8307, 0.0027, 0.0030, 0.0181, 0.0470, 0.0367, 0.0618],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.018

[Epoch: 175, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8279e-03, 4.1490e-02, 4.0445e-03, 2.7323e-05, 5.6761e-02, 8.1387e-01,
        7.7984e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.066

[Epoch: 175, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0030, 0.0040, 0.0031, 0.0717, 0.0030, 0.0037, 0.9114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 175, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0440, 0.0466, 0.0467, 0.0456, 0.7185, 0.0488, 0.0499],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 175, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0026, 0.0029, 0.0026, 0.0030, 0.0028, 0.0031, 0.9830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 175, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8395, 0.0033, 0.0034, 0.0150, 0.0466, 0.0348, 0.0574],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.023

[Epoch: 176, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.1102e-03, 5.4719e-02, 4.4118e-03, 3.0508e-05, 4.7019e-02, 8.0779e-01,
        7.9916e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 176, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0039, 0.0040, 0.0777, 0.0028, 0.0042, 0.9038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.014

[Epoch: 176, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0430, 0.0463, 0.0462, 0.0478, 0.7186, 0.0513, 0.0469],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 176, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0034, 0.0038, 0.0038, 0.0040, 0.0034, 0.9780],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 176, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8489, 0.0031, 0.0030, 0.0161, 0.0447, 0.0308, 0.0533],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 177, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5532e-03, 3.7378e-02, 4.1689e-03, 2.8941e-05, 4.3919e-02, 8.2500e-01,
        8.1954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.051

[Epoch: 177, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0043, 0.0040, 0.0049, 0.0909, 0.0046, 0.0045, 0.8869],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 177, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0458, 0.0477, 0.0499, 0.0487, 0.7116, 0.0498, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.489

[Epoch: 177, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0030, 0.0034, 0.0034, 0.0032, 0.0035, 0.0037, 0.9798],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 177, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8169, 0.0036, 0.0039, 0.0182, 0.0504, 0.0396, 0.0675],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 178, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.9748e-03, 4.7910e-02, 3.8361e-03, 2.6630e-05, 5.6322e-02, 8.1359e-01,
        7.2344e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 178, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0031, 0.0031, 0.0629, 0.0041, 0.0030, 0.9209],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 178, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0491, 0.0469, 0.0518, 0.0545, 0.6976, 0.0495, 0.0505],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.474

[Epoch: 178, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0034, 0.0033, 0.0030, 0.0030, 0.0034, 0.9808],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.020

[Epoch: 178, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8112, 0.0035, 0.0044, 0.0192, 0.0498, 0.0399, 0.0721],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 179, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([1.1134e-02, 4.8870e-02, 3.7559e-03, 2.5826e-05, 5.3355e-02, 8.0426e-01,
        7.8596e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 179, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0027, 0.0032, 0.0022, 0.0747, 0.0015, 0.0025, 0.9132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.006

[Epoch: 179, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0435, 0.0422, 0.0459, 0.0469, 0.7287, 0.0472, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.466

[Epoch: 179, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0031, 0.0036, 0.0033, 0.0030, 0.0027, 0.0030, 0.9813],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 179, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8575, 0.0030, 0.0030, 0.0144, 0.0406, 0.0286, 0.0530],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.014

[Epoch: 180, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([4.8860e-03, 4.2543e-02, 3.7440e-03, 2.6071e-05, 4.3195e-02, 8.3621e-01,
        6.9394e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 180, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0032, 0.0029, 0.0028, 0.0805, 0.0029, 0.0036, 0.9040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 180, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0491, 0.0478, 0.0552, 0.0514, 0.6929, 0.0554, 0.0482],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.499

[Epoch: 180, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0027, 0.0030, 0.0028, 0.0029, 0.0035, 0.9823],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 180, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8552, 0.0024, 0.0027, 0.0123, 0.0471, 0.0281, 0.0522],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 181, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.0622e-03, 4.6543e-02, 3.0805e-03, 2.7007e-05, 5.5099e-02, 8.0670e-01,
        8.3485e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 181, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0036, 0.0029, 0.0764, 0.0033, 0.0030, 0.9072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 181, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0456, 0.0504, 0.0517, 0.0529, 0.7065, 0.0486, 0.0443],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 181, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0038, 0.0042, 0.0040, 0.0036, 0.0038, 0.9768],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.011

[Epoch: 181, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8514, 0.0029, 0.0036, 0.0149, 0.0420, 0.0327, 0.0525],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.027

[Epoch: 182, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.9844e-03, 4.9433e-02, 3.4109e-03, 2.7492e-05, 4.5304e-02, 8.1539e-01,
        7.9452e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.053

[Epoch: 182, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0038, 0.0035, 0.0033, 0.0850, 0.0034, 0.0035, 0.8976],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 182, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0463, 0.0464, 0.0504, 0.0471, 0.7107, 0.0502, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 182, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0039, 0.0042, 0.0041, 0.0035, 0.0038, 0.0035, 0.9769],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 182, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8240, 0.0038, 0.0055, 0.0174, 0.0475, 0.0368, 0.0651],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 183, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([9.0983e-03, 4.5584e-02, 3.8883e-03, 2.2522e-05, 4.7655e-02, 8.2842e-01,
        6.5336e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 183, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0045, 0.0041, 0.0806, 0.0045, 0.0044, 0.8984],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 183, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0504, 0.0484, 0.0509, 0.0542, 0.6946, 0.0518, 0.0496],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.471

[Epoch: 183, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0027, 0.0033, 0.0028, 0.0032, 0.0031, 0.9814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.021

[Epoch: 183, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8095, 0.0042, 0.0038, 0.0194, 0.0519, 0.0386, 0.0726],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 184, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.0742e-03, 4.5428e-02, 3.9831e-03, 4.3613e-05, 6.3835e-02, 7.8839e-01,
        9.3247e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 184, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0040, 0.0034, 0.0688, 0.0030, 0.0038, 0.9134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.009

[Epoch: 184, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0362, 0.0376, 0.0432, 0.0439, 0.7502, 0.0468, 0.0420],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.472

[Epoch: 184, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0028, 0.0027, 0.0030, 0.0024, 0.0031, 0.9833],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.008

[Epoch: 184, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8288, 0.0021, 0.0025, 0.0172, 0.0423, 0.0370, 0.0701],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.029

[Epoch: 185, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.5569e-03, 4.1242e-02, 3.3828e-03, 3.6064e-05, 3.9308e-02, 8.3855e-01,
        6.9922e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.049

[Epoch: 185, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0036, 0.0023, 0.0037, 0.0712, 0.0028, 0.0031, 0.9134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.026

[Epoch: 185, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0542, 0.0604, 0.0571, 0.0544, 0.6826, 0.0480, 0.0432],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.505

[Epoch: 185, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0033, 0.0034, 0.0035, 0.0031, 0.0031, 0.9803],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.009

[Epoch: 185, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8584, 0.0034, 0.0033, 0.0132, 0.0438, 0.0310, 0.0468],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.007

[Epoch: 186, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.5704e-03, 5.9080e-02, 3.7476e-03, 3.0942e-05, 5.7848e-02, 8.0115e-01,
        7.2577e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 186, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0031, 0.0032, 0.0671, 0.0024, 0.0029, 0.9184],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.018

[Epoch: 186, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0455, 0.0417, 0.0450, 0.0469, 0.7291, 0.0452, 0.0466],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.451

[Epoch: 186, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0039, 0.0038, 0.0039, 0.0037, 0.0032, 0.9783],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.016

[Epoch: 186, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8319, 0.0034, 0.0039, 0.0166, 0.0510, 0.0360, 0.0572],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.021

[Epoch: 187, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([7.6412e-03, 4.3514e-02, 3.7933e-03, 2.8689e-05, 4.9558e-02, 8.1487e-01,
        8.0596e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 187, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0028, 0.0026, 0.0929, 0.0034, 0.0030, 0.8922],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.016

[Epoch: 187, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0468, 0.0449, 0.0517, 0.0503, 0.7014, 0.0532, 0.0517],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.486

[Epoch: 187, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0035, 0.0037, 0.0035, 0.0038, 0.0034, 0.0035, 0.9786],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.012

[Epoch: 187, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8077, 0.0036, 0.0044, 0.0184, 0.0537, 0.0365, 0.0757],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.030

[Epoch: 188, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.6001e-03, 4.5880e-02, 3.7092e-03, 3.0140e-05, 5.0892e-02, 8.1287e-01,
        8.0017e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.056

[Epoch: 188, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0037, 0.0037, 0.0036, 0.0670, 0.0044, 0.0043, 0.9133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 188, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0467, 0.0483, 0.0511, 0.0515, 0.6983, 0.0552, 0.0489],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.471

[Epoch: 188, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0037, 0.0035, 0.0037, 0.0035, 0.0033, 0.0029, 0.9795],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.014

[Epoch: 188, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8405, 0.0029, 0.0031, 0.0149, 0.0443, 0.0340, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.026

[Epoch: 189, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.8202e-03, 4.7569e-02, 3.5547e-03, 3.2252e-05, 5.0924e-02, 8.2357e-01,
        6.8527e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.050

[Epoch: 189, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0039, 0.0045, 0.0825, 0.0038, 0.0039, 0.8971],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.015

[Epoch: 189, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0461, 0.0471, 0.0486, 0.0500, 0.7124, 0.0512, 0.0446],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.475

[Epoch: 189, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0040, 0.0041, 0.0038, 0.0039, 0.0036, 0.0036, 0.9770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 189, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8440, 0.0030, 0.0031, 0.0154, 0.0480, 0.0334, 0.0531],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.022

[Epoch: 190, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.5989e-03, 4.2926e-02, 3.5299e-03, 2.7901e-05, 4.7771e-02, 8.1539e-01,
        8.3753e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.059

[Epoch: 190, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0039, 0.0032, 0.0038, 0.0788, 0.0038, 0.0032, 0.9032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.011

[Epoch: 190, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0467, 0.0455, 0.0476, 0.0492, 0.7176, 0.0476, 0.0457],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.484

[Epoch: 190, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0038, 0.0038, 0.0034, 0.0035, 0.0033, 0.0034, 0.9788],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 190, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8319, 0.0031, 0.0038, 0.0169, 0.0512, 0.0346, 0.0584],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 191, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2068e-03, 4.6592e-02, 3.3427e-03, 2.4349e-05, 4.9363e-02, 8.1755e-01,
        7.6923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 191, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0035, 0.0030, 0.0028, 0.0718, 0.0028, 0.0037, 0.9124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.013

[Epoch: 191, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0463, 0.0459, 0.0488, 0.0494, 0.7157, 0.0476, 0.0462],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.485

[Epoch: 191, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0033, 0.0033, 0.0027, 0.0029, 0.0026, 0.0031, 0.9822],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.023

[Epoch: 191, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8254, 0.0034, 0.0040, 0.0180, 0.0475, 0.0357, 0.0660],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.028

[Epoch: 192, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.8133e-03, 4.8355e-02, 3.5186e-03, 2.3786e-05, 4.8605e-02, 8.1424e-01,
        7.8439e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 192, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0031, 0.0032, 0.0032, 0.0762, 0.0038, 0.0031, 0.9074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.010

[Epoch: 192, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0450, 0.0458, 0.0495, 0.0495, 0.7143, 0.0494, 0.0464],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.474

[Epoch: 192, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0028, 0.0032, 0.0029, 0.0027, 0.0024, 0.0026, 0.9835],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 192, batch: 210/213] total loss per batch: 0.652
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8350, 0.0028, 0.0030, 0.0170, 0.0463, 0.0356, 0.0603],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.031

[Epoch: 193, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.8111e-03, 4.5482e-02, 3.7352e-03, 2.7128e-05, 5.3616e-02, 8.2173e-01,
        6.8594e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 193, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0029, 0.0041, 0.0043, 0.0853, 0.0043, 0.0038, 0.8953],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 193, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0464, 0.0463, 0.0521, 0.0497, 0.7060, 0.0507, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.477

[Epoch: 193, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0031, 0.0034, 0.0037, 0.0034, 0.0031, 0.9805],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.015

[Epoch: 193, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8420, 0.0027, 0.0026, 0.0133, 0.0492, 0.0320, 0.0581],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 194, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.6282e-03, 4.5107e-02, 4.0788e-03, 2.6003e-05, 4.8400e-02, 8.1626e-01,
        7.9504e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.055

[Epoch: 194, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0044, 0.0032, 0.0038, 0.0662, 0.0037, 0.0036, 0.9150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.012

[Epoch: 194, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0465, 0.0488, 0.0505, 0.0510, 0.7063, 0.0504, 0.0465],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.473

[Epoch: 194, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0039, 0.0037, 0.0041, 0.0037, 0.0047, 0.0042, 0.9757],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.013

[Epoch: 194, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8387, 0.0033, 0.0032, 0.0159, 0.0491, 0.0340, 0.0557],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.020

[Epoch: 195, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.3888e-03, 4.9200e-02, 3.6955e-03, 2.9413e-05, 5.1449e-02, 7.9877e-01,
        8.8471e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.057

[Epoch: 195, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0041, 0.0025, 0.0025, 0.0801, 0.0024, 0.0037, 0.9047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.020

[Epoch: 195, batch: 126/213] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0525, 0.0488, 0.0505, 0.0520, 0.6968, 0.0536, 0.0458],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.492

[Epoch: 195, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0044, 0.0047, 0.0046, 0.0041, 0.0035, 0.0048, 0.9739],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.029

[Epoch: 195, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8201, 0.0047, 0.0080, 0.0183, 0.0494, 0.0383, 0.0612],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 196, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.2003e-03, 4.6082e-02, 3.4680e-03, 2.7212e-05, 4.0309e-02, 8.3599e-01,
        6.7923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.054

[Epoch: 196, batch: 84/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0022, 0.0027, 0.0027, 0.0660, 0.0028, 0.0032, 0.9205],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.022

[Epoch: 196, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0387, 0.0391, 0.0397, 0.0468, 0.7424, 0.0482, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.463

[Epoch: 196, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0039, 0.0026, 0.0032, 0.0030, 0.0031, 0.9811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.025

[Epoch: 196, batch: 210/213] total loss per batch: 0.660
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.7987, 0.0086, 0.0100, 0.0274, 0.0657, 0.0322, 0.0575],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.050

[Epoch: 197, batch: 42/213] total loss per batch: 0.663
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([8.3820e-03, 4.1105e-02, 4.2221e-03, 2.5447e-05, 6.6273e-02, 7.9153e-01,
        8.8462e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.062

[Epoch: 197, batch: 84/213] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0022, 0.0026, 0.0027, 0.0961, 0.0045, 0.0025, 0.8894],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.007

[Epoch: 197, batch: 126/213] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0401, 0.0533, 0.0503, 0.0503, 0.7077, 0.0496, 0.0486],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.492

[Epoch: 197, batch: 168/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0027, 0.0023, 0.0026, 0.0023, 0.0023, 0.0026, 0.9852],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 197, batch: 210/213] total loss per batch: 0.665
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8060, 0.0027, 0.0027, 0.0179, 0.0628, 0.0355, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.019

[Epoch: 198, batch: 42/213] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.6897e-03, 4.0989e-02, 4.1765e-03, 1.6014e-05, 4.3221e-02, 8.2754e-01,
        7.8365e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.052

[Epoch: 198, batch: 84/213] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0020, 0.0022, 0.0027, 0.0773, 0.0097, 0.0046, 0.9015],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.006

[Epoch: 198, batch: 126/213] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0553, 0.0517, 0.0515, 0.0538, 0.6885, 0.0493, 0.0500],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.486

[Epoch: 198, batch: 168/213] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0029, 0.0034, 0.0033, 0.0027, 0.0026, 0.0024, 0.9828],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.017

[Epoch: 198, batch: 210/213] total loss per batch: 0.656
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8307, 0.0032, 0.0029, 0.0148, 0.0517, 0.0342, 0.0626],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.008

[Epoch: 199, batch: 42/213] total loss per batch: 0.659
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([5.1401e-03, 4.2050e-02, 4.0342e-03, 1.3780e-05, 5.9039e-02, 8.1242e-01,
        7.7301e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.046

[Epoch: 199, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0019, 0.0030, 0.0034, 0.0712, 0.0042, 0.0037, 0.9126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 -0.001

[Epoch: 199, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0516, 0.0472, 0.0477, 0.0508, 0.7056, 0.0482, 0.0488],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.487

[Epoch: 199, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0031, 0.0036, 0.0025, 0.0025, 0.0027, 0.9825],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.019

[Epoch: 199, batch: 210/213] total loss per batch: 0.654
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8370, 0.0029, 0.0033, 0.0152, 0.0510, 0.0318, 0.0589],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

[Epoch: 200, batch: 42/213] total loss per batch: 0.658
Policy (actual, predicted): 5 5
Policy data: tensor([0.0067, 0.0467, 0.0033, 0.0000, 0.0500, 0.8167, 0.0767])
Policy pred: tensor([6.8591e-03, 4.8313e-02, 4.5527e-03, 1.8111e-05, 4.9347e-02, 8.1373e-01,
        7.7184e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.055 0.060

[Epoch: 200, batch: 84/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0767, 0.0033, 0.0033, 0.9067])
Policy pred: tensor([0.0023, 0.0027, 0.0030, 0.0821, 0.0040, 0.0038, 0.9021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.013 0.004

[Epoch: 200, batch: 126/213] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0467, 0.0467, 0.0500, 0.0500, 0.7100, 0.0500, 0.0467])
Policy pred: tensor([0.0498, 0.0451, 0.0476, 0.0489, 0.7122, 0.0472, 0.0493],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.480 0.483

[Epoch: 200, batch: 168/213] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.0033, 0.9800])
Policy pred: tensor([0.0032, 0.0029, 0.0033, 0.0028, 0.0030, 0.0028, 0.9820],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.017 -0.022

[Epoch: 200, batch: 210/213] total loss per batch: 0.653
Policy (actual, predicted): 0 0
Policy data: tensor([0.8367, 0.0033, 0.0033, 0.0167, 0.0467, 0.0333, 0.0600])
Policy pred: tensor([0.8334, 0.0032, 0.0031, 0.0177, 0.0466, 0.0342, 0.0618],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.020 0.015

