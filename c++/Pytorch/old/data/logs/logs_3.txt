Training set samples: 6113
Batch size: 32
[Epoch: 1, batch: 38/192] total loss per batch: 1.159
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([2.7626e-03, 1.6840e-02, 8.7248e-01, 2.5379e-02, 7.7320e-04, 6.6320e-02,
        1.5443e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 1, batch: 76/192] total loss per batch: 1.083
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5643, 0.0022, 0.0036, 0.0039, 0.2941, 0.0300, 0.1018],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 1, batch: 114/192] total loss per batch: 1.078
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([2.8226e-03, 1.3365e-02, 2.8809e-03, 8.0487e-04, 9.2596e-01, 5.3763e-02,
        4.0112e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.004

[Epoch: 1, batch: 152/192] total loss per batch: 1.110
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([9.2541e-02, 3.3798e-01, 7.5335e-06, 7.7257e-02, 4.4444e-01, 1.4078e-05,
        4.7771e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 1, batch: 190/192] total loss per batch: 1.070
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([1.8596e-01, 2.7082e-01, 1.5957e-04, 1.8921e-01, 1.7904e-05, 3.7094e-06,
        3.5383e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.004

[Epoch: 2, batch: 38/192] total loss per batch: 0.906
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([1.7768e-03, 6.1940e-03, 9.5146e-01, 1.5640e-02, 8.7763e-04, 1.2857e-02,
        1.1193e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 2, batch: 76/192] total loss per batch: 0.877
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5650, 0.0044, 0.0038, 0.0037, 0.3063, 0.0214, 0.0953],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 2, batch: 114/192] total loss per batch: 0.854
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([1.7997e-03, 8.3851e-03, 5.2905e-03, 7.9169e-04, 9.6733e-01, 1.6197e-02,
        2.0888e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.005

[Epoch: 2, batch: 152/192] total loss per batch: 0.893
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([8.2418e-02, 1.2874e-01, 3.9197e-06, 6.1968e-02, 7.1484e-01, 2.5614e-06,
        1.2035e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.004

[Epoch: 2, batch: 190/192] total loss per batch: 0.878
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([2.9974e-01, 1.2700e-01, 3.2092e-05, 9.0669e-02, 9.2570e-05, 2.3219e-06,
        4.8247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.003

[Epoch: 3, batch: 38/192] total loss per batch: 0.811
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0119, 0.8770, 0.0450, 0.0012, 0.0241, 0.0366],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.006

[Epoch: 3, batch: 76/192] total loss per batch: 0.813
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5717, 0.0052, 0.0047, 0.0048, 0.2799, 0.0167, 0.1171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 3, batch: 114/192] total loss per batch: 0.798
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([2.6487e-03, 6.5916e-03, 7.0045e-03, 9.2815e-04, 9.6711e-01, 1.5437e-02,
        2.8361e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 3, batch: 152/192] total loss per batch: 0.832
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([3.3760e-02, 4.1187e-02, 2.2113e-06, 4.4097e-02, 8.6286e-01, 4.5988e-06,
        1.8089e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.003

[Epoch: 3, batch: 190/192] total loss per batch: 0.821
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.2969e-01, 2.4902e-01, 1.4130e-04, 1.0931e-01, 3.7092e-05, 4.7130e-05,
        3.1175e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.005

[Epoch: 4, batch: 38/192] total loss per batch: 0.797
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0104, 0.9222, 0.0214, 0.0014, 0.0110, 0.0295],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.009

[Epoch: 4, batch: 76/192] total loss per batch: 0.793
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5690, 0.0055, 0.0061, 0.0052, 0.2854, 0.0173, 0.1117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 4, batch: 114/192] total loss per batch: 0.778
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([2.6585e-03, 6.3226e-03, 2.3089e-03, 2.1263e-03, 9.5512e-01, 3.0979e-02,
        4.8139e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 4, batch: 152/192] total loss per batch: 0.796
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([3.4745e-02, 7.1991e-02, 2.4220e-06, 1.7306e-02, 8.2216e-01, 3.3164e-06,
        5.3792e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 4, batch: 190/192] total loss per batch: 0.788
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2341e-01, 9.5300e-02, 1.4049e-04, 2.0788e-01, 1.2911e-04, 5.3990e-05,
        2.7309e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.000

[Epoch: 5, batch: 38/192] total loss per batch: 0.780
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0027, 0.0254, 0.8605, 0.0460, 0.0010, 0.0159, 0.0485],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.005

[Epoch: 5, batch: 76/192] total loss per batch: 0.779
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5593, 0.0048, 0.0059, 0.0046, 0.3012, 0.0162, 0.1081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 5, batch: 114/192] total loss per batch: 0.755
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([2.2369e-03, 6.9100e-03, 6.4166e-03, 1.9973e-03, 9.6150e-01, 2.0508e-02,
        4.3526e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 5, batch: 152/192] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([4.1027e-02, 5.4546e-02, 2.0478e-05, 2.5664e-02, 8.6816e-01, 6.4132e-06,
        1.0578e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.001

[Epoch: 5, batch: 190/192] total loss per batch: 0.759
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.7659e-01, 1.6334e-01, 1.3852e-04, 1.0212e-01, 4.7675e-05, 1.9406e-05,
        2.5774e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 6, batch: 38/192] total loss per batch: 0.752
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([3.2785e-03, 1.2896e-02, 9.5293e-01, 7.7609e-03, 7.1954e-04, 4.4301e-03,
        1.7982e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.004

[Epoch: 6, batch: 76/192] total loss per batch: 0.757
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5451, 0.0057, 0.0058, 0.0057, 0.3029, 0.0212, 0.1135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 6, batch: 114/192] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([3.6005e-03, 3.5268e-03, 5.8609e-03, 1.8486e-03, 9.6509e-01, 1.9405e-02,
        6.6718e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 6, batch: 152/192] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.9806e-02, 4.7401e-02, 3.4769e-06, 1.6130e-02, 8.8873e-01, 5.5881e-06,
        1.7926e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 6, batch: 190/192] total loss per batch: 0.741
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([5.2549e-01, 7.1556e-02, 1.7756e-04, 1.0071e-01, 7.4031e-05, 2.0553e-05,
        3.0197e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.002

[Epoch: 7, batch: 38/192] total loss per batch: 0.745
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0136, 0.8883, 0.0160, 0.0020, 0.0331, 0.0406],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 7, batch: 76/192] total loss per batch: 0.745
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5456, 0.0053, 0.0059, 0.0058, 0.3131, 0.0146, 0.1097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 7, batch: 114/192] total loss per batch: 0.734
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0044, 0.0105, 0.0060, 0.0041, 0.9326, 0.0411, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.004

[Epoch: 7, batch: 152/192] total loss per batch: 0.745
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0410e-02, 3.8925e-02, 9.9900e-06, 1.4900e-02, 9.1493e-01, 2.1477e-06,
        1.0822e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.002

[Epoch: 7, batch: 190/192] total loss per batch: 0.732
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1224e-01, 7.8946e-02, 6.9004e-05, 1.1975e-01, 2.8059e-05, 4.8964e-06,
        3.8896e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.000

[Epoch: 8, batch: 38/192] total loss per batch: 0.738
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([4.3287e-03, 1.4367e-02, 9.3279e-01, 2.2905e-02, 9.0465e-04, 4.9087e-03,
        1.9795e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 8, batch: 76/192] total loss per batch: 0.737
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5652, 0.0047, 0.0067, 0.0053, 0.2981, 0.0157, 0.1044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 8, batch: 114/192] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0018, 0.0035, 0.0047, 0.0032, 0.9689, 0.0164, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 8, batch: 152/192] total loss per batch: 0.744
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.9934e-02, 4.4106e-02, 3.9439e-06, 1.1592e-02, 8.9656e-01, 3.8810e-06,
        1.7798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.001

[Epoch: 8, batch: 190/192] total loss per batch: 0.730
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([2.9528e-01, 9.4253e-02, 1.3053e-04, 1.1391e-01, 1.9447e-05, 3.1015e-05,
        4.9637e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.002

[Epoch: 9, batch: 38/192] total loss per batch: 0.733
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([3.5206e-03, 5.4408e-03, 9.5944e-01, 1.2555e-02, 8.0731e-04, 4.7891e-03,
        1.3446e-02], grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 9, batch: 76/192] total loss per batch: 0.740
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5629, 0.0049, 0.0055, 0.0044, 0.2984, 0.0146, 0.1092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 9, batch: 114/192] total loss per batch: 0.720
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0057, 0.0041, 0.0032, 0.9589, 0.0223, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 9, batch: 152/192] total loss per batch: 0.733
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.0979e-02, 3.3861e-02, 2.3817e-06, 1.3012e-02, 9.3328e-01, 2.1689e-06,
        8.8587e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.004

[Epoch: 9, batch: 190/192] total loss per batch: 0.718
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.7030e-01, 6.4985e-02, 6.7221e-05, 1.2510e-01, 5.8307e-05, 2.6471e-05,
        4.3947e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.003

[Epoch: 10, batch: 38/192] total loss per batch: 0.726
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0098, 0.9551, 0.0101, 0.0018, 0.0093, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 10, batch: 76/192] total loss per batch: 0.737
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5576, 0.0059, 0.0064, 0.0079, 0.2888, 0.0189, 0.1145],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 10, batch: 114/192] total loss per batch: 0.718
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0037, 0.0061, 0.0082, 0.0036, 0.9479, 0.0286, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 10, batch: 152/192] total loss per batch: 0.731
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.7027e-02, 2.7660e-02, 1.1814e-05, 2.4879e-02, 9.0538e-01, 2.4735e-06,
        1.5040e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 10, batch: 190/192] total loss per batch: 0.716
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3067e-01, 6.6723e-02, 9.6765e-05, 4.9427e-02, 1.4931e-05, 1.1339e-05,
        4.5305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.004

[Epoch: 11, batch: 38/192] total loss per batch: 0.722
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0071, 0.9619, 0.0115, 0.0016, 0.0040, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.002

[Epoch: 11, batch: 76/192] total loss per batch: 0.729
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5733, 0.0052, 0.0051, 0.0048, 0.2912, 0.0151, 0.1054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 11, batch: 114/192] total loss per batch: 0.710
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0036, 0.0045, 0.0056, 0.0041, 0.9609, 0.0195, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 11, batch: 152/192] total loss per batch: 0.722
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1226e-02, 3.6189e-02, 4.7890e-06, 1.1901e-02, 9.0304e-01, 6.5429e-06,
        2.7633e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 11, batch: 190/192] total loss per batch: 0.713
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3702e-01, 4.5912e-02, 3.2699e-05, 8.4946e-02, 1.8093e-05, 1.5407e-05,
        4.3205e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 12, batch: 38/192] total loss per batch: 0.709
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.0120, 0.9247, 0.0133, 0.0024, 0.0133, 0.0248],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 12, batch: 76/192] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5618, 0.0053, 0.0056, 0.0058, 0.2958, 0.0156, 0.1101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 12, batch: 114/192] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0033, 0.0044, 0.0047, 0.0032, 0.9626, 0.0202, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 12, batch: 152/192] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0168e-02, 2.7590e-02, 1.0499e-05, 1.5931e-02, 9.1594e-01, 1.9911e-06,
        2.0354e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 12, batch: 190/192] total loss per batch: 0.698
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.5211e-01, 4.6514e-02, 8.3132e-05, 6.3516e-02, 1.8249e-05, 8.4802e-06,
        5.3775e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 13, batch: 38/192] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0087, 0.9542, 0.0130, 0.0015, 0.0045, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.001

[Epoch: 13, batch: 76/192] total loss per batch: 0.709
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5563, 0.0057, 0.0055, 0.0053, 0.2969, 0.0168, 0.1134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 13, batch: 114/192] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0037, 0.0060, 0.0048, 0.0040, 0.9567, 0.0221, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.000

[Epoch: 13, batch: 152/192] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.3631e-02, 2.1371e-02, 1.1993e-06, 1.3913e-02, 9.3717e-01, 2.9912e-06,
        1.3909e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 13, batch: 190/192] total loss per batch: 0.693
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.7527e-01, 3.6247e-02, 3.0689e-05, 8.5964e-02, 1.2820e-05, 1.1410e-05,
        5.0247e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 14, batch: 38/192] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0086, 0.0091, 0.9401, 0.0194, 0.0028, 0.0071, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 14, batch: 76/192] total loss per batch: 0.707
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5626, 0.0053, 0.0051, 0.0060, 0.2895, 0.0166, 0.1148],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 14, batch: 114/192] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0043, 0.0048, 0.0058, 0.0039, 0.9570, 0.0217, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 14, batch: 152/192] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.3909e-02, 2.3054e-02, 5.0937e-06, 1.8187e-02, 9.2151e-01, 1.9728e-06,
        1.3331e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 14, batch: 190/192] total loss per batch: 0.691
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.1187e-01, 6.4735e-02, 1.8075e-05, 8.2765e-02, 2.0435e-05, 7.1038e-06,
        5.4058e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 15, batch: 38/192] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0051, 0.9532, 0.0102, 0.0022, 0.0048, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 15, batch: 76/192] total loss per batch: 0.708
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5535, 0.0049, 0.0052, 0.0057, 0.3021, 0.0172, 0.1114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 15, batch: 114/192] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0030, 0.0027, 0.0040, 0.0027, 0.9634, 0.0216, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 15, batch: 152/192] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.1519e-02, 1.9301e-02, 1.0612e-06, 7.9828e-03, 9.5053e-01, 2.5983e-06,
        1.0661e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 15, batch: 190/192] total loss per batch: 0.692
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.4598e-01, 5.2182e-02, 2.8387e-05, 6.4883e-02, 1.4032e-05, 9.4849e-06,
        4.3691e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 16, batch: 38/192] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.0082, 0.9475, 0.0120, 0.0019, 0.0071, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 16, batch: 76/192] total loss per batch: 0.711
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5620, 0.0056, 0.0058, 0.0065, 0.2916, 0.0170, 0.1115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 16, batch: 114/192] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0060, 0.0079, 0.0058, 0.0043, 0.9439, 0.0290, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 16, batch: 152/192] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7962e-02, 2.4606e-02, 7.1089e-06, 3.1341e-02, 9.1036e-01, 5.0219e-06,
        1.5719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 16, batch: 190/192] total loss per batch: 0.693
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0145e-01, 9.3993e-02, 3.4241e-05, 1.1276e-01, 2.9359e-05, 2.3206e-05,
        3.9172e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.001

[Epoch: 17, batch: 38/192] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0106, 0.9236, 0.0257, 0.0043, 0.0103, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 17, batch: 76/192] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5683, 0.0050, 0.0052, 0.0056, 0.2903, 0.0175, 0.1081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 17, batch: 114/192] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0030, 0.0025, 0.0046, 0.0034, 0.9680, 0.0158, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 17, batch: 152/192] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5745e-02, 2.1707e-02, 1.2306e-06, 6.8773e-03, 9.4633e-01, 2.1094e-06,
        9.3361e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 17, batch: 190/192] total loss per batch: 0.697
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.6145e-01, 5.0354e-02, 1.4024e-05, 9.3386e-02, 1.5985e-05, 6.8785e-06,
        3.9477e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 18, batch: 38/192] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.0073, 0.9492, 0.0094, 0.0030, 0.0071, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 18, batch: 76/192] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5740, 0.0054, 0.0051, 0.0065, 0.2899, 0.0156, 0.1035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 18, batch: 114/192] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0033, 0.0060, 0.0048, 0.0032, 0.9573, 0.0219, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 18, batch: 152/192] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([3.1562e-02, 5.3734e-02, 1.0129e-05, 3.5462e-02, 8.4272e-01, 3.6505e-06,
        3.6510e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 18, batch: 190/192] total loss per batch: 0.701
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.2243e-01, 7.3793e-02, 4.8709e-05, 7.2639e-02, 1.0367e-05, 1.7494e-05,
        5.3107e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.002

[Epoch: 19, batch: 38/192] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0030, 0.9596, 0.0099, 0.0018, 0.0055, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 19, batch: 76/192] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5648, 0.0067, 0.0057, 0.0056, 0.2924, 0.0166, 0.1082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 19, batch: 114/192] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0055, 0.0040, 0.0064, 0.0049, 0.9587, 0.0173, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 0.000

[Epoch: 19, batch: 152/192] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.2338e-02, 1.3685e-02, 1.8162e-06, 1.1774e-02, 9.5831e-01, 8.7918e-07,
        3.8862e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 19, batch: 190/192] total loss per batch: 0.702
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.6189e-01, 4.0550e-02, 3.6779e-05, 9.4422e-02, 3.7569e-05, 1.8656e-05,
        5.0305e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.001

[Epoch: 20, batch: 38/192] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0035, 0.0044, 0.9712, 0.0060, 0.0025, 0.0028, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 20, batch: 76/192] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5662, 0.0057, 0.0055, 0.0057, 0.2915, 0.0188, 0.1067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 20, batch: 114/192] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0037, 0.0045, 0.0025, 0.0055, 0.9556, 0.0258, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 20, batch: 152/192] total loss per batch: 0.712
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([4.1058e-02, 2.6661e-02, 4.5874e-06, 3.6801e-02, 8.8059e-01, 5.6693e-06,
        1.4877e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 20, batch: 190/192] total loss per batch: 0.701
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2736e-01, 7.1631e-02, 5.6385e-05, 7.5657e-02, 2.3447e-05, 1.4088e-05,
        4.2526e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.001

[Epoch: 21, batch: 38/192] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.0050, 0.9415, 0.0134, 0.0046, 0.0099, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 21, batch: 76/192] total loss per batch: 0.711
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5720, 0.0051, 0.0046, 0.0052, 0.2896, 0.0156, 0.1079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 21, batch: 114/192] total loss per batch: 0.697
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0040, 0.0072, 0.0043, 0.9527, 0.0229, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 21, batch: 152/192] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5984e-02, 1.9472e-02, 3.6074e-06, 1.0379e-02, 9.4534e-01, 3.5077e-06,
        8.8232e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 21, batch: 190/192] total loss per batch: 0.696
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([5.0227e-01, 3.0973e-02, 4.0072e-05, 5.5031e-02, 1.3234e-05, 4.7871e-06,
        4.1166e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 22, batch: 38/192] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0036, 0.9615, 0.0089, 0.0024, 0.0098, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 22, batch: 76/192] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5611, 0.0060, 0.0056, 0.0054, 0.2962, 0.0171, 0.1086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 22, batch: 114/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0062, 0.0048, 0.9461, 0.0293, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 22, batch: 152/192] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.0896e-02, 1.2486e-02, 3.2777e-06, 9.4089e-03, 9.5179e-01, 1.6572e-06,
        1.5417e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 22, batch: 190/192] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.7356e-01, 5.0169e-02, 1.6077e-05, 6.4958e-02, 6.5959e-06, 9.5460e-06,
        5.1128e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 23, batch: 38/192] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0044, 0.9591, 0.0091, 0.0028, 0.0065, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 23, batch: 76/192] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5669, 0.0048, 0.0050, 0.0060, 0.2948, 0.0166, 0.1059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 23, batch: 114/192] total loss per batch: 0.684
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0038, 0.0053, 0.0044, 0.9593, 0.0194, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 23, batch: 152/192] total loss per batch: 0.695
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1104e-02, 2.9231e-02, 2.1790e-06, 1.6125e-02, 9.1669e-01, 3.0381e-06,
        1.6843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 23, batch: 190/192] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3040e-01, 5.3012e-02, 3.0759e-05, 6.9530e-02, 8.8483e-06, 5.9873e-06,
        4.4701e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 -0.000

[Epoch: 24, batch: 38/192] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0041, 0.9652, 0.0086, 0.0031, 0.0060, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 24, batch: 76/192] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5448, 0.0054, 0.0053, 0.0056, 0.3081, 0.0166, 0.1142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 24, batch: 114/192] total loss per batch: 0.683
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0040, 0.0041, 0.0060, 0.0047, 0.9509, 0.0262, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 24, batch: 152/192] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.4518e-02, 1.7914e-02, 2.6172e-06, 1.2496e-02, 9.4439e-01, 1.5388e-06,
        1.0682e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 24, batch: 190/192] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.7039e-01, 4.3635e-02, 1.3652e-05, 5.2147e-02, 5.4115e-06, 5.2746e-06,
        5.3380e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 25, batch: 38/192] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0044, 0.9568, 0.0100, 0.0042, 0.0081, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 25, batch: 76/192] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5723, 0.0052, 0.0053, 0.0055, 0.2902, 0.0166, 0.1049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 25, batch: 114/192] total loss per batch: 0.683
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.0055, 0.0050, 0.9541, 0.0221, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 25, batch: 152/192] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5065e-02, 1.7464e-02, 1.5101e-06, 1.3141e-02, 9.4463e-01, 1.0482e-06,
        9.6960e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 25, batch: 190/192] total loss per batch: 0.681
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1125e-01, 5.8476e-02, 1.5667e-05, 7.3157e-02, 9.1622e-06, 4.9143e-06,
        4.5709e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 26, batch: 38/192] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0062, 0.9556, 0.0098, 0.0054, 0.0056, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 26, batch: 76/192] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5655, 0.0050, 0.0045, 0.0053, 0.2990, 0.0154, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 26, batch: 114/192] total loss per batch: 0.684
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0039, 0.0047, 0.0061, 0.9521, 0.0241, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 26, batch: 152/192] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1913e-02, 2.1550e-02, 2.5501e-06, 1.4066e-02, 9.2825e-01, 1.4515e-06,
        1.4213e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 26, batch: 190/192] total loss per batch: 0.682
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.5150e-01, 6.1034e-02, 1.5249e-05, 6.5286e-02, 7.9421e-06, 7.4902e-06,
        4.2215e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 27, batch: 38/192] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0044, 0.9627, 0.0087, 0.0048, 0.0051, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 27, batch: 76/192] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5577, 0.0060, 0.0058, 0.0061, 0.2976, 0.0174, 0.1095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 27, batch: 114/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0059, 0.0087, 0.0039, 0.9480, 0.0249, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 27, batch: 152/192] total loss per batch: 0.697
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7809e-02, 1.7874e-02, 1.5807e-06, 1.5785e-02, 9.3552e-01, 2.6863e-06,
        1.3002e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 27, batch: 190/192] total loss per batch: 0.685
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2287e-01, 4.1143e-02, 3.8374e-05, 7.5773e-02, 9.5284e-06, 7.2514e-06,
        4.6016e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 28, batch: 38/192] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0066, 0.9468, 0.0127, 0.0061, 0.0090, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 28, batch: 76/192] total loss per batch: 0.704
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5756, 0.0047, 0.0047, 0.0048, 0.2943, 0.0153, 0.1006],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 28, batch: 114/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.0038, 0.0056, 0.9549, 0.0226, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 28, batch: 152/192] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7460e-02, 1.6600e-02, 3.0975e-06, 1.6123e-02, 9.4005e-01, 1.1511e-06,
        9.7628e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 0.000

[Epoch: 28, batch: 190/192] total loss per batch: 0.687
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0964e-01, 8.4814e-02, 2.0770e-05, 6.7907e-02, 8.8055e-06, 9.5072e-06,
        4.3760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 29, batch: 38/192] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0061, 0.0051, 0.9537, 0.0094, 0.0062, 0.0089, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 29, batch: 76/192] total loss per batch: 0.704
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5693, 0.0050, 0.0050, 0.0057, 0.2965, 0.0165, 0.1020],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 29, batch: 114/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0059, 0.0043, 0.0045, 0.9495, 0.0273, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 29, batch: 152/192] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8025e-02, 2.5900e-02, 2.5024e-06, 1.7715e-02, 9.2523e-01, 2.5891e-06,
        1.3128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 29, batch: 190/192] total loss per batch: 0.688
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3825e-01, 5.7880e-02, 4.6122e-05, 7.7312e-02, 1.3119e-05, 1.0431e-05,
        4.2649e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 30, batch: 38/192] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0044, 0.9704, 0.0053, 0.0044, 0.0029, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 30, batch: 76/192] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5542, 0.0051, 0.0058, 0.0066, 0.3023, 0.0170, 0.1090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 30, batch: 114/192] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0055, 0.0057, 0.0059, 0.0062, 0.9487, 0.0245, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 30, batch: 152/192] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5825e-02, 1.7242e-02, 1.3401e-06, 8.7319e-03, 9.4636e-01, 1.3853e-06,
        1.1837e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 30, batch: 190/192] total loss per batch: 0.691
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.7201e-01, 5.1967e-02, 3.3163e-05, 7.7984e-02, 2.4515e-05, 9.7721e-06,
        4.9797e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.002

[Epoch: 31, batch: 38/192] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.0063, 0.9503, 0.0092, 0.0063, 0.0083, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 31, batch: 76/192] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5723, 0.0062, 0.0054, 0.0056, 0.2851, 0.0173, 0.1083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 31, batch: 114/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0042, 0.0037, 0.0043, 0.0041, 0.9552, 0.0244, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 31, batch: 152/192] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.9981e-02, 2.9165e-02, 3.8841e-06, 1.5789e-02, 9.0568e-01, 5.2732e-06,
        1.9375e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 31, batch: 190/192] total loss per batch: 0.686
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.2917e-01, 4.9548e-02, 2.2054e-05, 8.0018e-02, 5.8530e-06, 1.1529e-05,
        5.4122e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 32, batch: 38/192] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.9569, 0.0120, 0.0067, 0.0062, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.002

[Epoch: 32, batch: 76/192] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5702, 0.0043, 0.0050, 0.0053, 0.2959, 0.0152, 0.1040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 32, batch: 114/192] total loss per batch: 0.683
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0037, 0.0044, 0.0043, 0.9556, 0.0239, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 32, batch: 152/192] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.6906e-02, 2.0574e-02, 2.4340e-06, 1.2636e-02, 9.3621e-01, 3.7820e-06,
        1.3671e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 32, batch: 190/192] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0762e-01, 4.9026e-02, 1.5361e-05, 6.4388e-02, 7.8500e-06, 8.5938e-06,
        4.7893e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 33, batch: 38/192] total loss per batch: 0.687
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0053, 0.9541, 0.0096, 0.0070, 0.0083, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 33, batch: 76/192] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5745, 0.0053, 0.0052, 0.0054, 0.2896, 0.0158, 0.1042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 33, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0042, 0.0037, 0.0050, 0.0050, 0.9564, 0.0220, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 33, batch: 152/192] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7155e-02, 1.8705e-02, 2.7079e-06, 1.2134e-02, 9.4157e-01, 3.3102e-06,
        1.0427e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 33, batch: 190/192] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2653e-01, 5.7974e-02, 1.1573e-05, 5.9608e-02, 5.9263e-06, 6.6281e-06,
        4.5586e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 34, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0041, 0.9567, 0.0102, 0.0084, 0.0057, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 34, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5628, 0.0052, 0.0054, 0.0058, 0.2941, 0.0171, 0.1096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 34, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0046, 0.0051, 0.9526, 0.0242, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 34, batch: 152/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2753e-02, 1.8110e-02, 1.9863e-06, 1.3276e-02, 9.3568e-01, 3.1520e-06,
        1.0173e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 34, batch: 190/192] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1587e-01, 5.5798e-02, 1.4940e-05, 7.0373e-02, 6.0702e-06, 3.8529e-06,
        4.5793e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 35, batch: 38/192] total loss per batch: 0.685
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0053, 0.9536, 0.0100, 0.0079, 0.0071, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 35, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5718, 0.0055, 0.0057, 0.0062, 0.2908, 0.0167, 0.1033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 35, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0043, 0.0042, 0.0062, 0.0046, 0.9538, 0.0232, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 35, batch: 152/192] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2160e-02, 1.9930e-02, 1.6513e-06, 1.3878e-02, 9.3263e-01, 1.9385e-06,
        1.1395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 35, batch: 190/192] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.4631e-01, 4.7175e-02, 1.3001e-05, 5.6844e-02, 6.0230e-06, 5.8438e-06,
        4.4965e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 36, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0037, 0.9602, 0.0088, 0.0080, 0.0055, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 36, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5631, 0.0047, 0.0053, 0.0056, 0.2969, 0.0162, 0.1082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 36, batch: 114/192] total loss per batch: 0.681
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.0043, 0.0052, 0.9514, 0.0258, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 36, batch: 152/192] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.4909e-02, 1.8510e-02, 2.7526e-06, 1.4744e-02, 9.4440e-01, 2.6270e-06,
        7.4288e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 36, batch: 190/192] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1235e-01, 7.1517e-02, 8.9627e-06, 6.3415e-02, 4.8483e-06, 4.0009e-06,
        4.5270e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 37, batch: 38/192] total loss per batch: 0.687
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.0088, 0.9376, 0.0139, 0.0103, 0.0085, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 37, batch: 76/192] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5728, 0.0054, 0.0057, 0.0062, 0.2887, 0.0166, 0.1045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 37, batch: 114/192] total loss per batch: 0.682
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0053, 0.0045, 0.0069, 0.0044, 0.9500, 0.0250, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 37, batch: 152/192] total loss per batch: 0.693
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([3.2153e-02, 2.4736e-02, 1.7669e-06, 1.5326e-02, 9.1401e-01, 1.7183e-06,
        1.3774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 37, batch: 190/192] total loss per batch: 0.680
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.4920e-01, 5.1471e-02, 1.7398e-05, 5.9355e-02, 6.7293e-06, 8.4571e-06,
        4.3994e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 38, batch: 38/192] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0065, 0.0053, 0.9561, 0.0082, 0.0099, 0.0059, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 38, batch: 76/192] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5709, 0.0051, 0.0049, 0.0052, 0.2937, 0.0150, 0.1051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 38, batch: 114/192] total loss per batch: 0.683
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0055, 0.0049, 0.0044, 0.0059, 0.9474, 0.0271, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 38, batch: 152/192] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.2001e-02, 1.5228e-02, 1.7230e-06, 1.6213e-02, 9.5115e-01, 1.6859e-06,
        5.4020e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 38, batch: 190/192] total loss per batch: 0.682
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.4368e-01, 4.7019e-02, 6.0259e-06, 7.4622e-02, 5.1212e-06, 3.2996e-06,
        5.3466e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 39, batch: 38/192] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0073, 0.9307, 0.0210, 0.0116, 0.0108, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 39, batch: 76/192] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5652, 0.0057, 0.0056, 0.0060, 0.2901, 0.0170, 0.1104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 39, batch: 114/192] total loss per batch: 0.684
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0039, 0.0047, 0.0056, 0.0048, 0.9555, 0.0220, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 39, batch: 152/192] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.8389e-02, 2.9336e-02, 2.6226e-06, 1.7564e-02, 9.0888e-01, 2.9706e-06,
        1.5822e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 39, batch: 190/192] total loss per batch: 0.683
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.8602e-01, 6.5927e-02, 4.6091e-05, 6.3253e-02, 1.5234e-05, 1.4342e-05,
        4.8473e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.002

[Epoch: 40, batch: 38/192] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0073, 0.0051, 0.9540, 0.0078, 0.0079, 0.0066, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 40, batch: 76/192] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5608, 0.0053, 0.0065, 0.0057, 0.2997, 0.0161, 0.1059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 40, batch: 114/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0042, 0.0053, 0.0036, 0.0062, 0.9495, 0.0264, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.002

[Epoch: 40, batch: 152/192] total loss per batch: 0.696
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.2049e-02, 2.2282e-02, 3.3274e-06, 1.3532e-02, 9.4084e-01, 2.5565e-06,
        1.1285e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 40, batch: 190/192] total loss per batch: 0.684
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.5079e-01, 6.8928e-02, 1.4502e-05, 9.4174e-02, 1.0841e-05, 8.9873e-06,
        3.8608e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 41, batch: 38/192] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0070, 0.0074, 0.9451, 0.0122, 0.0095, 0.0094, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 41, batch: 76/192] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5660, 0.0056, 0.0055, 0.0059, 0.2907, 0.0180, 0.1083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 41, batch: 114/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0042, 0.0040, 0.0054, 0.0044, 0.9538, 0.0250, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 41, batch: 152/192] total loss per batch: 0.694
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8178e-02, 1.8066e-02, 2.1080e-06, 1.5004e-02, 9.3917e-01, 1.3923e-06,
        9.5749e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 41, batch: 190/192] total loss per batch: 0.681
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.9097e-01, 4.0426e-02, 2.8706e-05, 5.1022e-02, 8.1204e-06, 1.0084e-05,
        4.1753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 42, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0052, 0.9485, 0.0118, 0.0106, 0.0078, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 42, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5604, 0.0053, 0.0057, 0.0058, 0.3003, 0.0153, 0.1072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 42, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0051, 0.0051, 0.9530, 0.0237, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 42, batch: 152/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7423e-02, 1.7474e-02, 2.3661e-06, 1.6440e-02, 9.4072e-01, 1.4083e-06,
        7.9362e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 42, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1915e-01, 6.2268e-02, 1.9738e-05, 6.1566e-02, 4.7402e-06, 6.1010e-06,
        4.5698e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 43, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0049, 0.9569, 0.0079, 0.0093, 0.0058, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 43, batch: 76/192] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5676, 0.0052, 0.0056, 0.0059, 0.2923, 0.0168, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 43, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0042, 0.0045, 0.0046, 0.0046, 0.9557, 0.0227, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 43, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2806e-02, 2.4037e-02, 2.0888e-06, 1.9786e-02, 9.1979e-01, 2.0517e-06,
        1.3573e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 43, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3017e-01, 5.5587e-02, 1.1647e-05, 6.4946e-02, 4.3256e-06, 7.4090e-06,
        4.4927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 44, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0055, 0.9501, 0.0099, 0.0134, 0.0069, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 44, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5639, 0.0051, 0.0057, 0.0056, 0.2982, 0.0159, 0.1056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 44, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0054, 0.0061, 0.0055, 0.0065, 0.9458, 0.0260, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 44, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.6928e-02, 1.7696e-02, 2.2912e-06, 1.4077e-02, 9.3954e-01, 1.9797e-06,
        1.1750e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 44, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0512e-01, 5.4224e-02, 1.7921e-05, 6.9638e-02, 4.3916e-06, 4.9644e-06,
        4.7099e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 45, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0044, 0.9578, 0.0072, 0.0107, 0.0059, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 45, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5694, 0.0048, 0.0053, 0.0057, 0.2919, 0.0166, 0.1062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 45, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0058, 0.0050, 0.9515, 0.0242, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 45, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0784e-02, 2.2972e-02, 1.1232e-06, 1.7780e-02, 9.2719e-01, 1.4510e-06,
        1.1267e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 45, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1909e-01, 5.4586e-02, 9.2765e-06, 5.7471e-02, 3.3399e-06, 5.0122e-06,
        4.6883e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 46, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.9532, 0.0092, 0.0144, 0.0060, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 46, batch: 76/192] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5610, 0.0053, 0.0055, 0.0060, 0.3009, 0.0164, 0.1050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 46, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0054, 0.0059, 0.0065, 0.9447, 0.0280, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 46, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5860e-02, 1.7085e-02, 2.2208e-06, 1.4757e-02, 9.4243e-01, 1.3019e-06,
        9.8607e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 46, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0932e-01, 5.5879e-02, 1.1480e-05, 6.1286e-02, 3.4840e-06, 3.9578e-06,
        4.7349e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 47, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0058, 0.9517, 0.0073, 0.0131, 0.0065, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 47, batch: 76/192] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5672, 0.0049, 0.0053, 0.0059, 0.2939, 0.0162, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 47, batch: 114/192] total loss per batch: 0.679
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0057, 0.0053, 0.0060, 0.0052, 0.9491, 0.0238, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 47, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0196e-02, 2.2763e-02, 7.4077e-07, 1.6724e-02, 9.3070e-01, 1.1101e-06,
        9.6118e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 47, batch: 190/192] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3791e-01, 5.4668e-02, 1.1992e-05, 6.7555e-02, 3.9147e-06, 5.8390e-06,
        4.3985e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 48, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0040, 0.0038, 0.9604, 0.0079, 0.0115, 0.0052, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 48, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5576, 0.0052, 0.0056, 0.0061, 0.3034, 0.0169, 0.1052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 48, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0048, 0.0055, 0.0055, 0.9489, 0.0266, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 48, batch: 152/192] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7127e-02, 1.8227e-02, 1.8400e-06, 1.3544e-02, 9.4290e-01, 1.0324e-06,
        8.1971e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 48, batch: 190/192] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2411e-01, 6.1879e-02, 8.7877e-06, 5.8737e-02, 3.5607e-06, 5.2220e-06,
        4.5525e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 49, batch: 38/192] total loss per batch: 0.687
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0077, 0.0079, 0.9404, 0.0092, 0.0146, 0.0088, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 49, batch: 76/192] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5732, 0.0048, 0.0056, 0.0057, 0.2916, 0.0158, 0.1034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 49, batch: 114/192] total loss per batch: 0.681
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0065, 0.0062, 0.0053, 0.0052, 0.9463, 0.0257, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 49, batch: 152/192] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.5293e-02, 2.5357e-02, 8.4532e-07, 2.5118e-02, 9.1047e-01, 1.6034e-06,
        1.3764e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 49, batch: 190/192] total loss per batch: 0.679
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0058e-01, 5.1280e-02, 2.1579e-05, 6.5529e-02, 4.2193e-06, 4.4688e-06,
        4.8258e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 50, batch: 38/192] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0056, 0.9535, 0.0109, 0.0127, 0.0058, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 50, batch: 76/192] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5553, 0.0051, 0.0054, 0.0059, 0.3029, 0.0167, 0.1086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 50, batch: 114/192] total loss per batch: 0.681
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.0070, 0.0057, 0.9479, 0.0257, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 50, batch: 152/192] total loss per batch: 0.692
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9139e-02, 1.6875e-02, 3.0230e-06, 1.2236e-02, 9.4358e-01, 1.8637e-06,
        8.1631e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 50, batch: 190/192] total loss per batch: 0.680
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2722e-01, 6.2008e-02, 1.0526e-05, 5.7269e-02, 5.3230e-06, 6.9598e-06,
        4.5348e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 51, batch: 38/192] total loss per batch: 0.688
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0040, 0.9622, 0.0053, 0.0106, 0.0069, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 51, batch: 76/192] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5586, 0.0049, 0.0057, 0.0058, 0.2998, 0.0165, 0.1088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 51, batch: 114/192] total loss per batch: 0.681
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0058, 0.0048, 0.0058, 0.9495, 0.0241, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 51, batch: 152/192] total loss per batch: 0.691
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.3280e-02, 2.2937e-02, 1.0670e-06, 2.0025e-02, 9.1976e-01, 1.8615e-06,
        1.3996e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 51, batch: 190/192] total loss per batch: 0.679
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0167e-01, 5.1463e-02, 1.3767e-05, 7.3222e-02, 4.2602e-06, 6.5714e-06,
        4.7362e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 52, batch: 38/192] total loss per batch: 0.685
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0043, 0.9531, 0.0092, 0.0142, 0.0067, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 52, batch: 76/192] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5630, 0.0049, 0.0057, 0.0056, 0.2966, 0.0162, 0.1081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 52, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0050, 0.0058, 0.0059, 0.9486, 0.0253, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 52, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9855e-02, 1.9381e-02, 1.9870e-06, 1.5419e-02, 9.3409e-01, 1.7914e-06,
        1.1252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 52, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2510e-01, 5.7728e-02, 1.2974e-05, 6.4425e-02, 4.6059e-06, 7.0604e-06,
        4.5272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 53, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.9571, 0.0079, 0.0128, 0.0065, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 53, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5643, 0.0049, 0.0055, 0.0055, 0.2970, 0.0163, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 53, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.0056, 0.0058, 0.9490, 0.0245, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 53, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7097e-02, 1.8385e-02, 1.1799e-06, 1.4016e-02, 9.4115e-01, 1.0960e-06,
        9.3443e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 53, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1943e-01, 6.0883e-02, 8.7255e-06, 6.4347e-02, 3.2383e-06, 4.0745e-06,
        4.5533e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 54, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0038, 0.9559, 0.0072, 0.0155, 0.0053, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 54, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5665, 0.0049, 0.0053, 0.0055, 0.2955, 0.0158, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 54, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.0053, 0.0060, 0.9486, 0.0255, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 54, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0138e-02, 2.0486e-02, 1.2721e-06, 1.4008e-02, 9.3548e-01, 1.2086e-06,
        9.8819e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 54, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0521e-01, 5.9345e-02, 1.1378e-05, 6.8206e-02, 3.2604e-06, 4.5977e-06,
        4.6722e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 55, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0047, 0.9516, 0.0081, 0.0156, 0.0066, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 55, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5681, 0.0050, 0.0053, 0.0054, 0.2954, 0.0159, 0.1048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 55, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.0061, 0.0058, 0.9484, 0.0250, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 55, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8467e-02, 1.8585e-02, 9.7454e-07, 1.4802e-02, 9.3811e-01, 7.6640e-07,
        1.0038e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 55, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2025e-01, 6.1380e-02, 6.5933e-06, 7.0780e-02, 2.4752e-06, 3.4141e-06,
        4.4758e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 56, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0041, 0.9568, 0.0072, 0.0149, 0.0055, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 56, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5655, 0.0049, 0.0054, 0.0056, 0.2969, 0.0162, 0.1056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 56, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.0058, 0.0060, 0.9460, 0.0268, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 56, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9285e-02, 2.0969e-02, 9.4360e-07, 1.5890e-02, 9.3591e-01, 1.0843e-06,
        7.9471e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 56, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1541e-01, 5.0361e-02, 9.0323e-06, 6.7095e-02, 3.3163e-06, 3.8825e-06,
        4.6711e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 57, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0052, 0.9480, 0.0088, 0.0178, 0.0065, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 57, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5659, 0.0050, 0.0054, 0.0057, 0.2964, 0.0162, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 57, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0065, 0.0057, 0.9495, 0.0238, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 57, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9672e-02, 1.8256e-02, 1.2205e-06, 1.5500e-02, 9.3525e-01, 8.5441e-07,
        1.1323e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 57, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2653e-01, 6.3714e-02, 7.9195e-06, 7.1624e-02, 2.1601e-06, 3.6681e-06,
        4.3812e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 58, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0040, 0.9605, 0.0066, 0.0131, 0.0055, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 58, batch: 76/192] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5610, 0.0052, 0.0055, 0.0055, 0.3001, 0.0161, 0.1066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 58, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0053, 0.0043, 0.0051, 0.0051, 0.9481, 0.0273, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 58, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0103e-02, 2.3579e-02, 7.2307e-07, 1.3990e-02, 9.3308e-01, 9.8182e-07,
        9.2419e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 58, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0026e-01, 6.3497e-02, 7.4418e-06, 7.8210e-02, 4.5079e-06, 4.3177e-06,
        4.5802e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 59, batch: 38/192] total loss per batch: 0.685
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0046, 0.9568, 0.0064, 0.0167, 0.0046, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.001

[Epoch: 59, batch: 76/192] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5621, 0.0050, 0.0055, 0.0058, 0.2986, 0.0171, 0.1059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 59, batch: 114/192] total loss per batch: 0.679
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0044, 0.0045, 0.0056, 0.0053, 0.9534, 0.0222, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 59, batch: 152/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.6429e-02, 1.3217e-02, 8.0537e-07, 1.4112e-02, 9.4999e-01, 9.5456e-07,
        6.2546e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 59, batch: 190/192] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.4183e-01, 4.3261e-02, 1.2500e-05, 6.0027e-02, 2.2473e-06, 3.7051e-06,
        4.5487e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 60, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0043, 0.9538, 0.0072, 0.0144, 0.0074, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 60, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5717, 0.0053, 0.0057, 0.0054, 0.2923, 0.0153, 0.1044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 60, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0039, 0.0047, 0.0042, 0.9534, 0.0252, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 60, batch: 152/192] total loss per batch: 0.690
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.5602e-02, 2.8754e-02, 1.2970e-06, 2.4483e-02, 9.0776e-01, 1.4092e-06,
        1.3395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 60, batch: 190/192] total loss per batch: 0.678
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0857e-01, 6.6805e-02, 9.3934e-06, 7.6201e-02, 6.6518e-06, 8.4656e-06,
        4.4840e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 61, batch: 38/192] total loss per batch: 0.686
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0038, 0.9628, 0.0060, 0.0136, 0.0040, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 61, batch: 76/192] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5623, 0.0051, 0.0052, 0.0057, 0.2983, 0.0166, 0.1068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 61, batch: 114/192] total loss per batch: 0.680
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0044, 0.0047, 0.0046, 0.0053, 0.9525, 0.0236, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 61, batch: 152/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5148e-02, 1.9521e-02, 1.1631e-06, 1.1650e-02, 9.4530e-01, 1.8189e-06,
        8.3755e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 61, batch: 190/192] total loss per batch: 0.677
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3815e-01, 5.0985e-02, 1.0898e-05, 6.4521e-02, 4.2569e-06, 5.7486e-06,
        4.4632e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 62, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0035, 0.9606, 0.0058, 0.0143, 0.0050, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 62, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5661, 0.0049, 0.0053, 0.0054, 0.2983, 0.0158, 0.1042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 62, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0044, 0.0042, 0.0052, 0.0051, 0.9520, 0.0246, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 62, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0937e-02, 2.0595e-02, 1.0358e-06, 1.6604e-02, 9.3212e-01, 1.1773e-06,
        9.7400e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 62, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1861e-01, 5.0260e-02, 7.0838e-06, 6.3519e-02, 3.1934e-06, 4.7123e-06,
        4.6760e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 63, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.9541, 0.0071, 0.0172, 0.0056, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 63, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5625, 0.0054, 0.0054, 0.0057, 0.2981, 0.0164, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 63, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0051, 0.0058, 0.0056, 0.9482, 0.0256, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 63, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.7298e-02, 1.8949e-02, 8.4026e-07, 1.3677e-02, 9.4070e-01, 1.1110e-06,
        9.3705e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 63, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1227e-01, 5.8568e-02, 6.9477e-06, 6.2403e-02, 2.7044e-06, 4.2157e-06,
        4.6674e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 64, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0047, 0.9522, 0.0068, 0.0179, 0.0059, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 64, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5640, 0.0051, 0.0056, 0.0057, 0.2975, 0.0165, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 64, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.0055, 0.0055, 0.9493, 0.0252, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 64, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8909e-02, 2.0276e-02, 5.9818e-07, 1.4518e-02, 9.3776e-01, 8.8635e-07,
        8.5330e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 64, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2073e-01, 5.3190e-02, 6.4165e-06, 6.6270e-02, 2.6265e-06, 3.8878e-06,
        4.5980e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 65, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.9522, 0.0078, 0.0175, 0.0061, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 65, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5623, 0.0053, 0.0055, 0.0057, 0.2984, 0.0163, 0.1065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 65, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.0058, 0.0053, 0.9491, 0.0252, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 65, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8587e-02, 1.8810e-02, 6.4279e-07, 1.3981e-02, 9.3929e-01, 7.9416e-07,
        9.3261e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 65, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3293e-01, 5.9563e-02, 5.9048e-06, 6.5320e-02, 2.3954e-06, 3.5681e-06,
        4.4217e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 66, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.9507, 0.0071, 0.0188, 0.0056, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 66, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5627, 0.0051, 0.0056, 0.0056, 0.2984, 0.0163, 0.1063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 66, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.0055, 0.0055, 0.9498, 0.0245, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 66, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0114e-02, 2.0442e-02, 4.9550e-07, 1.5089e-02, 9.3528e-01, 8.0589e-07,
        9.0771e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 66, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1052e-01, 5.3258e-02, 6.9578e-06, 6.9177e-02, 2.2810e-06, 3.5467e-06,
        4.6704e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 67, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0048, 0.9500, 0.0076, 0.0184, 0.0063, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 67, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5638, 0.0052, 0.0057, 0.0057, 0.2969, 0.0165, 0.1062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 67, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0056, 0.0054, 0.9492, 0.0254, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 67, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2035e-02, 2.3116e-02, 6.6199e-07, 1.7160e-02, 9.2478e-01, 9.0964e-07,
        1.2908e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 67, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2771e-01, 5.8204e-02, 5.6651e-06, 6.4472e-02, 2.0497e-06, 2.8634e-06,
        4.4960e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 68, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.9573, 0.0061, 0.0165, 0.0050, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 68, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5698, 0.0050, 0.0054, 0.0055, 0.2935, 0.0159, 0.1048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 68, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0053, 0.0049, 0.0053, 0.9505, 0.0243, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 68, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9638e-02, 1.8507e-02, 6.0305e-07, 1.3393e-02, 9.3931e-01, 8.3900e-07,
        9.1492e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 68, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0025e-01, 5.3104e-02, 7.2027e-06, 6.8485e-02, 2.4215e-06, 3.0269e-06,
        4.7815e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 69, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.9498, 0.0068, 0.0209, 0.0058, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 69, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5624, 0.0050, 0.0056, 0.0054, 0.2998, 0.0162, 0.1056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 69, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0059, 0.0056, 0.9453, 0.0275, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 69, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2705e-02, 2.0771e-02, 7.4682e-07, 2.1166e-02, 9.2392e-01, 9.2943e-07,
        1.1437e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 69, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3731e-01, 6.1767e-02, 5.3849e-06, 5.8532e-02, 1.8998e-06, 2.4502e-06,
        4.4238e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 70, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.9579, 0.0066, 0.0150, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 70, batch: 76/192] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5687, 0.0053, 0.0056, 0.0056, 0.2930, 0.0167, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 70, batch: 114/192] total loss per batch: 0.679
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0051, 0.0062, 0.9473, 0.0262, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 70, batch: 152/192] total loss per batch: 0.689
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.6659e-02, 1.8743e-02, 5.1887e-07, 9.7173e-03, 9.4595e-01, 6.9243e-07,
        8.9250e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 70, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.8786e-01, 5.1707e-02, 6.3469e-06, 7.8435e-02, 2.9146e-06, 3.2108e-06,
        4.8198e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.001

[Epoch: 71, batch: 38/192] total loss per batch: 0.684
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0038, 0.0041, 0.9545, 0.0072, 0.0184, 0.0048, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 71, batch: 76/192] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5648, 0.0051, 0.0052, 0.0051, 0.2997, 0.0154, 0.1047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 71, batch: 114/192] total loss per batch: 0.679
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0052, 0.0053, 0.0061, 0.0056, 0.9450, 0.0277, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 71, batch: 152/192] total loss per batch: 0.688
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0366e-02, 2.1034e-02, 1.0808e-06, 2.0043e-02, 9.2788e-01, 1.0555e-06,
        1.0672e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 71, batch: 190/192] total loss per batch: 0.676
Policy (actual, predicted): 6 0
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.4650e-01, 6.1252e-02, 7.2812e-06, 6.1470e-02, 2.9561e-06, 3.8434e-06,
        4.3077e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 72, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0058, 0.0058, 0.9499, 0.0077, 0.0179, 0.0061, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 -0.000

[Epoch: 72, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5669, 0.0054, 0.0055, 0.0058, 0.2958, 0.0163, 0.1044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 72, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0052, 0.0053, 0.9520, 0.0239, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 72, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.6574e-02, 1.9206e-02, 5.8526e-07, 1.3549e-02, 9.4130e-01, 6.4023e-07,
        9.3657e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 72, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0037e-01, 5.6820e-02, 5.6727e-06, 7.1010e-02, 2.0436e-06, 3.0344e-06,
        4.7179e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 73, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0054, 0.9493, 0.0077, 0.0197, 0.0056, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 73, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5641, 0.0053, 0.0056, 0.0057, 0.2978, 0.0161, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 73, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0043, 0.0052, 0.0051, 0.9522, 0.0240, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 73, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9583e-02, 2.0248e-02, 7.2408e-07, 1.5651e-02, 9.3357e-01, 7.5009e-07,
        1.0948e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 73, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2136e-01, 5.0965e-02, 5.2838e-06, 6.4434e-02, 1.9884e-06, 3.2605e-06,
        4.6323e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 74, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.9546, 0.0066, 0.0172, 0.0054, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 74, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5640, 0.0052, 0.0055, 0.0056, 0.2981, 0.0162, 0.1054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 74, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.0050, 0.0052, 0.9524, 0.0241, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 74, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9516e-02, 2.0702e-02, 6.2226e-07, 1.5650e-02, 9.3304e-01, 6.8315e-07,
        1.1088e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 74, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1271e-01, 5.3870e-02, 5.1083e-06, 7.1577e-02, 1.8869e-06, 2.8938e-06,
        4.6184e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 75, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.9530, 0.0068, 0.0182, 0.0057, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 75, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5637, 0.0050, 0.0056, 0.0056, 0.2990, 0.0160, 0.1051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 75, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.0050, 0.0054, 0.9514, 0.0245, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 75, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8668e-02, 2.0499e-02, 6.2740e-07, 1.5392e-02, 9.3499e-01, 6.6891e-07,
        1.0454e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 75, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1718e-01, 5.4651e-02, 4.3461e-06, 6.7868e-02, 1.7694e-06, 2.7138e-06,
        4.6029e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 76, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0046, 0.9556, 0.0063, 0.0178, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 76, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5656, 0.0050, 0.0054, 0.0055, 0.2979, 0.0160, 0.1046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 76, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0055, 0.0058, 0.9487, 0.0254, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 76, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9334e-02, 1.9912e-02, 5.6329e-07, 1.5558e-02, 9.3552e-01, 5.7639e-07,
        9.6763e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 76, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1962e-01, 5.6691e-02, 4.6053e-06, 6.8509e-02, 1.6003e-06, 2.7641e-06,
        4.5517e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 77, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9533, 0.0065, 0.0185, 0.0056, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 77, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5657, 0.0049, 0.0055, 0.0055, 0.2985, 0.0157, 0.1041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 77, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0052, 0.0060, 0.9480, 0.0256, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 77, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9324e-02, 2.0112e-02, 5.1143e-07, 1.4792e-02, 9.3561e-01, 5.4811e-07,
        1.0157e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 77, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0740e-01, 5.8789e-02, 4.1881e-06, 7.3625e-02, 1.7598e-06, 2.4917e-06,
        4.6018e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 78, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.9548, 0.0064, 0.0185, 0.0049, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 78, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5635, 0.0051, 0.0054, 0.0055, 0.2996, 0.0161, 0.1049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 78, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0053, 0.0058, 0.0060, 0.9459, 0.0267, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 78, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8480e-02, 2.0947e-02, 4.3694e-07, 1.5278e-02, 9.3546e-01, 5.4143e-07,
        9.8294e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 78, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2130e-01, 5.6025e-02, 4.3187e-06, 7.1288e-02, 1.6327e-06, 2.5149e-06,
        4.5138e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 79, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0056, 0.9476, 0.0073, 0.0211, 0.0062, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 79, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5643, 0.0049, 0.0054, 0.0056, 0.2993, 0.0159, 0.1046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 79, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.0052, 0.0056, 0.9493, 0.0253, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 79, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9887e-02, 1.9040e-02, 3.4491e-07, 1.4377e-02, 9.3809e-01, 4.5343e-07,
        8.6099e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 79, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([3.9165e-01, 5.8011e-02, 5.4163e-06, 7.1061e-02, 1.6069e-06, 2.7164e-06,
        4.7927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 80, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.9587, 0.0061, 0.0160, 0.0046, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 80, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5651, 0.0052, 0.0055, 0.0056, 0.2963, 0.0162, 0.1061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 80, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.0052, 0.0055, 0.9504, 0.0242, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 80, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9133e-02, 2.1282e-02, 4.6564e-07, 1.3789e-02, 9.3363e-01, 6.2385e-07,
        1.2162e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 80, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1516e-01, 5.3575e-02, 4.5536e-06, 7.2776e-02, 2.2108e-06, 1.9567e-06,
        4.5848e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 81, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.9518, 0.0063, 0.0190, 0.0057, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 81, batch: 76/192] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5624, 0.0050, 0.0055, 0.0057, 0.3008, 0.0160, 0.1047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 81, batch: 114/192] total loss per batch: 0.678
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0052, 0.0046, 0.0055, 0.0057, 0.9474, 0.0268, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 81, batch: 152/192] total loss per batch: 0.687
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.2657e-02, 2.1622e-02, 4.9451e-07, 1.7451e-02, 9.2872e-01, 5.9264e-07,
        9.5480e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 81, batch: 190/192] total loss per batch: 0.675
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2412e-01, 5.4414e-02, 5.7751e-06, 6.2746e-02, 1.5430e-06, 2.4240e-06,
        4.5871e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 82, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.9593, 0.0058, 0.0161, 0.0044, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 82, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5657, 0.0051, 0.0055, 0.0056, 0.2968, 0.0161, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 82, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0045, 0.0055, 0.0054, 0.9516, 0.0237, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 82, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1482e-02, 2.1399e-02, 6.0922e-07, 1.6513e-02, 9.2760e-01, 9.0749e-07,
        1.3007e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 82, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2326e-01, 5.2715e-02, 4.0136e-06, 6.6655e-02, 1.7997e-06, 1.9774e-06,
        4.5736e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 83, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0046, 0.9537, 0.0061, 0.0197, 0.0051, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 83, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5628, 0.0050, 0.0055, 0.0056, 0.3003, 0.0159, 0.1050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 83, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0051, 0.0055, 0.9504, 0.0245, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 83, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0358e-02, 2.0256e-02, 5.2841e-07, 1.5215e-02, 9.3392e-01, 6.1959e-07,
        1.0249e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 83, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2148e-01, 5.4179e-02, 3.9963e-06, 6.5620e-02, 1.4884e-06, 1.8827e-06,
        4.5872e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 84, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9538, 0.0063, 0.0190, 0.0050, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 84, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5635, 0.0051, 0.0054, 0.0056, 0.2992, 0.0161, 0.1052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 84, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0055, 0.0057, 0.9484, 0.0253, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 84, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9250e-02, 1.9195e-02, 4.4653e-07, 1.5096e-02, 9.3666e-01, 5.8929e-07,
        9.7968e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 84, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1289e-01, 5.5776e-02, 3.5484e-06, 6.8599e-02, 1.4177e-06, 2.0112e-06,
        4.6272e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 85, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.9489, 0.0068, 0.0213, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 85, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5624, 0.0050, 0.0055, 0.0056, 0.3002, 0.0161, 0.1053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 85, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.0053, 0.0055, 0.9502, 0.0244, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 85, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9385e-02, 1.9558e-02, 4.0325e-07, 1.4432e-02, 9.3686e-01, 4.7621e-07,
        9.7600e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 85, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1046e-01, 5.5836e-02, 3.5852e-06, 7.1835e-02, 1.4038e-06, 2.0622e-06,
        4.6187e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 86, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.9523, 0.0068, 0.0189, 0.0054, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 86, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5618, 0.0051, 0.0055, 0.0056, 0.3005, 0.0162, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 86, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.0055, 0.0056, 0.9480, 0.0260, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 86, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8788e-02, 1.9977e-02, 3.4996e-07, 1.5042e-02, 9.3687e-01, 4.5578e-07,
        9.3246e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 86, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1218e-01, 5.7336e-02, 3.6663e-06, 7.3285e-02, 1.2838e-06, 1.9336e-06,
        4.5720e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 87, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.9536, 0.0063, 0.0191, 0.0053, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 87, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5631, 0.0048, 0.0055, 0.0056, 0.3003, 0.0160, 0.1047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 87, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0051, 0.0051, 0.9522, 0.0237, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 87, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0519e-02, 1.9211e-02, 3.5988e-07, 1.4828e-02, 9.3529e-01, 4.3639e-07,
        1.0148e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 87, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1793e-01, 5.4497e-02, 3.9220e-06, 7.0029e-02, 1.4035e-06, 1.9186e-06,
        4.5753e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 88, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.9574, 0.0060, 0.0173, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 88, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5631, 0.0050, 0.0054, 0.0055, 0.3002, 0.0161, 0.1046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 88, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0045, 0.0054, 0.0056, 0.9486, 0.0259, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 88, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1675e-02, 2.2373e-02, 3.5930e-07, 1.6512e-02, 9.2921e-01, 4.9551e-07,
        1.0234e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 88, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1212e-01, 6.1233e-02, 3.5870e-06, 7.6141e-02, 1.3569e-06, 1.9313e-06,
        4.5050e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 89, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0042, 0.9570, 0.0056, 0.0190, 0.0044, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 89, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5670, 0.0046, 0.0052, 0.0053, 0.2984, 0.0155, 0.1039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 89, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0046, 0.0041, 0.0046, 0.0051, 0.9540, 0.0230, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 89, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8780e-02, 1.7657e-02, 3.7831e-07, 1.3177e-02, 9.4067e-01, 4.8953e-07,
        9.7177e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 89, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2299e-01, 5.0296e-02, 4.1124e-06, 6.4281e-02, 1.2259e-06, 1.7199e-06,
        4.6243e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 90, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0040, 0.9605, 0.0050, 0.0172, 0.0042, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 90, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5608, 0.0051, 0.0053, 0.0056, 0.3025, 0.0161, 0.1046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 90, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0059, 0.0061, 0.9445, 0.0275, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 90, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.4002e-02, 2.3319e-02, 5.1322e-07, 1.8233e-02, 9.2292e-01, 5.5544e-07,
        1.1522e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 90, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0057e-01, 6.0396e-02, 3.5011e-06, 7.4147e-02, 1.6235e-06, 2.1460e-06,
        4.6488e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 91, batch: 38/192] total loss per batch: 0.683
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0047, 0.9539, 0.0058, 0.0203, 0.0045, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 91, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5661, 0.0051, 0.0054, 0.0055, 0.2970, 0.0157, 0.1051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 91, batch: 114/192] total loss per batch: 0.677
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0042, 0.0052, 0.0055, 0.9513, 0.0241, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 91, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.5836e-02, 1.6535e-02, 2.9286e-07, 1.1696e-02, 9.4775e-01, 4.2456e-07,
        8.1854e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 91, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.2435e-01, 4.8907e-02, 3.7227e-06, 6.7732e-02, 1.2409e-06, 1.8395e-06,
        4.5901e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 92, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.9528, 0.0062, 0.0199, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 92, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5627, 0.0054, 0.0055, 0.0057, 0.2981, 0.0163, 0.1063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 92, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.0054, 0.0054, 0.9497, 0.0251, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 92, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0340e-02, 1.9230e-02, 3.9670e-07, 1.5199e-02, 9.3490e-01, 4.6475e-07,
        1.0333e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.000

[Epoch: 92, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1501e-01, 5.5255e-02, 3.6389e-06, 6.9020e-02, 1.3194e-06, 2.0337e-06,
        4.6071e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 93, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.9508, 0.0064, 0.0205, 0.0056, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 93, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5647, 0.0051, 0.0054, 0.0056, 0.2977, 0.0160, 0.1055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 93, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.0052, 0.0053, 0.9511, 0.0246, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 93, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9471e-02, 2.0159e-02, 3.6024e-07, 1.4653e-02, 9.3595e-01, 4.2984e-07,
        9.7696e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 93, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1547e-01, 5.2409e-02, 3.5513e-06, 6.7269e-02, 1.2569e-06, 1.9430e-06,
        4.6484e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 94, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.9552, 0.0059, 0.0185, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 94, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5647, 0.0050, 0.0055, 0.0055, 0.2987, 0.0160, 0.1047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 94, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.0053, 0.0055, 0.9495, 0.0252, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 94, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0610e-02, 2.0459e-02, 3.5491e-07, 1.5666e-02, 9.3301e-01, 4.5755e-07,
        1.0252e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 94, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1701e-01, 5.4197e-02, 3.3937e-06, 6.9400e-02, 1.2595e-06, 1.8266e-06,
        4.5939e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 95, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.9548, 0.0059, 0.0193, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 95, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5657, 0.0049, 0.0054, 0.0054, 0.2985, 0.0158, 0.1043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 95, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.0053, 0.0055, 0.9491, 0.0252, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 95, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.0258e-02, 2.0241e-02, 3.2555e-07, 1.4902e-02, 9.3458e-01, 4.1598e-07,
        1.0022e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 95, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1356e-01, 5.5848e-02, 3.1851e-06, 7.0158e-02, 1.1968e-06, 1.7296e-06,
        4.6043e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 96, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.9553, 0.0057, 0.0191, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 96, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5653, 0.0049, 0.0054, 0.0054, 0.2990, 0.0159, 0.1042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 96, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0054, 0.0057, 0.9487, 0.0254, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 96, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9623e-02, 2.0030e-02, 3.1690e-07, 1.5020e-02, 9.3576e-01, 4.2340e-07,
        9.5687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 96, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1232e-01, 5.6096e-02, 3.1658e-06, 7.2310e-02, 1.1813e-06, 1.6581e-06,
        4.5927e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 97, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.9531, 0.0061, 0.0203, 0.0052, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 97, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5646, 0.0049, 0.0053, 0.0054, 0.2994, 0.0158, 0.1046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 97, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0053, 0.0055, 0.9491, 0.0254, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 97, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9916e-02, 2.0138e-02, 2.8645e-07, 1.4087e-02, 9.3596e-01, 3.6474e-07,
        9.8943e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 97, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1229e-01, 5.7859e-02, 2.8515e-06, 7.2221e-02, 1.0822e-06, 1.5746e-06,
        4.5762e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 98, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.9553, 0.0056, 0.0193, 0.0048, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 98, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5640, 0.0049, 0.0053, 0.0055, 0.3000, 0.0159, 0.1042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 98, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.0053, 0.0055, 0.9500, 0.0249, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 98, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.9827e-02, 1.9865e-02, 3.0921e-07, 1.5368e-02, 9.3543e-01, 3.9574e-07,
        9.5133e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 98, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.0847e-01, 5.6902e-02, 2.9836e-06, 7.3789e-02, 1.0396e-06, 1.4850e-06,
        4.6083e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 99, batch: 38/192] total loss per batch: 0.681
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.9531, 0.0061, 0.0198, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 99, batch: 76/192] total loss per batch: 0.690
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5636, 0.0050, 0.0053, 0.0055, 0.2998, 0.0159, 0.1048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 0.000

[Epoch: 99, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.0050, 0.0052, 0.9517, 0.0244, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 99, batch: 152/192] total loss per batch: 0.685
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([2.1042e-02, 2.1467e-02, 2.7966e-07, 1.4415e-02, 9.3234e-01, 3.8081e-07,
        1.0734e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 99, batch: 190/192] total loss per batch: 0.673
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.1403e-01, 5.4756e-02, 2.4774e-06, 6.8208e-02, 9.8399e-07, 1.4008e-06,
        4.6300e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

[Epoch: 100, batch: 38/192] total loss per batch: 0.682
Policy (actual, predicted): 2 2
Policy data: tensor([0.0050, 0.0050, 0.9550, 0.0050, 0.0200, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0050, 0.9559, 0.0056, 0.0184, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.000 0.000

[Epoch: 100, batch: 76/192] total loss per batch: 0.691
Policy (actual, predicted): 0 0
Policy data: tensor([0.5650, 0.0050, 0.0050, 0.0050, 0.3000, 0.0150, 0.1050])
Policy pred: tensor([0.5654, 0.0048, 0.0052, 0.0055, 0.2989, 0.0158, 0.1044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.000

[Epoch: 100, batch: 114/192] total loss per batch: 0.676
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9500, 0.0250, 0.0050])
Policy pred: tensor([0.0049, 0.0043, 0.0050, 0.0050, 0.9525, 0.0239, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.004 -0.001

[Epoch: 100, batch: 152/192] total loss per batch: 0.686
Policy (actual, predicted): 4 4
Policy data: tensor([0.0200, 0.0200, 0.0000, 0.0150, 0.9350, 0.0000, 0.0100])
Policy pred: tensor([1.8974e-02, 1.7976e-02, 3.2582e-07, 1.5189e-02, 9.3806e-01, 4.3137e-07,
        9.8002e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.001 -0.001

[Epoch: 100, batch: 190/192] total loss per batch: 0.674
Policy (actual, predicted): 6 6
Policy data: tensor([0.4150, 0.0550, 0.0000, 0.0700, 0.0000, 0.0000, 0.4600])
Policy pred: tensor([4.3122e-01, 5.5711e-02, 3.3737e-06, 7.1472e-02, 1.0540e-06, 1.4287e-06,
        4.4159e-01], grad_fn=<SelectBackward>)Value (actual, predicted): 0.050 0.000

