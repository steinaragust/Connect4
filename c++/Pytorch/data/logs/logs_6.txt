Training set samples: 6269
Batch size: 32
[Epoch: 1, batch: 39/196] total loss per batch: 0.911
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0057, 0.0144, 0.0228, 0.9379, 0.0058, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 1, batch: 78/196] total loss per batch: 0.891
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0046, 0.0285, 0.0212, 0.0263, 0.8973, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.008

[Epoch: 1, batch: 117/196] total loss per batch: 0.853
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0030, 0.0021, 0.5696, 0.0059, 0.1958, 0.2194, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.005

[Epoch: 1, batch: 156/196] total loss per batch: 0.835
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0035, 0.0056, 0.1332, 0.0102, 0.0088, 0.8313, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.001

[Epoch: 1, batch: 195/196] total loss per batch: 0.846
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1319, 0.8348, 0.0072, 0.0031, 0.0049, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 2, batch: 39/196] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0068, 0.0031, 0.0021, 0.0080, 0.9686, 0.0036, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 2, batch: 78/196] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0042, 0.0018, 0.0373, 0.0102, 0.0473, 0.8468, 0.0524],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.003

[Epoch: 2, batch: 117/196] total loss per batch: 0.683
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0029, 0.5327, 0.0074, 0.1105, 0.3366, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.005

[Epoch: 2, batch: 156/196] total loss per batch: 0.662
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0025, 0.0052, 0.0439, 0.0036, 0.0077, 0.9319, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.002

[Epoch: 2, batch: 195/196] total loss per batch: 0.671
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.1367, 0.8323, 0.0050, 0.0049, 0.0063, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 3, batch: 39/196] total loss per batch: 0.645
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0024, 0.0041, 0.0082, 0.9704, 0.0038, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 3, batch: 78/196] total loss per batch: 0.632
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0022, 0.0029, 0.0503, 0.0050, 0.0427, 0.8517, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 3, batch: 117/196] total loss per batch: 0.627
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0089, 0.0061, 0.3668, 0.0127, 0.1206, 0.4791, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.007

[Epoch: 3, batch: 156/196] total loss per batch: 0.615
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0026, 0.0067, 0.1744, 0.0057, 0.0044, 0.8019, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 3, batch: 195/196] total loss per batch: 0.621
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0089, 0.1372, 0.8337, 0.0045, 0.0043, 0.0054, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 4, batch: 39/196] total loss per batch: 0.622
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([4.7727e-03, 7.7206e-04, 4.1888e-03, 4.2496e-03, 9.7977e-01, 3.6708e-03,
        2.5741e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 4, batch: 78/196] total loss per batch: 0.605
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0027, 0.0015, 0.1628, 0.0090, 0.0422, 0.7202, 0.0616],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 4, batch: 117/196] total loss per batch: 0.603
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0078, 0.0061, 0.2796, 0.0106, 0.0694, 0.6207, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.005

[Epoch: 4, batch: 156/196] total loss per batch: 0.603
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0025, 0.0047, 0.1259, 0.0096, 0.0058, 0.8452, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.002

[Epoch: 4, batch: 195/196] total loss per batch: 0.606
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.1487, 0.8203, 0.0049, 0.0054, 0.0046, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 5, batch: 39/196] total loss per batch: 0.613
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0020, 0.0033, 0.0118, 0.9686, 0.0042, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 5, batch: 78/196] total loss per batch: 0.600
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0013, 0.0017, 0.0176, 0.0071, 0.0084, 0.9314, 0.0325],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 5, batch: 117/196] total loss per batch: 0.592
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0046, 0.8076, 0.0052, 0.0323, 0.1395, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.003

[Epoch: 5, batch: 156/196] total loss per batch: 0.594
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0029, 0.0046, 0.1526, 0.0081, 0.0058, 0.8210, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 5, batch: 195/196] total loss per batch: 0.599
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.1376, 0.8364, 0.0041, 0.0033, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 6, batch: 39/196] total loss per batch: 0.602
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0072, 0.0023, 0.0025, 0.0117, 0.9670, 0.0054, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 6, batch: 78/196] total loss per batch: 0.588
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0029, 0.0011, 0.0472, 0.0069, 0.0140, 0.8248, 0.1031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 6, batch: 117/196] total loss per batch: 0.585
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0065, 0.0068, 0.1245, 0.0083, 0.0414, 0.8067, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.003

[Epoch: 6, batch: 156/196] total loss per batch: 0.582
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0079, 0.1278, 0.0115, 0.0070, 0.8300, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.001

[Epoch: 6, batch: 195/196] total loss per batch: 0.590
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.1460, 0.8248, 0.0041, 0.0046, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 7, batch: 39/196] total loss per batch: 0.594
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0018, 0.0045, 0.0066, 0.9741, 0.0044, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 7, batch: 78/196] total loss per batch: 0.580
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0065, 0.0016, 0.0204, 0.0255, 0.0282, 0.8445, 0.0734],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 7, batch: 117/196] total loss per batch: 0.582
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0086, 0.0066, 0.6758, 0.0048, 0.0777, 0.2189, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 7, batch: 156/196] total loss per batch: 0.575
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0043, 0.0043, 0.2063, 0.0092, 0.0073, 0.7626, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 7, batch: 195/196] total loss per batch: 0.585
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.1382, 0.8336, 0.0044, 0.0043, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 8, batch: 39/196] total loss per batch: 0.589
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0021, 0.0018, 0.0064, 0.9753, 0.0049, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 8, batch: 78/196] total loss per batch: 0.576
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0011, 0.0014, 0.0636, 0.0034, 0.0077, 0.8691, 0.0537],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 8, batch: 117/196] total loss per batch: 0.579
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0124, 0.0088, 0.2555, 0.0051, 0.0330, 0.6801, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 8, batch: 156/196] total loss per batch: 0.573
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0027, 0.0087, 0.0559, 0.0080, 0.0041, 0.9125, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.000

[Epoch: 8, batch: 195/196] total loss per batch: 0.578
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1564, 0.8163, 0.0035, 0.0038, 0.0053, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 9, batch: 39/196] total loss per batch: 0.588
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0022, 0.0058, 0.0063, 0.9751, 0.0029, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 9, batch: 78/196] total loss per batch: 0.571
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0035, 0.0017, 0.0091, 0.0053, 0.0085, 0.9490, 0.0229],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 9, batch: 117/196] total loss per batch: 0.574
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0074, 0.0081, 0.5708, 0.0079, 0.0864, 0.3140, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 9, batch: 156/196] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0058, 0.0056, 0.1813, 0.0084, 0.0065, 0.7848, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 9, batch: 195/196] total loss per batch: 0.574
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.1322, 0.8370, 0.0046, 0.0048, 0.0058, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 10, batch: 39/196] total loss per batch: 0.584
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0040, 0.0031, 0.0093, 0.9675, 0.0057, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 10, batch: 78/196] total loss per batch: 0.568
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0053, 0.0213, 0.0079, 0.0075, 0.8525, 0.1009],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 10, batch: 117/196] total loss per batch: 0.572
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0081, 0.0061, 0.2709, 0.0092, 0.0530, 0.6482, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.004

[Epoch: 10, batch: 156/196] total loss per batch: 0.564
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0080, 0.1679, 0.0095, 0.0065, 0.7961, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 10, batch: 195/196] total loss per batch: 0.572
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.1451, 0.8268, 0.0045, 0.0037, 0.0053, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 11, batch: 39/196] total loss per batch: 0.579
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0041, 0.0047, 0.0065, 0.9739, 0.0028, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 11, batch: 78/196] total loss per batch: 0.567
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0023, 0.0018, 0.0106, 0.0042, 0.0076, 0.9193, 0.0543],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 11, batch: 117/196] total loss per batch: 0.567
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0085, 0.0062, 0.6087, 0.0093, 0.0771, 0.2843, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.003

[Epoch: 11, batch: 156/196] total loss per batch: 0.560
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0030, 0.0041, 0.1234, 0.0067, 0.0044, 0.8525, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 11, batch: 195/196] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0104, 0.1355, 0.8300, 0.0054, 0.0057, 0.0066, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 12, batch: 39/196] total loss per batch: 0.574
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0043, 0.0021, 0.0051, 0.9775, 0.0030, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 12, batch: 78/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0036, 0.0021, 0.0180, 0.0039, 0.0082, 0.9005, 0.0638],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 12, batch: 117/196] total loss per batch: 0.560
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0074, 0.0069, 0.4189, 0.0083, 0.0689, 0.4843, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.003

[Epoch: 12, batch: 156/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0059, 0.1648, 0.0084, 0.0049, 0.8037, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 12, batch: 195/196] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1396, 0.8284, 0.0051, 0.0051, 0.0065, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 13, batch: 39/196] total loss per batch: 0.570
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.0034, 0.0041, 0.9756, 0.0038, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 13, batch: 78/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0036, 0.0021, 0.0132, 0.0054, 0.0071, 0.8866, 0.0821],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 13, batch: 117/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0087, 0.0061, 0.4039, 0.0066, 0.0735, 0.4961, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 13, batch: 156/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0030, 0.0047, 0.1239, 0.0050, 0.0040, 0.8532, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 13, batch: 195/196] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0108, 0.1385, 0.8271, 0.0054, 0.0055, 0.0066, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.000

[Epoch: 14, batch: 39/196] total loss per batch: 0.568
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0038, 0.0026, 0.0046, 0.9775, 0.0037, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 14, batch: 78/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0029, 0.0025, 0.0102, 0.0036, 0.0071, 0.9028, 0.0708],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 14, batch: 117/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0096, 0.0070, 0.4497, 0.0093, 0.0628, 0.4568, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 14, batch: 156/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0042, 0.0063, 0.1662, 0.0059, 0.0053, 0.8041, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.003

[Epoch: 14, batch: 195/196] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1401, 0.8284, 0.0052, 0.0051, 0.0059, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 15, batch: 39/196] total loss per batch: 0.567
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0039, 0.0029, 0.0048, 0.9757, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 15, batch: 78/196] total loss per batch: 0.555
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0034, 0.0024, 0.0124, 0.0042, 0.0075, 0.9013, 0.0688],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 15, batch: 117/196] total loss per batch: 0.559
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0080, 0.0054, 0.4321, 0.0076, 0.0635, 0.4792, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 15, batch: 156/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0038, 0.0056, 0.1209, 0.0068, 0.0041, 0.8526, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 15, batch: 195/196] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1358, 0.8313, 0.0052, 0.0054, 0.0064, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 16, batch: 39/196] total loss per batch: 0.569
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0062, 0.0054, 0.0058, 0.0047, 0.9678, 0.0054, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 16, batch: 78/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0037, 0.0034, 0.0072, 0.0056, 0.0084, 0.9083, 0.0634],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.002

[Epoch: 16, batch: 117/196] total loss per batch: 0.560
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0082, 0.0074, 0.4663, 0.0063, 0.0752, 0.4319, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 16, batch: 156/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0034, 0.0062, 0.1998, 0.0062, 0.0060, 0.7712, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 16, batch: 195/196] total loss per batch: 0.561
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1417, 0.8283, 0.0049, 0.0049, 0.0054, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 17, batch: 39/196] total loss per batch: 0.571
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0035, 0.0024, 0.0058, 0.9740, 0.0049, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 17, batch: 78/196] total loss per batch: 0.557
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0041, 0.0042, 0.0149, 0.0048, 0.0068, 0.8921, 0.0730],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 17, batch: 117/196] total loss per batch: 0.562
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0073, 0.0055, 0.4090, 0.0072, 0.0583, 0.5095, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.003

[Epoch: 17, batch: 156/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0050, 0.1185, 0.0070, 0.0040, 0.8533, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 17, batch: 195/196] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0105, 0.1362, 0.8311, 0.0050, 0.0049, 0.0066, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 18, batch: 39/196] total loss per batch: 0.573
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0035, 0.0053, 0.0044, 0.9727, 0.0036, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 18, batch: 78/196] total loss per batch: 0.560
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0034, 0.0023, 0.0089, 0.0061, 0.0069, 0.9123, 0.0601],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 18, batch: 117/196] total loss per batch: 0.564
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0091, 0.0056, 0.4883, 0.0059, 0.0816, 0.4048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 18, batch: 156/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0039, 0.0048, 0.1209, 0.0050, 0.0047, 0.8551, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.002

[Epoch: 18, batch: 195/196] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1411, 0.8277, 0.0057, 0.0050, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 19, batch: 39/196] total loss per batch: 0.575
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0040, 0.0020, 0.0041, 0.9759, 0.0041, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 19, batch: 78/196] total loss per batch: 0.561
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0062, 0.0036, 0.0122, 0.0072, 0.0068, 0.8776, 0.0864],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 19, batch: 117/196] total loss per batch: 0.565
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0091, 0.0052, 0.4690, 0.0073, 0.0601, 0.4443, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.005

[Epoch: 19, batch: 156/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0066, 0.1745, 0.0098, 0.0056, 0.7885, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 19, batch: 195/196] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0112, 0.1444, 0.8205, 0.0059, 0.0059, 0.0071, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 20, batch: 39/196] total loss per batch: 0.576
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0036, 0.0025, 0.0061, 0.9700, 0.0062, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 20, batch: 78/196] total loss per batch: 0.562
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0036, 0.0034, 0.0092, 0.0069, 0.0085, 0.9193, 0.0491],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 20, batch: 117/196] total loss per batch: 0.566
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0071, 0.0069, 0.4223, 0.0069, 0.0613, 0.4903, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 20, batch: 156/196] total loss per batch: 0.560
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0048, 0.0752, 0.0059, 0.0059, 0.8987, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.003

[Epoch: 20, batch: 195/196] total loss per batch: 0.565
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1471, 0.8238, 0.0048, 0.0049, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 21, batch: 39/196] total loss per batch: 0.575
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0041, 0.0040, 0.0034, 0.0036, 0.9789, 0.0026, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 21, batch: 78/196] total loss per batch: 0.559
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0022, 0.0305, 0.0048, 0.0084, 0.8861, 0.0627],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 21, batch: 117/196] total loss per batch: 0.564
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0075, 0.0067, 0.3896, 0.0060, 0.0560, 0.5281, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 21, batch: 156/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0088, 0.2973, 0.0102, 0.0048, 0.6627, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.001

[Epoch: 21, batch: 195/196] total loss per batch: 0.563
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0115, 0.1443, 0.8225, 0.0060, 0.0052, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 22, batch: 39/196] total loss per batch: 0.569
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0039, 0.0046, 0.0034, 0.0052, 0.9734, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 22, batch: 78/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0044, 0.0036, 0.0072, 0.0057, 0.0064, 0.9101, 0.0627],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 22, batch: 117/196] total loss per batch: 0.558
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0073, 0.0055, 0.4578, 0.0066, 0.0899, 0.4274, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 22, batch: 156/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0036, 0.0050, 0.0864, 0.0060, 0.0046, 0.8868, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 22, batch: 195/196] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.1446, 0.8253, 0.0056, 0.0051, 0.0057, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 23, batch: 39/196] total loss per batch: 0.565
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0042, 0.0046, 0.0032, 0.0050, 0.9742, 0.0041, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 23, batch: 78/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0041, 0.0030, 0.0080, 0.0049, 0.0069, 0.8933, 0.0797],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 23, batch: 117/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0068, 0.0057, 0.4252, 0.0075, 0.0750, 0.4748, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 23, batch: 156/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0039, 0.0056, 0.1252, 0.0048, 0.0047, 0.8487, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 23, batch: 195/196] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.1434, 0.8246, 0.0055, 0.0050, 0.0059, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 24, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.0049, 0.0046, 0.9717, 0.0053, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 24, batch: 78/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0040, 0.0028, 0.0094, 0.0036, 0.0040, 0.9217, 0.0545],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 24, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0072, 0.0049, 0.4469, 0.0072, 0.0580, 0.4709, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 24, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0059, 0.1689, 0.0057, 0.0043, 0.8040, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 24, batch: 195/196] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1414, 0.8280, 0.0053, 0.0049, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 25, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0060, 0.0035, 0.0054, 0.9706, 0.0043, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 25, batch: 78/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0062, 0.0052, 0.0070, 0.0061, 0.0070, 0.8744, 0.0941],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 25, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0061, 0.0046, 0.4251, 0.0070, 0.0754, 0.4783, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 25, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0034, 0.0046, 0.1404, 0.0047, 0.0033, 0.8374, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 25, batch: 195/196] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0091, 0.1414, 0.8294, 0.0053, 0.0048, 0.0052, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 26, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0049, 0.0044, 0.0055, 0.9698, 0.0058, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 26, batch: 78/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0047, 0.0036, 0.0083, 0.0047, 0.0050, 0.9173, 0.0565],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 26, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0073, 0.0047, 0.4447, 0.0053, 0.0675, 0.4663, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 26, batch: 156/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0043, 0.0054, 0.1442, 0.0058, 0.0049, 0.8285, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.000

[Epoch: 26, batch: 195/196] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0090, 0.1411, 0.8318, 0.0042, 0.0043, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 27, batch: 39/196] total loss per batch: 0.565
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0045, 0.0035, 0.0055, 0.9728, 0.0039, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 27, batch: 78/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0059, 0.0042, 0.0063, 0.0045, 0.0062, 0.8959, 0.0770],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 27, batch: 117/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0066, 0.0051, 0.4099, 0.0079, 0.0756, 0.4900, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 27, batch: 156/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0040, 0.0056, 0.1612, 0.0043, 0.0036, 0.8150, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 27, batch: 195/196] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1404, 0.8303, 0.0050, 0.0046, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 28, batch: 39/196] total loss per batch: 0.566
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0063, 0.0050, 0.0038, 0.0076, 0.9650, 0.0061, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 28, batch: 78/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0035, 0.0076, 0.0063, 0.0066, 0.8915, 0.0793],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.002

[Epoch: 28, batch: 117/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0065, 0.0041, 0.4492, 0.0050, 0.0679, 0.4636, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 28, batch: 156/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0051, 0.1317, 0.0047, 0.0057, 0.8427, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 28, batch: 195/196] total loss per batch: 0.557
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1422, 0.8267, 0.0054, 0.0050, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 29, batch: 39/196] total loss per batch: 0.568
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0051, 0.0056, 0.0051, 0.9697, 0.0047, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 29, batch: 78/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0072, 0.0046, 0.0073, 0.0066, 0.0085, 0.9069, 0.0589],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 29, batch: 117/196] total loss per batch: 0.559
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0067, 0.0049, 0.4702, 0.0063, 0.0440, 0.4630, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 29, batch: 156/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0062, 0.1907, 0.0054, 0.0038, 0.7826, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.000

[Epoch: 29, batch: 195/196] total loss per batch: 0.559
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.1395, 0.8349, 0.0043, 0.0039, 0.0039, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 30, batch: 39/196] total loss per batch: 0.570
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0079, 0.0040, 0.0047, 0.0057, 0.9670, 0.0046, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 30, batch: 78/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0034, 0.0038, 0.0154, 0.0034, 0.0036, 0.9301, 0.0403],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.002

[Epoch: 30, batch: 117/196] total loss per batch: 0.561
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0074, 0.0041, 0.3708, 0.0038, 0.0652, 0.5448, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 30, batch: 156/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0043, 0.0046, 0.1283, 0.0052, 0.0042, 0.8460, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 30, batch: 195/196] total loss per batch: 0.560
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.1355, 0.8308, 0.0055, 0.0053, 0.0064, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 31, batch: 39/196] total loss per batch: 0.571
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0044, 0.0053, 0.0031, 0.0033, 0.9739, 0.0049, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 31, batch: 78/196] total loss per batch: 0.558
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0043, 0.0095, 0.0041, 0.0059, 0.8586, 0.1124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 31, batch: 117/196] total loss per batch: 0.560
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0083, 0.0079, 0.4835, 0.0083, 0.1002, 0.3852, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 31, batch: 156/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0059, 0.0060, 0.1578, 0.0065, 0.0044, 0.8122, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 31, batch: 195/196] total loss per batch: 0.558
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1377, 0.8332, 0.0052, 0.0049, 0.0051, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 32, batch: 39/196] total loss per batch: 0.565
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0038, 0.0067, 0.0046, 0.9693, 0.0047, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 32, batch: 78/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0061, 0.0038, 0.0062, 0.0068, 0.0066, 0.9143, 0.0561],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 32, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0062, 0.0051, 0.3994, 0.0057, 0.0630, 0.5161, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 32, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0052, 0.1452, 0.0053, 0.0044, 0.8292, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 32, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1402, 0.8296, 0.0052, 0.0048, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 33, batch: 39/196] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0042, 0.0038, 0.0046, 0.9719, 0.0054, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 33, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0043, 0.0041, 0.0086, 0.0046, 0.0051, 0.8974, 0.0758],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 33, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0058, 0.0058, 0.4702, 0.0062, 0.0655, 0.4415, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 33, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.1490, 0.0046, 0.0038, 0.8264, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.002

[Epoch: 33, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1423, 0.8268, 0.0054, 0.0051, 0.0056, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 34, batch: 39/196] total loss per batch: 0.562
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0053, 0.0047, 0.0047, 0.9698, 0.0052, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 34, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0056, 0.0046, 0.0073, 0.0047, 0.0055, 0.9118, 0.0605],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 34, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0060, 0.0053, 0.4004, 0.0051, 0.0859, 0.4924, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 34, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.1495, 0.0046, 0.0040, 0.8266, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 34, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1367, 0.8333, 0.0051, 0.0048, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 35, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0040, 0.0046, 0.0045, 0.9721, 0.0046, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 35, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0050, 0.0051, 0.0044, 0.0048, 0.9012, 0.0744],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 35, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0052, 0.4700, 0.0048, 0.0549, 0.4552, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 35, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0043, 0.0050, 0.1471, 0.0054, 0.0046, 0.8266, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 35, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.1411, 0.8294, 0.0052, 0.0046, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 36, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0052, 0.0034, 0.0048, 0.9709, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 36, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0063, 0.0044, 0.0083, 0.0053, 0.0062, 0.9011, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 36, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0066, 0.0051, 0.4238, 0.0053, 0.0633, 0.4917, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 36, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.1515, 0.0044, 0.0049, 0.8232, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 36, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1412, 0.8281, 0.0050, 0.0051, 0.0056, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 37, batch: 39/196] total loss per batch: 0.562
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0066, 0.0050, 0.0094, 0.0067, 0.9594, 0.0063, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 37, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0043, 0.0044, 0.0043, 0.0038, 0.0053, 0.9110, 0.0668],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 37, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0066, 0.0054, 0.4222, 0.0060, 0.0971, 0.4568, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 37, batch: 156/196] total loss per batch: 0.545
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0052, 0.1506, 0.0065, 0.0046, 0.8225, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 37, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1397, 0.8304, 0.0055, 0.0048, 0.0054, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 38, batch: 39/196] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0060, 0.0030, 0.0043, 0.9718, 0.0042, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 38, batch: 78/196] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0059, 0.0047, 0.0080, 0.0042, 0.0052, 0.9036, 0.0684],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 38, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0046, 0.4468, 0.0049, 0.0486, 0.4861, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 38, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.1597, 0.0045, 0.0047, 0.8154, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 38, batch: 195/196] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1427, 0.8282, 0.0045, 0.0047, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 39, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0037, 0.0075, 0.0055, 0.9690, 0.0058, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.001

[Epoch: 39, batch: 78/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0045, 0.0039, 0.0068, 0.0047, 0.0051, 0.9005, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 39, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0053, 0.0064, 0.4788, 0.0063, 0.0715, 0.4266, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.002

[Epoch: 39, batch: 156/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.1429, 0.0054, 0.0049, 0.8325, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 39, batch: 195/196] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1386, 0.8305, 0.0050, 0.0050, 0.0057, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 40, batch: 39/196] total loss per batch: 0.566
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0062, 0.0043, 0.0062, 0.9671, 0.0045, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 40, batch: 78/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0049, 0.0062, 0.0045, 0.0059, 0.9133, 0.0600],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 40, batch: 117/196] total loss per batch: 0.556
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0069, 0.0043, 0.2875, 0.0060, 0.0701, 0.6214, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 40, batch: 156/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0061, 0.1367, 0.0048, 0.0046, 0.8372, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 40, batch: 195/196] total loss per batch: 0.556
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1458, 0.8226, 0.0053, 0.0053, 0.0057, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 41, batch: 39/196] total loss per batch: 0.566
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0044, 0.0042, 0.0045, 0.9720, 0.0052, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 41, batch: 78/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0035, 0.0082, 0.0071, 0.0077, 0.9014, 0.0676],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 41, batch: 117/196] total loss per batch: 0.557
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0050, 0.0053, 0.6608, 0.0052, 0.0722, 0.2459, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 41, batch: 156/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.1849, 0.0065, 0.0039, 0.7883, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 41, batch: 195/196] total loss per batch: 0.555
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.1389, 0.8316, 0.0050, 0.0048, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 42, batch: 39/196] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0047, 0.0035, 0.0041, 0.9731, 0.0041, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.009

[Epoch: 42, batch: 78/196] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0050, 0.0043, 0.0058, 0.0046, 0.0044, 0.8994, 0.0765],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 42, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0068, 0.0065, 0.3426, 0.0057, 0.0724, 0.5615, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 42, batch: 156/196] total loss per batch: 0.545
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0032, 0.0049, 0.1369, 0.0039, 0.0042, 0.8421, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 42, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.1368, 0.8329, 0.0050, 0.0050, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 43, batch: 39/196] total loss per batch: 0.562
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0045, 0.0050, 0.0046, 0.9703, 0.0053, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 43, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0044, 0.0045, 0.0058, 0.0043, 0.0053, 0.9137, 0.0619],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 43, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.4793, 0.0059, 0.0734, 0.4241, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 43, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0054, 0.1564, 0.0048, 0.0050, 0.8185, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 43, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0094, 0.1419, 0.8288, 0.0046, 0.0048, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 44, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0052, 0.0041, 0.0049, 0.9715, 0.0044, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 44, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0042, 0.0051, 0.0040, 0.0058, 0.8992, 0.0766],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 44, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0061, 0.0053, 0.4008, 0.0051, 0.0679, 0.5103, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 44, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0040, 0.0056, 0.1478, 0.0046, 0.0043, 0.8283, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 44, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0092, 0.1368, 0.8338, 0.0049, 0.0049, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 45, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0045, 0.0043, 0.0043, 0.0044, 0.9736, 0.0040, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 45, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0045, 0.0046, 0.0049, 0.0044, 0.0050, 0.9082, 0.0683],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 45, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0054, 0.4550, 0.0055, 0.0697, 0.4546, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 45, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0052, 0.1488, 0.0046, 0.0051, 0.8262, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 45, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1434, 0.8274, 0.0049, 0.0048, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 46, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.0054, 0.0057, 0.9678, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 46, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0054, 0.0059, 0.0053, 0.0067, 0.8990, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 46, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0062, 0.0050, 0.4365, 0.0057, 0.0834, 0.4584, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 46, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0056, 0.1559, 0.0051, 0.0044, 0.8186, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 46, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1405, 0.8280, 0.0052, 0.0051, 0.0058, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 47, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.0036, 0.0047, 0.9722, 0.0046, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 47, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0058, 0.0049, 0.0056, 0.0046, 0.0045, 0.9118, 0.0628],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 47, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0055, 0.3885, 0.0052, 0.0554, 0.5350, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 47, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0045, 0.0047, 0.1429, 0.0047, 0.0044, 0.8338, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 47, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0093, 0.1376, 0.8341, 0.0050, 0.0047, 0.0048, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 48, batch: 39/196] total loss per batch: 0.562
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0048, 0.0043, 0.0042, 0.0050, 0.9733, 0.0038, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 48, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0057, 0.0064, 0.0055, 0.0062, 0.8993, 0.0716],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.001

[Epoch: 48, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0062, 0.0057, 0.5252, 0.0061, 0.0816, 0.3697, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 48, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0054, 0.0066, 0.1793, 0.0058, 0.0056, 0.7907, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 48, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1461, 0.8223, 0.0050, 0.0051, 0.0055, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 49, batch: 39/196] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0055, 0.0049, 0.0057, 0.9663, 0.0061, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 49, batch: 78/196] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0058, 0.0040, 0.0065, 0.0062, 0.0052, 0.8909, 0.0814],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 49, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0061, 0.0058, 0.3516, 0.0054, 0.0531, 0.5730, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 49, batch: 156/196] total loss per batch: 0.545
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0040, 0.0037, 0.1165, 0.0030, 0.0045, 0.8636, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 49, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1418, 0.8268, 0.0052, 0.0052, 0.0054, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 50, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0064, 0.0043, 0.0026, 0.0039, 0.9743, 0.0034, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.013

[Epoch: 50, batch: 78/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0045, 0.0044, 0.0075, 0.0052, 0.0058, 0.9116, 0.0610],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 50, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0057, 0.0048, 0.5012, 0.0065, 0.0938, 0.3836, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 50, batch: 156/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0054, 0.1754, 0.0056, 0.0046, 0.7984, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 50, batch: 195/196] total loss per batch: 0.554
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.1458, 0.8260, 0.0048, 0.0047, 0.0045, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 51, batch: 39/196] total loss per batch: 0.564
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0037, 0.0041, 0.0060, 0.0041, 0.9730, 0.0048, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 51, batch: 78/196] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0050, 0.0039, 0.0047, 0.0051, 0.9127, 0.0635],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 51, batch: 117/196] total loss per batch: 0.555
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0046, 0.0051, 0.4129, 0.0045, 0.0427, 0.5264, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 51, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0038, 0.0036, 0.1408, 0.0032, 0.0046, 0.8391, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 51, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1404, 0.8293, 0.0050, 0.0048, 0.0056, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 52, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0049, 0.0043, 0.0044, 0.9716, 0.0046, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 52, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0047, 0.0051, 0.0053, 0.0045, 0.0046, 0.8918, 0.0840],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 52, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.4453, 0.0058, 0.0741, 0.4596, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 52, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0041, 0.0039, 0.1408, 0.0037, 0.0039, 0.8386, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 52, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0087, 0.1372, 0.8351, 0.0046, 0.0045, 0.0052, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 53, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0044, 0.0051, 0.0043, 0.9716, 0.0045, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 53, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0048, 0.0045, 0.0043, 0.0043, 0.0051, 0.9131, 0.0640],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 53, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0057, 0.0056, 0.4095, 0.0050, 0.0746, 0.4949, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 53, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0047, 0.1583, 0.0042, 0.0047, 0.8184, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 53, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1385, 0.8313, 0.0050, 0.0048, 0.0055, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 54, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0056, 0.0049, 0.9694, 0.0049, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 54, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0058, 0.0054, 0.0053, 0.0055, 0.0051, 0.8983, 0.0745],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 54, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0058, 0.0056, 0.4405, 0.0056, 0.0691, 0.4686, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 54, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0054, 0.0055, 0.1590, 0.0049, 0.0058, 0.8129, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 54, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.1410, 0.8292, 0.0049, 0.0049, 0.0053, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 55, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0059, 0.0052, 0.9674, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 55, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0057, 0.0057, 0.0057, 0.0057, 0.0059, 0.8989, 0.0724],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 55, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.4479, 0.0048, 0.0710, 0.4607, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 55, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0055, 0.0055, 0.1539, 0.0050, 0.0054, 0.8190, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 55, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1434, 0.8265, 0.0050, 0.0048, 0.0053, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 56, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0041, 0.0046, 0.9712, 0.0047, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 56, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0049, 0.0052, 0.0044, 0.0046, 0.9188, 0.0569],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 56, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0057, 0.0055, 0.4258, 0.0057, 0.0671, 0.4854, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 56, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.1456, 0.0049, 0.0052, 0.8278, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 56, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1418, 0.8287, 0.0049, 0.0048, 0.0052, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 57, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.0049, 0.0048, 0.9693, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 57, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0054, 0.0048, 0.0047, 0.0051, 0.8874, 0.0871],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 57, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.4266, 0.0046, 0.0682, 0.4865, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 57, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.1473, 0.0051, 0.0056, 0.8264, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 57, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1445, 0.8260, 0.0050, 0.0046, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 58, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0044, 0.0042, 0.0044, 0.9740, 0.0039, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 58, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0044, 0.0039, 0.0044, 0.0041, 0.0046, 0.9287, 0.0500],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 58, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.4622, 0.0061, 0.0604, 0.4554, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 58, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0045, 0.1540, 0.0041, 0.0045, 0.8230, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 58, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.1402, 0.8298, 0.0048, 0.0049, 0.0056, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 59, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0060, 0.0059, 0.0049, 0.0053, 0.9668, 0.0054, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 59, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0056, 0.0053, 0.0048, 0.0047, 0.0051, 0.8931, 0.0813],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 59, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0050, 0.0054, 0.3846, 0.0051, 0.1025, 0.4934, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 59, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.1396, 0.0038, 0.0053, 0.8370, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 59, batch: 195/196] total loss per batch: 0.552
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0088, 0.1375, 0.8360, 0.0045, 0.0042, 0.0045, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 60, batch: 39/196] total loss per batch: 0.562
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0043, 0.0038, 0.0059, 0.0040, 0.9732, 0.0040, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 60, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0049, 0.0034, 0.0039, 0.0040, 0.0051, 0.9143, 0.0644],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 0.000

[Epoch: 60, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.4756, 0.0046, 0.0515, 0.4526, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 60, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.1505, 0.0051, 0.0049, 0.8241, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 0.000

[Epoch: 60, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1374, 0.8315, 0.0048, 0.0050, 0.0056, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 61, batch: 39/196] total loss per batch: 0.563
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0046, 0.0041, 0.0047, 0.0052, 0.9726, 0.0042, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 61, batch: 78/196] total loss per batch: 0.550
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0057, 0.0054, 0.0066, 0.0065, 0.0061, 0.8959, 0.0738],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 61, batch: 117/196] total loss per batch: 0.554
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0059, 0.0055, 0.4452, 0.0069, 0.0741, 0.4574, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 61, batch: 156/196] total loss per batch: 0.546
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0065, 0.0062, 0.1661, 0.0060, 0.0066, 0.8006, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 61, batch: 195/196] total loss per batch: 0.553
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1402, 0.8298, 0.0050, 0.0050, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 62, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0042, 0.0058, 0.0045, 0.9694, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.007

[Epoch: 62, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0064, 0.0060, 0.0065, 0.0068, 0.0064, 0.8993, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 62, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0046, 0.4212, 0.0046, 0.0661, 0.4942, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 62, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.1481, 0.0053, 0.0050, 0.8268, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 62, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1437, 0.8251, 0.0052, 0.0050, 0.0054, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 63, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0049, 0.0047, 0.0050, 0.9708, 0.0047, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 63, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0055, 0.0060, 0.0055, 0.0054, 0.8911, 0.0811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 63, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0045, 0.0043, 0.4431, 0.0048, 0.0691, 0.4700, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 63, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0048, 0.1480, 0.0048, 0.0049, 0.8271, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 63, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1425, 0.8277, 0.0049, 0.0047, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 64, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0045, 0.0043, 0.0047, 0.9717, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 64, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0050, 0.0049, 0.0051, 0.0052, 0.0049, 0.9141, 0.0608],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 64, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.4186, 0.0046, 0.0672, 0.4964, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 64, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.1530, 0.0050, 0.0049, 0.8223, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 64, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1394, 0.8306, 0.0049, 0.0048, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 65, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0051, 0.0050, 0.9689, 0.0052, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 65, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0052, 0.0050, 0.0054, 0.0055, 0.8960, 0.0774],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 65, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.4425, 0.0052, 0.0752, 0.4631, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 65, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.1478, 0.0051, 0.0050, 0.8275, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 65, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1412, 0.8281, 0.0050, 0.0050, 0.0054, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 66, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0055, 0.0059, 0.0056, 0.9665, 0.0054, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 66, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0050, 0.0053, 0.0052, 0.0052, 0.9085, 0.0658],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 66, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0050, 0.0047, 0.4337, 0.0049, 0.0679, 0.4793, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 66, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0050, 0.1531, 0.0051, 0.0053, 0.8210, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 66, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1406, 0.8296, 0.0049, 0.0048, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 67, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0057, 0.0056, 0.0054, 0.0056, 0.9661, 0.0057, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 67, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0051, 0.0050, 0.0049, 0.0052, 0.9051, 0.0694],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 67, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.4428, 0.0053, 0.0732, 0.4638, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 67, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0050, 0.1547, 0.0049, 0.0049, 0.8204, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 67, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1421, 0.8280, 0.0049, 0.0048, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 68, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.0053, 0.0052, 0.9681, 0.0055, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 68, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0049, 0.0050, 0.0048, 0.0050, 0.9072, 0.0680],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 68, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.4083, 0.0052, 0.0688, 0.5023, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 68, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0047, 0.0048, 0.1368, 0.0046, 0.0049, 0.8392, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 68, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1403, 0.8302, 0.0048, 0.0048, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 69, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0057, 0.0054, 0.0056, 0.9666, 0.0056, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 69, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0052, 0.0049, 0.0048, 0.0050, 0.9025, 0.0723],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 69, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.4702, 0.0059, 0.0720, 0.4369, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 69, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0047, 0.1769, 0.0045, 0.0049, 0.7985, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 69, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1428, 0.8277, 0.0049, 0.0046, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 70, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0052, 0.0046, 0.0051, 0.9688, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 70, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0051, 0.0055, 0.0049, 0.0047, 0.9022, 0.0722],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 70, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0052, 0.0050, 0.4037, 0.0043, 0.0590, 0.5182, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 70, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0039, 0.0046, 0.1046, 0.0037, 0.0042, 0.8746, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.000

[Epoch: 70, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1415, 0.8289, 0.0048, 0.0048, 0.0052, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 71, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.0056, 0.0047, 0.9687, 0.0052, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 71, batch: 78/196] total loss per batch: 0.549
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0041, 0.0037, 0.0040, 0.0034, 0.0039, 0.9282, 0.0526],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 71, batch: 117/196] total loss per batch: 0.553
Policy (actual, predicted): 5 2
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0057, 0.0055, 0.4470, 0.0064, 0.0862, 0.4433, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 71, batch: 156/196] total loss per batch: 0.544
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.2006, 0.0048, 0.0048, 0.7738, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 71, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1384, 0.8320, 0.0050, 0.0047, 0.0051, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 72, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0048, 0.0046, 0.0052, 0.9689, 0.0056, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 72, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0048, 0.0056, 0.0057, 0.0054, 0.8870, 0.0862],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 72, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0050, 0.4407, 0.0051, 0.0708, 0.4681, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 72, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0043, 0.0048, 0.1481, 0.0041, 0.0043, 0.8297, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 72, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0097, 0.1383, 0.8323, 0.0048, 0.0047, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 73, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0057, 0.0046, 0.9690, 0.0051, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 73, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0044, 0.0045, 0.0043, 0.0049, 0.9142, 0.0630],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 73, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0052, 0.4373, 0.0054, 0.0695, 0.4723, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 73, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0050, 0.1560, 0.0043, 0.0044, 0.8209, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 73, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1406, 0.8300, 0.0048, 0.0047, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 74, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0046, 0.0050, 0.0047, 0.9699, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 74, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0048, 0.0053, 0.0051, 0.0055, 0.9056, 0.0686],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 74, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.4391, 0.0048, 0.0696, 0.4718, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 74, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0049, 0.1472, 0.0048, 0.0046, 0.8289, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 74, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1405, 0.8303, 0.0048, 0.0047, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 75, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.0047, 0.0046, 0.9706, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 75, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0050, 0.0047, 0.0048, 0.0049, 0.0050, 0.9019, 0.0737],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 75, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.4271, 0.0047, 0.0685, 0.4859, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 75, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0047, 0.0049, 0.1504, 0.0048, 0.0046, 0.8255, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 75, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1414, 0.8291, 0.0049, 0.0047, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 76, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0045, 0.0049, 0.0049, 0.9702, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 76, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0048, 0.0047, 0.0047, 0.0048, 0.0051, 0.9125, 0.0634],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 76, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0047, 0.0045, 0.4419, 0.0047, 0.0694, 0.4704, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 76, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.1521, 0.0055, 0.0048, 0.8227, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 76, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1412, 0.8295, 0.0049, 0.0047, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 77, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0047, 0.0048, 0.9695, 0.0051, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 77, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0050, 0.0048, 0.0049, 0.0050, 0.0050, 0.8953, 0.0800],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 77, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0051, 0.0045, 0.4300, 0.0049, 0.0712, 0.4798, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 77, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0051, 0.0055, 0.1497, 0.0056, 0.0052, 0.8232, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 77, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1424, 0.8280, 0.0049, 0.0048, 0.0051, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 78, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0062, 0.0052, 0.9675, 0.0056, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 78, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0046, 0.0049, 0.0049, 0.0051, 0.9174, 0.0584],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 78, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0045, 0.4425, 0.0049, 0.0694, 0.4694, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 78, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0054, 0.0051, 0.1527, 0.0056, 0.0054, 0.8203, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 78, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.1403, 0.8311, 0.0047, 0.0047, 0.0049, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 79, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0052, 0.0048, 0.0050, 0.9689, 0.0051, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 79, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0049, 0.0047, 0.0050, 0.0050, 0.0050, 0.8924, 0.0830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 79, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0055, 0.0050, 0.4343, 0.0050, 0.0695, 0.4759, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 79, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0054, 0.0060, 0.1517, 0.0064, 0.0054, 0.8188, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 79, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1409, 0.8299, 0.0047, 0.0049, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 80, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.0060, 0.0052, 0.9675, 0.0057, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 80, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0049, 0.0044, 0.0049, 0.0043, 0.0049, 0.9234, 0.0532],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 80, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0053, 0.0053, 0.4284, 0.0057, 0.0743, 0.4755, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 80, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.1556, 0.0051, 0.0055, 0.8173, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 80, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1433, 0.8274, 0.0047, 0.0047, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 81, batch: 39/196] total loss per batch: 0.561
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0050, 0.0054, 0.0050, 0.9687, 0.0051, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 81, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0054, 0.0048, 0.0055, 0.0051, 0.0058, 0.8908, 0.0827],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 81, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0054, 0.4429, 0.0047, 0.0605, 0.4757, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 81, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0050, 0.1438, 0.0049, 0.0052, 0.8301, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 81, batch: 195/196] total loss per batch: 0.551
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0095, 0.1373, 0.8345, 0.0046, 0.0045, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 82, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0042, 0.0047, 0.9713, 0.0048, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 82, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0049, 0.0048, 0.0046, 0.0040, 0.0047, 0.9124, 0.0646],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 82, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0057, 0.4216, 0.0058, 0.0744, 0.4811, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 82, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0046, 0.0046, 0.1437, 0.0039, 0.0046, 0.8343, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 82, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1359, 0.8353, 0.0046, 0.0046, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 83, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0051, 0.0047, 0.0046, 0.0042, 0.9722, 0.0044, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 83, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0056, 0.0049, 0.0055, 0.0048, 0.0055, 0.8938, 0.0799],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 83, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.4382, 0.0052, 0.0697, 0.4705, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 83, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.1501, 0.0042, 0.0044, 0.8270, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 83, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1366, 0.8338, 0.0048, 0.0047, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 84, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0049, 0.0043, 0.0043, 0.0040, 0.9738, 0.0040, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 84, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0052, 0.0052, 0.0050, 0.0055, 0.9074, 0.0662],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 84, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0052, 0.0049, 0.4382, 0.0049, 0.0691, 0.4726, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 84, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.1497, 0.0044, 0.0044, 0.8274, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 84, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1387, 0.8308, 0.0050, 0.0049, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 85, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0041, 0.0041, 0.0039, 0.9747, 0.0040, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 85, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0050, 0.0054, 0.0052, 0.0055, 0.9006, 0.0727],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 85, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.4413, 0.0046, 0.0690, 0.4709, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 85, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.1528, 0.0049, 0.0048, 0.8225, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 85, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1392, 0.8308, 0.0049, 0.0047, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 86, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0041, 0.0038, 0.0038, 0.9750, 0.0040, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 86, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0049, 0.0046, 0.0050, 0.0051, 0.0052, 0.9093, 0.0659],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 86, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0044, 0.4340, 0.0045, 0.0689, 0.4788, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 86, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0051, 0.0050, 0.1502, 0.0053, 0.0050, 0.8243, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 86, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.1404, 0.8292, 0.0050, 0.0049, 0.0052, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 87, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0047, 0.0042, 0.0043, 0.0042, 0.9738, 0.0042, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 87, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0048, 0.0047, 0.0047, 0.0053, 0.0050, 0.9040, 0.0715],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 87, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0047, 0.0043, 0.4335, 0.0046, 0.0694, 0.4790, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 87, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0053, 0.0052, 0.1518, 0.0057, 0.0053, 0.8214, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 87, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1404, 0.8300, 0.0049, 0.0047, 0.0050, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 88, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0050, 0.0046, 0.0045, 0.0045, 0.9719, 0.0046, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 88, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0043, 0.0049, 0.0049, 0.0048, 0.9073, 0.0693],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 88, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0045, 0.4403, 0.0046, 0.0695, 0.4715, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 88, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.1482, 0.0055, 0.0054, 0.8255, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 88, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.1404, 0.8296, 0.0048, 0.0048, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 89, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.0055, 0.0047, 0.9693, 0.0049, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 89, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0051, 0.0051, 0.0045, 0.0051, 0.0051, 0.9076, 0.0677],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 89, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0051, 0.0048, 0.4312, 0.0052, 0.0732, 0.4756, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.000

[Epoch: 89, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0051, 0.0053, 0.1560, 0.0054, 0.0053, 0.8175, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 89, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0098, 0.1404, 0.8311, 0.0047, 0.0046, 0.0047, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 90, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0050, 0.0053, 0.0053, 0.9683, 0.0057, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 90, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0046, 0.0045, 0.0049, 0.0047, 0.0047, 0.9037, 0.0729],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.000

[Epoch: 90, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0057, 0.0050, 0.4285, 0.0051, 0.0744, 0.4757, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 90, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0047, 0.0047, 0.1430, 0.0052, 0.0049, 0.8326, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 90, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1403, 0.8302, 0.0047, 0.0048, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 91, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0059, 0.0059, 0.0057, 0.0052, 0.9661, 0.0054, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.003

[Epoch: 91, batch: 78/196] total loss per batch: 0.548
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0055, 0.0046, 0.0047, 0.0052, 0.9055, 0.0689],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 91, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0054, 0.0057, 0.4436, 0.0056, 0.0599, 0.4744, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 91, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0049, 0.1565, 0.0041, 0.0048, 0.8198, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 91, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0096, 0.1370, 0.8349, 0.0046, 0.0045, 0.0047, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 92, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0057, 0.0054, 0.9673, 0.0056, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.008

[Epoch: 92, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0046, 0.0050, 0.0043, 0.0050, 0.9064, 0.0696],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 92, batch: 117/196] total loss per batch: 0.552
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0053, 0.0051, 0.4334, 0.0051, 0.0730, 0.4727, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 92, batch: 156/196] total loss per batch: 0.543
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0044, 0.0044, 0.1459, 0.0041, 0.0043, 0.8327, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 92, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1375, 0.8328, 0.0048, 0.0048, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 93, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0051, 0.0047, 0.9692, 0.0050, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 93, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0058, 0.0056, 0.0051, 0.0047, 0.0056, 0.8983, 0.0750],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 93, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0051, 0.0051, 0.4320, 0.0049, 0.0675, 0.4804, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 93, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0048, 0.0048, 0.1496, 0.0045, 0.0048, 0.8267, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 93, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0099, 0.1396, 0.8305, 0.0050, 0.0048, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 94, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0048, 0.0052, 0.0049, 0.9694, 0.0052, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 94, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0057, 0.0054, 0.0056, 0.0052, 0.0059, 0.9023, 0.0698],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 94, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0048, 0.4409, 0.0047, 0.0738, 0.4662, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 94, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0049, 0.1524, 0.0051, 0.0050, 0.8226, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 94, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1402, 0.8296, 0.0050, 0.0049, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 95, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0052, 0.0047, 0.0046, 0.0047, 0.9707, 0.0050, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.002

[Epoch: 95, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0057, 0.0052, 0.0054, 0.0052, 0.0057, 0.9043, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 95, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0046, 0.4320, 0.0046, 0.0665, 0.4830, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 95, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.1497, 0.0054, 0.0052, 0.8243, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 95, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0100, 0.1412, 0.8292, 0.0049, 0.0048, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 96, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0053, 0.0049, 0.0054, 0.0051, 0.9687, 0.0054, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.004

[Epoch: 96, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0055, 0.0052, 0.0057, 0.0056, 0.0059, 0.9015, 0.0705],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 96, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0048, 0.0047, 0.4379, 0.0047, 0.0732, 0.4701, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 96, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0052, 0.0051, 0.1507, 0.0055, 0.0053, 0.8230, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 96, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1410, 0.8289, 0.0050, 0.0049, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 97, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0055, 0.0053, 0.0052, 0.0053, 0.9677, 0.0055, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 97, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0050, 0.0055, 0.0054, 0.0055, 0.9051, 0.0680],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 97, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0049, 0.0047, 0.4322, 0.0047, 0.0682, 0.4805, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 97, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0050, 0.0051, 0.1492, 0.0053, 0.0052, 0.8248, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 97, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1414, 0.8289, 0.0049, 0.0048, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 98, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.0055, 0.0052, 0.9671, 0.0057, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 98, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0053, 0.0051, 0.0058, 0.0053, 0.0056, 0.9018, 0.0711],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 98, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0052, 0.0052, 0.4366, 0.0052, 0.0734, 0.4693, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 98, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0049, 0.1495, 0.0050, 0.0051, 0.8255, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 98, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0103, 0.1408, 0.8288, 0.0050, 0.0049, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 99, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0056, 0.0053, 0.0055, 0.0053, 0.9670, 0.0057, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.006

[Epoch: 99, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0054, 0.0050, 0.0054, 0.0053, 0.0053, 0.9051, 0.0685],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 99, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0053, 0.0054, 0.4287, 0.0053, 0.0682, 0.4821, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 99, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0049, 0.0051, 0.1527, 0.0049, 0.0050, 0.8222, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 99, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0102, 0.1415, 0.8289, 0.0048, 0.0047, 0.0049, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 100, batch: 39/196] total loss per batch: 0.560
Policy (actual, predicted): 4 4
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.9700, 0.0050, 0.0050])
Policy pred: tensor([0.0054, 0.0053, 0.0051, 0.0049, 0.9682, 0.0057, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.003 -0.005

[Epoch: 100, batch: 78/196] total loss per batch: 0.547
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.9050, 0.0700])
Policy pred: tensor([0.0052, 0.0052, 0.0057, 0.0047, 0.0052, 0.9046, 0.0694],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.015 -0.001

[Epoch: 100, batch: 117/196] total loss per batch: 0.551
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.4350, 0.0050, 0.0700, 0.4750, 0.0050])
Policy pred: tensor([0.0056, 0.0057, 0.4391, 0.0053, 0.0704, 0.4683, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.002 -0.001

[Epoch: 100, batch: 156/196] total loss per batch: 0.542
Policy (actual, predicted): 5 5
Policy data: tensor([0.0050, 0.0050, 0.1500, 0.0050, 0.0050, 0.8250, 0.0050])
Policy pred: tensor([0.0045, 0.0044, 0.1462, 0.0043, 0.0045, 0.8315, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.027 -0.001

[Epoch: 100, batch: 195/196] total loss per batch: 0.550
Policy (actual, predicted): 2 2
Policy data: tensor([0.0100, 0.1400, 0.8300, 0.0050, 0.0050, 0.0050, 0.0050])
Policy pred: tensor([0.0101, 0.1398, 0.8300, 0.0050, 0.0049, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

