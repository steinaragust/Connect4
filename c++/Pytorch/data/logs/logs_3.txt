Training set samples: 6924
Batch size: 32
[Epoch: 1, batch: 43/217] total loss per batch: 1.367
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.1425, 0.0838, 0.0207, 0.4319, 0.0499, 0.1874, 0.0839],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 1, batch: 86/217] total loss per batch: 1.353
Policy (actual, predicted): 4 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0277, 0.0414, 0.0469, 0.5916, 0.0674, 0.1026, 0.1223],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.084

[Epoch: 1, batch: 129/217] total loss per batch: 1.328
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0180, 0.0697, 0.1870, 0.0526, 0.2114, 0.4496, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 1, batch: 172/217] total loss per batch: 1.244
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0037, 0.0079, 0.1995, 0.7182, 0.0313, 0.0307, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.016

[Epoch: 1, batch: 215/217] total loss per batch: 1.207
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0918, 0.4272, 0.3171, 0.0685, 0.0643, 0.0268],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 2, batch: 43/217] total loss per batch: 0.987
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.1446, 0.0178, 0.0075, 0.7028, 0.0203, 0.0940, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 2, batch: 86/217] total loss per batch: 0.995
Policy (actual, predicted): 4 3
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0422, 0.0287, 0.0163, 0.6273, 0.1685, 0.0848, 0.0322],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.066

[Epoch: 2, batch: 129/217] total loss per batch: 0.980
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0087, 0.0217, 0.0243, 0.0173, 0.3336, 0.5881, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.020

[Epoch: 2, batch: 172/217] total loss per batch: 0.905
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([6.4265e-04, 2.4180e-03, 8.8052e-02, 8.6156e-01, 2.5297e-02, 1.7118e-02,
        4.9111e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.019

[Epoch: 2, batch: 215/217] total loss per batch: 0.903
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0499, 0.5311, 0.2886, 0.0597, 0.0490, 0.0200],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.018

[Epoch: 3, batch: 43/217] total loss per batch: 0.828
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0278, 0.0044, 0.0054, 0.9334, 0.0048, 0.0212, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 3, batch: 86/217] total loss per batch: 0.829
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0169, 0.0111, 0.0108, 0.3944, 0.5138, 0.0467, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.135

[Epoch: 3, batch: 129/217] total loss per batch: 0.819
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0086, 0.0188, 0.0143, 0.0181, 0.3805, 0.5517, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.019

[Epoch: 3, batch: 172/217] total loss per batch: 0.777
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([5.1436e-04, 3.1345e-03, 4.9591e-02, 9.2322e-01, 7.9683e-03, 1.2460e-02,
        3.1077e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 3, batch: 215/217] total loss per batch: 0.784
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0592, 0.5749, 0.2582, 0.0498, 0.0318, 0.0247],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.014

[Epoch: 4, batch: 43/217] total loss per batch: 0.789
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0198, 0.0060, 0.0078, 0.9283, 0.0038, 0.0317, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.033

[Epoch: 4, batch: 86/217] total loss per batch: 0.783
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0163, 0.0031, 0.0054, 0.2015, 0.7397, 0.0276, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.289

[Epoch: 4, batch: 129/217] total loss per batch: 0.777
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0086, 0.0182, 0.0100, 0.0156, 0.4488, 0.4929, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 4, batch: 172/217] total loss per batch: 0.756
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0011, 0.0043, 0.0440, 0.8863, 0.0087, 0.0529, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 4, batch: 215/217] total loss per batch: 0.762
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0596, 0.5467, 0.3245, 0.0245, 0.0237, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 5, batch: 43/217] total loss per batch: 0.766
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0762, 0.0033, 0.0094, 0.8025, 0.0109, 0.0912, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.027

[Epoch: 5, batch: 86/217] total loss per batch: 0.776
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0141, 0.0097, 0.0039, 0.1231, 0.7930, 0.0452, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.422

[Epoch: 5, batch: 129/217] total loss per batch: 0.760
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0099, 0.0201, 0.0117, 0.0152, 0.3314, 0.6022, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 5, batch: 172/217] total loss per batch: 0.746
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([5.3308e-04, 3.6597e-03, 3.9921e-02, 9.1322e-01, 2.0368e-02, 1.9642e-02,
        2.6558e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.007

[Epoch: 5, batch: 215/217] total loss per batch: 0.755
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0423, 0.5364, 0.3234, 0.0361, 0.0294, 0.0292],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.015

[Epoch: 6, batch: 43/217] total loss per batch: 0.748
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0133, 0.0038, 0.0101, 0.9541, 0.0026, 0.0135, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.030

[Epoch: 6, batch: 86/217] total loss per batch: 0.765
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0059, 0.0042, 0.0037, 0.1066, 0.8389, 0.0256, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.611

[Epoch: 6, batch: 129/217] total loss per batch: 0.745
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0110, 0.0304, 0.0131, 0.0164, 0.3586, 0.5636, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 6, batch: 172/217] total loss per batch: 0.729
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0018, 0.0033, 0.0267, 0.9490, 0.0048, 0.0119, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 6, batch: 215/217] total loss per batch: 0.743
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0360, 0.6967, 0.2160, 0.0191, 0.0199, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 7, batch: 43/217] total loss per batch: 0.737
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0402, 0.0069, 0.0075, 0.8848, 0.0110, 0.0454, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.024

[Epoch: 7, batch: 86/217] total loss per batch: 0.750
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.0038, 0.0022, 0.0595, 0.9047, 0.0176, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.802

[Epoch: 7, batch: 129/217] total loss per batch: 0.734
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0094, 0.0171, 0.0095, 0.0129, 0.3692, 0.5743, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 7, batch: 172/217] total loss per batch: 0.722
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0017, 0.0062, 0.0205, 0.9487, 0.0071, 0.0128, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 7, batch: 215/217] total loss per batch: 0.726
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0303, 0.3173, 0.6104, 0.0161, 0.0145, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.011

[Epoch: 8, batch: 43/217] total loss per batch: 0.730
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0307, 0.0054, 0.0130, 0.9210, 0.0061, 0.0195, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 8, batch: 86/217] total loss per batch: 0.739
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0021, 0.0020, 0.0671, 0.8886, 0.0220, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.880

[Epoch: 8, batch: 129/217] total loss per batch: 0.730
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0100, 0.0260, 0.0123, 0.0145, 0.3528, 0.5755, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 8, batch: 172/217] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0019, 0.0029, 0.0222, 0.9430, 0.0097, 0.0162, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 8, batch: 215/217] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0130, 0.6336, 0.2970, 0.0163, 0.0237, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.015

[Epoch: 9, batch: 43/217] total loss per batch: 0.723
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0334, 0.0050, 0.0166, 0.9021, 0.0112, 0.0274, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.023

[Epoch: 9, batch: 86/217] total loss per batch: 0.732
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0061, 0.0102, 0.0012, 0.0592, 0.8938, 0.0191, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.900

[Epoch: 9, batch: 129/217] total loss per batch: 0.726
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0112, 0.0238, 0.0127, 0.0178, 0.3587, 0.5690, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 9, batch: 172/217] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0043, 0.0050, 0.0104, 0.9482, 0.0140, 0.0139, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.007

[Epoch: 9, batch: 215/217] total loss per batch: 0.721
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0142, 0.6742, 0.2532, 0.0269, 0.0195, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 10, batch: 43/217] total loss per batch: 0.721
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0225, 0.0038, 0.0124, 0.9322, 0.0059, 0.0180, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 10, batch: 86/217] total loss per batch: 0.731
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0074, 0.0021, 0.0010, 0.0403, 0.9263, 0.0145, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.899

[Epoch: 10, batch: 129/217] total loss per batch: 0.725
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0112, 0.0216, 0.0133, 0.0135, 0.3524, 0.5819, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 10, batch: 172/217] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0026, 0.0026, 0.0249, 0.9407, 0.0056, 0.0219, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 10, batch: 215/217] total loss per batch: 0.719
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0146, 0.4774, 0.4555, 0.0127, 0.0216, 0.0154],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 11, batch: 43/217] total loss per batch: 0.717
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0135, 0.0032, 0.0079, 0.9477, 0.0068, 0.0172, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 11, batch: 86/217] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0036, 0.0013, 0.0449, 0.9256, 0.0129, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.914

[Epoch: 11, batch: 129/217] total loss per batch: 0.723
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0109, 0.0245, 0.0171, 0.0183, 0.3436, 0.5786, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 11, batch: 172/217] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0025, 0.0044, 0.0249, 0.9395, 0.0118, 0.0132, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 11, batch: 215/217] total loss per batch: 0.717
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0140, 0.6481, 0.3033, 0.0136, 0.0141, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.015

[Epoch: 12, batch: 43/217] total loss per batch: 0.717
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0351, 0.0111, 0.0111, 0.8935, 0.0104, 0.0322, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 12, batch: 86/217] total loss per batch: 0.726
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([5.8390e-03, 1.5056e-03, 7.0581e-04, 3.6465e-02, 9.3616e-01, 1.1240e-02,
        8.0800e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.898

[Epoch: 12, batch: 129/217] total loss per batch: 0.723
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0297, 0.0145, 0.0116, 0.4146, 0.5074, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 12, batch: 172/217] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0044, 0.0035, 0.0173, 0.9425, 0.0072, 0.0226, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 12, batch: 215/217] total loss per batch: 0.718
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0126, 0.5615, 0.3852, 0.0123, 0.0207, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 13, batch: 43/217] total loss per batch: 0.718
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0162, 0.0021, 0.0069, 0.9473, 0.0067, 0.0158, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 13, batch: 86/217] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0071, 0.0030, 0.0022, 0.0502, 0.9188, 0.0139, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.880

[Epoch: 13, batch: 129/217] total loss per batch: 0.721
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0117, 0.0200, 0.0147, 0.0184, 0.3809, 0.5454, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 13, batch: 172/217] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0034, 0.0026, 0.0098, 0.9651, 0.0097, 0.0062, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 13, batch: 215/217] total loss per batch: 0.715
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0096, 0.6314, 0.3256, 0.0105, 0.0111, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 14, batch: 43/217] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0295, 0.0057, 0.0108, 0.9169, 0.0110, 0.0183, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 14, batch: 86/217] total loss per batch: 0.727
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0016, 0.0013, 0.0327, 0.9447, 0.0074, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.939

[Epoch: 14, batch: 129/217] total loss per batch: 0.718
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0148, 0.0221, 0.0143, 0.0142, 0.4087, 0.5175, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 14, batch: 172/217] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0016, 0.0123, 0.9336, 0.0094, 0.0337, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 14, batch: 215/217] total loss per batch: 0.713
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0115, 0.5911, 0.3517, 0.0137, 0.0223, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.015

[Epoch: 15, batch: 43/217] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0251, 0.0065, 0.0117, 0.9257, 0.0063, 0.0195, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.003

[Epoch: 15, batch: 86/217] total loss per batch: 0.728
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([6.6778e-03, 2.3248e-03, 8.8969e-04, 2.4782e-02, 9.4846e-01, 9.6270e-03,
        7.2373e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.899

[Epoch: 15, batch: 129/217] total loss per batch: 0.720
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0096, 0.0225, 0.0136, 0.0166, 0.3804, 0.5495, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 15, batch: 172/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0031, 0.0070, 0.0162, 0.9512, 0.0109, 0.0089, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 15, batch: 215/217] total loss per batch: 0.712
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0074, 0.6103, 0.3535, 0.0106, 0.0093, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 16, batch: 43/217] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0197, 0.0037, 0.0059, 0.9368, 0.0111, 0.0144, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 16, batch: 86/217] total loss per batch: 0.725
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0063, 0.0021, 0.0014, 0.0356, 0.9407, 0.0077, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.916

[Epoch: 16, batch: 129/217] total loss per batch: 0.718
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0231, 0.0145, 0.0162, 0.3756, 0.5484, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 16, batch: 172/217] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0061, 0.0146, 0.9491, 0.0092, 0.0106, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 16, batch: 215/217] total loss per batch: 0.714
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0124, 0.6134, 0.3403, 0.0117, 0.0125, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 17, batch: 43/217] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0438, 0.0067, 0.0216, 0.8947, 0.0068, 0.0207, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 17, batch: 86/217] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0076, 0.0018, 0.0016, 0.0182, 0.9559, 0.0078, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.924

[Epoch: 17, batch: 129/217] total loss per batch: 0.717
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0120, 0.0228, 0.0139, 0.0185, 0.3916, 0.5288, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 17, batch: 172/217] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0037, 0.0027, 0.0230, 0.9307, 0.0131, 0.0207, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 17, batch: 215/217] total loss per batch: 0.713
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0095, 0.5142, 0.4330, 0.0171, 0.0133, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.009

[Epoch: 18, batch: 43/217] total loss per batch: 0.714
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0172, 0.0037, 0.0071, 0.9457, 0.0081, 0.0103, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 18, batch: 86/217] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0072, 0.0020, 0.0017, 0.0425, 0.9337, 0.0063, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.944

[Epoch: 18, batch: 129/217] total loss per batch: 0.716
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0240, 0.0158, 0.0205, 0.3448, 0.5721, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 18, batch: 172/217] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0046, 0.0026, 0.0137, 0.9558, 0.0048, 0.0135, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 18, batch: 215/217] total loss per batch: 0.709
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0103, 0.6402, 0.3097, 0.0130, 0.0154, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.014

[Epoch: 19, batch: 43/217] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0436, 0.0069, 0.0094, 0.9061, 0.0100, 0.0180, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.005

[Epoch: 19, batch: 86/217] total loss per batch: 0.721
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.9472e-03, 2.3896e-03, 7.4972e-04, 1.6852e-02, 9.6418e-01, 7.9566e-03,
        2.9253e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.926

[Epoch: 19, batch: 129/217] total loss per batch: 0.720
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0111, 0.0203, 0.0130, 0.0151, 0.3677, 0.5633, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 19, batch: 172/217] total loss per batch: 0.704
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0046, 0.0043, 0.0404, 0.9087, 0.0207, 0.0168, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.009

[Epoch: 19, batch: 215/217] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0088, 0.6281, 0.3310, 0.0130, 0.0094, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 20, batch: 43/217] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0165, 0.0117, 0.0053, 0.9437, 0.0050, 0.0127, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.021

[Epoch: 20, batch: 86/217] total loss per batch: 0.721
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0056, 0.0029, 0.0013, 0.0193, 0.9602, 0.0056, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.930

[Epoch: 20, batch: 129/217] total loss per batch: 0.717
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0107, 0.0239, 0.0158, 0.0213, 0.3680, 0.5452, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 20, batch: 172/217] total loss per batch: 0.704
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0038, 0.0021, 0.0106, 0.9522, 0.0101, 0.0164, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.009

[Epoch: 20, batch: 215/217] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0103, 0.4933, 0.4596, 0.0156, 0.0112, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.011

[Epoch: 21, batch: 43/217] total loss per batch: 0.714
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0336, 0.0068, 0.0077, 0.9187, 0.0151, 0.0090, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.007

[Epoch: 21, batch: 86/217] total loss per batch: 0.721
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0090, 0.0062, 0.0016, 0.0287, 0.9348, 0.0116, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.928

[Epoch: 21, batch: 129/217] total loss per batch: 0.715
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0149, 0.0228, 0.0138, 0.0146, 0.3704, 0.5539, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 21, batch: 172/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0023, 0.0107, 0.9638, 0.0042, 0.0110, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.008

[Epoch: 21, batch: 215/217] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0048, 0.7265, 0.2445, 0.0084, 0.0101, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 22, batch: 43/217] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0182, 0.0044, 0.0078, 0.9485, 0.0080, 0.0087, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.004

[Epoch: 22, batch: 86/217] total loss per batch: 0.723
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0068, 0.0037, 0.0018, 0.0157, 0.9599, 0.0100, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.943

[Epoch: 22, batch: 129/217] total loss per batch: 0.713
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0107, 0.0253, 0.0163, 0.0221, 0.3659, 0.5473, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 22, batch: 172/217] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0024, 0.0020, 0.0083, 0.9615, 0.0063, 0.0166, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.004

[Epoch: 22, batch: 215/217] total loss per batch: 0.708
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0050, 0.3906, 0.5662, 0.0111, 0.0113, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 23, batch: 43/217] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0301, 0.0098, 0.0058, 0.9313, 0.0096, 0.0094, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 23, batch: 86/217] total loss per batch: 0.719
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.7121e-03, 2.6840e-03, 7.2222e-04, 3.1620e-02, 9.4538e-01, 4.6797e-03,
        1.0198e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.913

[Epoch: 23, batch: 129/217] total loss per batch: 0.712
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0208, 0.0113, 0.0135, 0.3752, 0.5542, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 23, batch: 172/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0053, 0.0147, 0.9518, 0.0058, 0.0124, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 23, batch: 215/217] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0139, 0.8181, 0.1459, 0.0087, 0.0069, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.012

[Epoch: 24, batch: 43/217] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0282, 0.0076, 0.0053, 0.9308, 0.0105, 0.0134, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 24, batch: 86/217] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0060, 0.0041, 0.0027, 0.0202, 0.9511, 0.0119, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.935

[Epoch: 24, batch: 129/217] total loss per batch: 0.712
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0105, 0.0210, 0.0169, 0.0182, 0.3590, 0.5644, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 24, batch: 172/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0081, 0.0034, 0.0121, 0.9409, 0.0156, 0.0131, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 24, batch: 215/217] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.0117, 0.5121, 0.4467, 0.0105, 0.0085, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.011

[Epoch: 25, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0320, 0.0071, 0.0053, 0.9333, 0.0100, 0.0071, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.027

[Epoch: 25, batch: 86/217] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([3.0154e-03, 1.8823e-03, 6.3936e-04, 1.6180e-02, 9.6857e-01, 7.1957e-03,
        2.5199e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.915

[Epoch: 25, batch: 129/217] total loss per batch: 0.710
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0123, 0.0204, 0.0136, 0.0169, 0.3593, 0.5660, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 25, batch: 172/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0057, 0.0023, 0.0160, 0.9547, 0.0040, 0.0125, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 25, batch: 215/217] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0059, 0.6264, 0.3389, 0.0078, 0.0099, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.009

[Epoch: 26, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0207, 0.0091, 0.0053, 0.9349, 0.0118, 0.0137, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 26, batch: 86/217] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.0348e-03, 2.1482e-03, 8.7527e-04, 1.3668e-02, 9.7274e-01, 3.8236e-03,
        2.7079e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.940

[Epoch: 26, batch: 129/217] total loss per batch: 0.708
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0243, 0.0156, 0.0162, 0.3795, 0.5417, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 26, batch: 172/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0052, 0.0171, 0.9419, 0.0102, 0.0154, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.003

[Epoch: 26, batch: 215/217] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0109, 0.6088, 0.3582, 0.0068, 0.0092, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 27, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0220, 0.0056, 0.0041, 0.9437, 0.0118, 0.0078, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 27, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0050, 0.0015, 0.0011, 0.0171, 0.9676, 0.0046, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.959

[Epoch: 27, batch: 129/217] total loss per batch: 0.708
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0112, 0.0179, 0.0126, 0.0175, 0.3481, 0.5825, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 27, batch: 172/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0081, 0.0053, 0.0108, 0.9468, 0.0093, 0.0153, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 27, batch: 215/217] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0076, 0.5482, 0.4170, 0.0117, 0.0063, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 28, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0318, 0.0073, 0.0055, 0.9276, 0.0095, 0.0098, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 28, batch: 86/217] total loss per batch: 0.713
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0063, 0.0041, 0.0021, 0.0186, 0.9583, 0.0055, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.950

[Epoch: 28, batch: 129/217] total loss per batch: 0.708
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0206, 0.0134, 0.0250, 0.3701, 0.5445, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 28, batch: 172/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0030, 0.0028, 0.0270, 0.9395, 0.0083, 0.0151, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 28, batch: 215/217] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0051, 0.0112, 0.6621, 0.2897, 0.0094, 0.0142, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.011

[Epoch: 29, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0237, 0.0058, 0.0042, 0.9449, 0.0082, 0.0092, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 29, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.4661e-03, 2.6410e-03, 8.6629e-04, 1.8095e-02, 9.6258e-01, 7.3714e-03,
        3.9802e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.939

[Epoch: 29, batch: 129/217] total loss per batch: 0.706
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0241, 0.0129, 0.0166, 0.3280, 0.5945, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 29, batch: 172/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0081, 0.0036, 0.0083, 0.9513, 0.0157, 0.0110, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 29, batch: 215/217] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0064, 0.5857, 0.3852, 0.0093, 0.0070, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 30, batch: 43/217] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0064, 0.0106, 0.9309, 0.0117, 0.0069, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 30, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.0256e-03, 1.1305e-03, 9.5578e-04, 1.3122e-02, 9.7415e-01, 3.5519e-03,
        3.0654e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.963

[Epoch: 30, batch: 129/217] total loss per batch: 0.706
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0151, 0.0201, 0.0137, 0.0197, 0.3735, 0.5433, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 30, batch: 172/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0031, 0.0044, 0.0101, 0.9588, 0.0069, 0.0131, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 30, batch: 215/217] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0070, 0.6024, 0.3665, 0.0073, 0.0077, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 31, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0177, 0.0069, 0.0051, 0.9505, 0.0079, 0.0070, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 31, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0083, 0.0066, 0.0031, 0.0245, 0.9198, 0.0157, 0.0220],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.940

[Epoch: 31, batch: 129/217] total loss per batch: 0.707
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0117, 0.0246, 0.0106, 0.0176, 0.3595, 0.5654, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 31, batch: 172/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0072, 0.0019, 0.0111, 0.9391, 0.0271, 0.0116, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.004

[Epoch: 31, batch: 215/217] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0083, 0.5962, 0.3731, 0.0100, 0.0067, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 32, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0599, 0.0124, 0.0141, 0.8793, 0.0117, 0.0147, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 32, batch: 86/217] total loss per batch: 0.715
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0030, 0.0014, 0.0221, 0.9640, 0.0042, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.914

[Epoch: 32, batch: 129/217] total loss per batch: 0.709
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0230, 0.0147, 0.0233, 0.3522, 0.5596, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 32, batch: 172/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0099, 0.0027, 0.0161, 0.9442, 0.0108, 0.0121, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 32, batch: 215/217] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0046, 0.6954, 0.2759, 0.0082, 0.0076, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 33, batch: 43/217] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0122, 0.0059, 0.0035, 0.9549, 0.0110, 0.0080, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 33, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([2.9549e-03, 3.9468e-03, 9.4962e-04, 1.1315e-02, 9.6694e-01, 8.4279e-03,
        5.4632e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.938

[Epoch: 33, batch: 129/217] total loss per batch: 0.709
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0152, 0.0250, 0.0154, 0.0172, 0.3453, 0.5686, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 33, batch: 172/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0060, 0.0172, 0.9289, 0.0181, 0.0208, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 33, batch: 215/217] total loss per batch: 0.702
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0085, 0.5309, 0.4406, 0.0066, 0.0077, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 34, batch: 43/217] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0249, 0.0053, 0.0071, 0.9446, 0.0052, 0.0081, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 34, batch: 86/217] total loss per batch: 0.714
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([3.0037e-03, 1.3534e-03, 8.4962e-04, 1.2560e-02, 9.7565e-01, 3.3147e-03,
        3.2687e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.960

[Epoch: 34, batch: 129/217] total loss per batch: 0.710
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0122, 0.0202, 0.0156, 0.0143, 0.3633, 0.5612, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 34, batch: 172/217] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0028, 0.0028, 0.0047, 0.9741, 0.0063, 0.0057, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 34, batch: 215/217] total loss per batch: 0.779
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0267, 0.6202, 0.2981, 0.0193, 0.0268, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.025

[Epoch: 35, batch: 43/217] total loss per batch: 0.788
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0714, 0.0134, 0.0118, 0.8429, 0.0306, 0.0220, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 35, batch: 86/217] total loss per batch: 0.795
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0231, 0.0264, 0.0081, 0.0339, 0.8701, 0.0148, 0.0235],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.746

[Epoch: 35, batch: 129/217] total loss per batch: 0.781
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0184, 0.0227, 0.0123, 0.0245, 0.3517, 0.5584, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.018

[Epoch: 35, batch: 172/217] total loss per batch: 0.757
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0047, 0.0029, 0.0254, 0.9445, 0.0030, 0.0144, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.019

[Epoch: 35, batch: 215/217] total loss per batch: 0.778
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0159, 0.6505, 0.3121, 0.0111, 0.0055, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 36, batch: 43/217] total loss per batch: 0.754
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0111, 0.0078, 0.0131, 0.9431, 0.0086, 0.0114, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 36, batch: 86/217] total loss per batch: 0.756
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([1.5777e-03, 3.0244e-04, 7.3140e-04, 1.4117e-02, 9.7752e-01, 3.0667e-03,
        2.6881e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.962

[Epoch: 36, batch: 129/217] total loss per batch: 0.750
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0147, 0.0316, 0.0109, 0.0169, 0.2871, 0.6249, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.004

[Epoch: 36, batch: 172/217] total loss per batch: 0.720
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0043, 0.0037, 0.0125, 0.9410, 0.0177, 0.0174, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 36, batch: 215/217] total loss per batch: 0.725
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0080, 0.3900, 0.5730, 0.0093, 0.0100, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 37, batch: 43/217] total loss per batch: 0.718
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0392, 0.0146, 0.0172, 0.8862, 0.0111, 0.0258, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 37, batch: 86/217] total loss per batch: 0.724
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([2.7220e-03, 6.6045e-04, 5.3543e-04, 1.7314e-02, 9.7128e-01, 3.2657e-03,
        4.2181e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.969

[Epoch: 37, batch: 129/217] total loss per batch: 0.713
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0117, 0.0246, 0.0098, 0.0181, 0.3574, 0.5642, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 37, batch: 172/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0038, 0.0046, 0.0157, 0.9385, 0.0114, 0.0219, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 37, batch: 215/217] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0055, 0.7140, 0.2513, 0.0100, 0.0116, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 38, batch: 43/217] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0263, 0.0093, 0.0163, 0.9105, 0.0117, 0.0189, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 38, batch: 86/217] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([3.4124e-03, 7.8232e-04, 8.0513e-04, 1.4082e-02, 9.7098e-01, 4.6491e-03,
        5.2923e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.954

[Epoch: 38, batch: 129/217] total loss per batch: 0.702
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0198, 0.0104, 0.0176, 0.3559, 0.5720, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 38, batch: 172/217] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0040, 0.0127, 0.9483, 0.0093, 0.0179, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 38, batch: 215/217] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0046, 0.6335, 0.3347, 0.0100, 0.0088, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 39, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0235, 0.0050, 0.0149, 0.9291, 0.0103, 0.0109, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 39, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([2.9220e-03, 7.2767e-04, 8.8709e-04, 1.1841e-02, 9.7679e-01, 4.4553e-03,
        2.3811e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.960

[Epoch: 39, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0122, 0.0197, 0.0099, 0.0192, 0.3513, 0.5754, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 39, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0049, 0.0032, 0.0079, 0.9568, 0.0096, 0.0147, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 39, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0048, 0.5858, 0.3835, 0.0100, 0.0074, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 40, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0283, 0.0070, 0.0126, 0.9295, 0.0095, 0.0080, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.023

[Epoch: 40, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([2.6976e-03, 7.0001e-04, 8.9085e-04, 9.2679e-03, 9.7998e-01, 3.9892e-03,
        2.4748e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.956

[Epoch: 40, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0193, 0.0106, 0.0187, 0.3580, 0.5675, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 40, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0070, 0.0043, 0.0130, 0.9478, 0.0112, 0.0135, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 40, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0047, 0.6428, 0.3281, 0.0091, 0.0082, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 41, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0244, 0.0065, 0.0121, 0.9343, 0.0089, 0.0085, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.023

[Epoch: 41, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([3.0422e-03, 7.4040e-04, 8.1846e-04, 8.2248e-03, 9.8067e-01, 4.1685e-03,
        2.3311e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.965

[Epoch: 41, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0201, 0.0104, 0.0199, 0.3466, 0.5777, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 41, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0029, 0.0093, 0.9516, 0.0103, 0.0160, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 41, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0047, 0.6284, 0.3435, 0.0085, 0.0078, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 42, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0280, 0.0074, 0.0096, 0.9329, 0.0088, 0.0070, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 42, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([3.3602e-03, 8.4942e-04, 1.0921e-03, 7.0418e-03, 9.8045e-01, 3.9744e-03,
        3.2334e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.963

[Epoch: 42, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0187, 0.0104, 0.0189, 0.3616, 0.5641, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 42, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0044, 0.0135, 0.9461, 0.0122, 0.0133, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 42, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0041, 0.6078, 0.3642, 0.0084, 0.0079, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 43, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0271, 0.0054, 0.0111, 0.9356, 0.0092, 0.0072, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 43, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0013, 0.0011, 0.0100, 0.9771, 0.0047, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 43, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0124, 0.0197, 0.0104, 0.0211, 0.3356, 0.5884, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 43, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0030, 0.0075, 0.9534, 0.0121, 0.0131, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 43, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0040, 0.5941, 0.3783, 0.0079, 0.0089, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 44, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0250, 0.0086, 0.0077, 0.9346, 0.0079, 0.0095, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 44, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.0012, 0.0016, 0.0092, 0.9747, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.966

[Epoch: 44, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0203, 0.0120, 0.0182, 0.3532, 0.5689, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 44, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0051, 0.0047, 0.0183, 0.9433, 0.0119, 0.0134, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 44, batch: 215/217] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0069, 0.6987, 0.2712, 0.0092, 0.0073, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 45, batch: 43/217] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0249, 0.0061, 0.0106, 0.9396, 0.0096, 0.0051, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 45, batch: 86/217] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0018, 0.0014, 0.0078, 0.9796, 0.0040, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 45, batch: 129/217] total loss per batch: 0.702
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0123, 0.0215, 0.0113, 0.0204, 0.3723, 0.5502, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 45, batch: 172/217] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0031, 0.0059, 0.9603, 0.0119, 0.0095, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 45, batch: 215/217] total loss per batch: 0.698
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0017, 0.0057, 0.3845, 0.5854, 0.0079, 0.0086, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 46, batch: 43/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0274, 0.0086, 0.0073, 0.9327, 0.0070, 0.0090, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.020

[Epoch: 46, batch: 86/217] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0014, 0.0017, 0.0086, 0.9749, 0.0044, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.957

[Epoch: 46, batch: 129/217] total loss per batch: 0.703
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0167, 0.0245, 0.0119, 0.0169, 0.3322, 0.5838, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 46, batch: 172/217] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0051, 0.0236, 0.9281, 0.0155, 0.0171, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 46, batch: 215/217] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0069, 0.8187, 0.1487, 0.0095, 0.0089, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 47, batch: 43/217] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0188, 0.0060, 0.0080, 0.9443, 0.0087, 0.0093, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.020

[Epoch: 47, batch: 86/217] total loss per batch: 0.710
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0025, 0.0021, 0.0111, 0.9721, 0.0038, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 47, batch: 129/217] total loss per batch: 0.704
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0205, 0.0133, 0.0197, 0.3489, 0.5740, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 47, batch: 172/217] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0065, 0.0050, 0.0149, 0.9457, 0.0113, 0.0123, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 47, batch: 215/217] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0065, 0.5143, 0.4504, 0.0126, 0.0085, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 48, batch: 43/217] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0431, 0.0113, 0.0076, 0.9124, 0.0101, 0.0088, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 48, batch: 86/217] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0064, 0.0016, 0.0011, 0.0061, 0.9785, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 48, batch: 129/217] total loss per batch: 0.705
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0139, 0.0197, 0.0097, 0.0237, 0.3638, 0.5567, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 48, batch: 172/217] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0072, 0.0032, 0.0106, 0.9507, 0.0066, 0.0184, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 48, batch: 215/217] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0094, 0.6916, 0.2670, 0.0086, 0.0104, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 49, batch: 43/217] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0161, 0.0039, 0.0070, 0.9437, 0.0113, 0.0132, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 49, batch: 86/217] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0024, 0.0012, 0.0107, 0.9724, 0.0038, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 49, batch: 129/217] total loss per batch: 0.704
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0143, 0.0212, 0.0115, 0.0220, 0.3307, 0.5893, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 49, batch: 172/217] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0037, 0.0044, 0.0085, 0.9555, 0.0126, 0.0115, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 49, batch: 215/217] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0072, 0.5435, 0.4220, 0.0099, 0.0105, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 50, batch: 43/217] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0501, 0.0118, 0.0071, 0.9076, 0.0084, 0.0058, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 50, batch: 86/217] total loss per batch: 0.711
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.0011, 0.0060, 0.9805, 0.0036, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 50, batch: 129/217] total loss per batch: 0.703
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0119, 0.0190, 0.0133, 0.0194, 0.3602, 0.5650, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 50, batch: 172/217] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0109, 0.0071, 0.0145, 0.9365, 0.0164, 0.0076, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.009

[Epoch: 50, batch: 215/217] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0076, 0.6780, 0.2887, 0.0094, 0.0066, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 51, batch: 43/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0150, 0.0074, 0.0086, 0.9432, 0.0096, 0.0087, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.021

[Epoch: 51, batch: 86/217] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([4.4860e-03, 1.7319e-03, 9.4072e-04, 6.7517e-03, 9.7799e-01, 3.2592e-03,
        4.8447e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 51, batch: 129/217] total loss per batch: 0.702
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0184, 0.0125, 0.0174, 0.3548, 0.5711, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 51, batch: 172/217] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0044, 0.0017, 0.0107, 0.9553, 0.0088, 0.0163, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 51, batch: 215/217] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0060, 0.5573, 0.4139, 0.0081, 0.0074, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 52, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0237, 0.0062, 0.0076, 0.9373, 0.0103, 0.0079, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 52, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.0026, 0.0020, 0.0085, 0.9736, 0.0043, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.959

[Epoch: 52, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0141, 0.0185, 0.0115, 0.0231, 0.3493, 0.5720, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 52, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0046, 0.0103, 0.9538, 0.0119, 0.0101, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 52, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0042, 0.6371, 0.3418, 0.0059, 0.0055, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 53, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0282, 0.0065, 0.0054, 0.9402, 0.0090, 0.0058, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.020

[Epoch: 53, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0023, 0.0013, 0.0063, 0.9781, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 53, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0190, 0.0133, 0.0194, 0.3636, 0.5600, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 53, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0036, 0.0125, 0.9524, 0.0118, 0.0095, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 53, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0038, 0.6338, 0.3442, 0.0053, 0.0053, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 54, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0294, 0.0066, 0.0061, 0.9319, 0.0103, 0.0085, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.017

[Epoch: 54, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0025, 0.0024, 0.0063, 0.9758, 0.0038, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.967

[Epoch: 54, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0129, 0.0169, 0.0116, 0.0199, 0.3612, 0.5653, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 54, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0028, 0.0092, 0.9571, 0.0125, 0.0094, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 54, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0041, 0.6018, 0.3755, 0.0068, 0.0058, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 55, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0250, 0.0062, 0.0069, 0.9430, 0.0076, 0.0051, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 55, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0027, 0.0019, 0.0045, 0.9789, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 55, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0185, 0.0133, 0.0215, 0.3644, 0.5570, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 55, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0035, 0.0118, 0.9544, 0.0111, 0.0107, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 55, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0048, 0.6565, 0.3192, 0.0052, 0.0066, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 56, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0283, 0.0065, 0.0060, 0.9335, 0.0110, 0.0082, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 56, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0029, 0.0030, 0.0067, 0.9748, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 56, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0149, 0.0111, 0.0202, 0.3547, 0.5745, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 56, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0056, 0.0027, 0.0074, 0.9602, 0.0125, 0.0088, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 56, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0049, 0.5449, 0.4324, 0.0067, 0.0047, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 57, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0204, 0.0071, 0.0061, 0.9447, 0.0091, 0.0058, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 57, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0023, 0.0020, 0.0047, 0.9799, 0.0035, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.969

[Epoch: 57, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0116, 0.0177, 0.0105, 0.0200, 0.3379, 0.5903, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 57, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0049, 0.0117, 0.9498, 0.0133, 0.0104, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 57, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0053, 0.6798, 0.2971, 0.0058, 0.0057, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 58, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0303, 0.0066, 0.0067, 0.9272, 0.0140, 0.0073, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 58, batch: 86/217] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0028, 0.0019, 0.0053, 0.9794, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 58, batch: 129/217] total loss per batch: 0.700
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0208, 0.0125, 0.0211, 0.3782, 0.5414, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 58, batch: 172/217] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0090, 0.0021, 0.0075, 0.9523, 0.0133, 0.0128, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 58, batch: 215/217] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0047, 0.5076, 0.4693, 0.0049, 0.0070, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 59, batch: 43/217] total loss per batch: 0.700
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0199, 0.0062, 0.0023, 0.9530, 0.0083, 0.0051, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 59, batch: 86/217] total loss per batch: 0.708
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0021, 0.0030, 0.0053, 0.9732, 0.0063, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.966

[Epoch: 59, batch: 129/217] total loss per batch: 0.702
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0121, 0.0156, 0.0112, 0.0215, 0.3313, 0.5955, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 59, batch: 172/217] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0055, 0.0078, 0.0263, 0.9265, 0.0160, 0.0132, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 59, batch: 215/217] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0037, 0.7412, 0.2390, 0.0076, 0.0041, 0.0017],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 60, batch: 43/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0393, 0.0092, 0.0083, 0.9084, 0.0142, 0.0151, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 60, batch: 86/217] total loss per batch: 0.709
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0040, 0.0020, 0.0081, 0.9766, 0.0029, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 60, batch: 129/217] total loss per batch: 0.703
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0175, 0.0097, 0.0205, 0.3558, 0.5719, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 60, batch: 172/217] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0011, 0.0038, 0.9658, 0.0107, 0.0116, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 60, batch: 215/217] total loss per batch: 0.699
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0071, 0.4830, 0.4933, 0.0050, 0.0049, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 61, batch: 43/217] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0299, 0.0059, 0.0053, 0.9336, 0.0107, 0.0094, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 61, batch: 86/217] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0033, 0.0066, 0.0055, 0.9687, 0.0062, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.960

[Epoch: 61, batch: 129/217] total loss per batch: 0.703
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0123, 0.0163, 0.0107, 0.0221, 0.3535, 0.5735, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 61, batch: 172/217] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0080, 0.0064, 0.0219, 0.9353, 0.0110, 0.0126, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 61, batch: 215/217] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0063, 0.6717, 0.3001, 0.0070, 0.0062, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 62, batch: 43/217] total loss per batch: 0.699
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0310, 0.0098, 0.0056, 0.9299, 0.0086, 0.0084, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 62, batch: 86/217] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0016, 0.0037, 0.0016, 0.0062, 0.9784, 0.0029, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.962

[Epoch: 62, batch: 129/217] total loss per batch: 0.701
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0152, 0.0189, 0.0109, 0.0206, 0.3414, 0.5804, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 62, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0084, 0.0032, 0.0070, 0.9525, 0.0117, 0.0142, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 62, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0100, 0.6175, 0.3467, 0.0075, 0.0080, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 63, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0170, 0.0061, 0.0076, 0.9427, 0.0107, 0.0079, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 63, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0017, 0.0016, 0.0062, 0.9810, 0.0029, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.977

[Epoch: 63, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0180, 0.0111, 0.0192, 0.3608, 0.5656, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 63, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0046, 0.0040, 0.0125, 0.9524, 0.0136, 0.0091, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 63, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0050, 0.5680, 0.4073, 0.0059, 0.0050, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 64, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0286, 0.0063, 0.0064, 0.9314, 0.0101, 0.0093, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 64, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0037, 0.0023, 0.0064, 0.9732, 0.0051, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.949

[Epoch: 64, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0202, 0.0123, 0.0211, 0.3698, 0.5489, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 64, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0078, 0.0041, 0.0125, 0.9496, 0.0121, 0.0109, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 64, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0049, 0.6888, 0.2886, 0.0062, 0.0052, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 65, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0328, 0.0066, 0.0064, 0.9323, 0.0104, 0.0065, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 65, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0028, 0.0028, 0.0060, 0.9779, 0.0036, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.965

[Epoch: 65, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0174, 0.0110, 0.0215, 0.3397, 0.5824, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 65, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0024, 0.0090, 0.9568, 0.0105, 0.0131, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 65, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0053, 0.5424, 0.4334, 0.0058, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 66, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0182, 0.0063, 0.0051, 0.9479, 0.0087, 0.0060, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 66, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0026, 0.0021, 0.0075, 0.9760, 0.0047, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.957

[Epoch: 66, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0127, 0.0188, 0.0117, 0.0189, 0.3713, 0.5549, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 66, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0088, 0.0041, 0.0089, 0.9543, 0.0116, 0.0085, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 66, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0093, 0.6904, 0.2834, 0.0053, 0.0056, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 67, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0367, 0.0099, 0.0085, 0.9188, 0.0127, 0.0064, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 67, batch: 86/217] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0052, 0.0049, 0.0058, 0.9732, 0.0043, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.967

[Epoch: 67, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0114, 0.0150, 0.0092, 0.0214, 0.3620, 0.5697, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 67, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0042, 0.0219, 0.9339, 0.0127, 0.0160, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 67, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0052, 0.5445, 0.4290, 0.0054, 0.0060, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 68, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0051, 0.0051, 0.9391, 0.0108, 0.0096, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 68, batch: 86/217] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0018, 0.0024, 0.0058, 0.9777, 0.0054, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 68, batch: 129/217] total loss per batch: 0.700
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0146, 0.0204, 0.0100, 0.0192, 0.3507, 0.5730, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 68, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0084, 0.0032, 0.0078, 0.9549, 0.0095, 0.0116, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 68, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0043, 0.6777, 0.2996, 0.0068, 0.0054, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 69, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0246, 0.0079, 0.0083, 0.9349, 0.0077, 0.0074, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 69, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0042, 0.0026, 0.0058, 0.9787, 0.0037, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.936

[Epoch: 69, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0189, 0.0160, 0.0237, 0.3596, 0.5552, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.005

[Epoch: 69, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0023, 0.0094, 0.9503, 0.0170, 0.0117, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.006

[Epoch: 69, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0024, 0.5850, 0.3967, 0.0048, 0.0051, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 70, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0232, 0.0072, 0.0051, 0.9451, 0.0082, 0.0065, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.021

[Epoch: 70, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0027, 0.0021, 0.0063, 0.9793, 0.0036, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 70, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0165, 0.0100, 0.0185, 0.3601, 0.5678, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 70, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0071, 0.0034, 0.0159, 0.9536, 0.0069, 0.0091, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 70, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0065, 0.6343, 0.3405, 0.0060, 0.0062, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 71, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0305, 0.0044, 0.0048, 0.9289, 0.0172, 0.0071, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 71, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0024, 0.0028, 0.0065, 0.9789, 0.0036, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.967

[Epoch: 71, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0134, 0.0217, 0.0102, 0.0196, 0.3258, 0.5947, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 71, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0052, 0.0033, 0.0078, 0.9497, 0.0186, 0.0123, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 71, batch: 215/217] total loss per batch: 0.695
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0034, 0.5953, 0.3863, 0.0045, 0.0046, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 72, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0313, 0.0097, 0.0067, 0.9309, 0.0084, 0.0079, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 72, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0054, 0.0033, 0.0031, 0.0063, 0.9754, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 72, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0151, 0.0101, 0.0197, 0.3806, 0.5504, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 72, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0052, 0.0030, 0.0138, 0.9550, 0.0077, 0.0102, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 72, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0061, 0.6406, 0.3380, 0.0050, 0.0048, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 73, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0254, 0.0065, 0.0069, 0.9388, 0.0093, 0.0074, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 73, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0050, 0.0056, 0.0097, 0.9624, 0.0064, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.951

[Epoch: 73, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0192, 0.0113, 0.0185, 0.3532, 0.5732, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 73, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0068, 0.0023, 0.0125, 0.9499, 0.0165, 0.0093, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 73, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0020, 0.5833, 0.3964, 0.0058, 0.0061, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 74, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0268, 0.0053, 0.0058, 0.9383, 0.0109, 0.0079, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 74, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0025, 0.0026, 0.0049, 0.9818, 0.0029, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 74, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0177, 0.0100, 0.0227, 0.3430, 0.5813, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 74, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0037, 0.0055, 0.0152, 0.9495, 0.0122, 0.0095, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 74, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0042, 0.6463, 0.3353, 0.0050, 0.0033, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 75, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0229, 0.0094, 0.0090, 0.9345, 0.0085, 0.0087, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 75, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0021, 0.0037, 0.0049, 0.9758, 0.0045, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.955

[Epoch: 75, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0117, 0.0170, 0.0121, 0.0203, 0.3627, 0.5630, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 75, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0089, 0.0018, 0.0078, 0.9562, 0.0110, 0.0115, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 75, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0032, 0.5953, 0.3834, 0.0068, 0.0045, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 76, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0381, 0.0063, 0.0059, 0.9289, 0.0109, 0.0056, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 76, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0046, 0.0043, 0.0031, 0.0077, 0.9745, 0.0031, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 76, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0201, 0.0111, 0.0228, 0.3522, 0.5645, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 76, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0038, 0.0035, 0.0105, 0.9582, 0.0130, 0.0090, 0.0019],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 76, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0063, 0.6344, 0.3438, 0.0057, 0.0048, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 77, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0160, 0.0078, 0.0063, 0.9447, 0.0107, 0.0061, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.020

[Epoch: 77, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0042, 0.0040, 0.0042, 0.9761, 0.0042, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.969

[Epoch: 77, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0159, 0.0105, 0.0212, 0.3588, 0.5683, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 77, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0117, 0.0050, 0.0166, 0.9328, 0.0127, 0.0138, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 77, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0029, 0.6154, 0.3642, 0.0059, 0.0047, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 78, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0410, 0.0082, 0.0096, 0.9203, 0.0087, 0.0063, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 78, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.0031, 0.0065, 0.9767, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.966

[Epoch: 78, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0113, 0.0148, 0.0088, 0.0181, 0.3474, 0.5868, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 78, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0038, 0.0031, 0.0111, 0.9594, 0.0120, 0.0086, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.003

[Epoch: 78, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0064, 0.6110, 0.3637, 0.0054, 0.0047, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 79, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0118, 0.0041, 0.0064, 0.9551, 0.0088, 0.0064, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 79, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0028, 0.0025, 0.0049, 0.9815, 0.0035, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 79, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0160, 0.0099, 0.0204, 0.3668, 0.5618, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 79, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0082, 0.0034, 0.0095, 0.9479, 0.0159, 0.0107, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 79, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0041, 0.6182, 0.3632, 0.0043, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.009

[Epoch: 80, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0470, 0.0065, 0.0069, 0.9194, 0.0078, 0.0066, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 80, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0023, 0.0048, 0.0055, 0.9743, 0.0036, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 80, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0150, 0.0199, 0.0127, 0.0244, 0.3497, 0.5617, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 80, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0078, 0.0032, 0.0161, 0.9474, 0.0121, 0.0112, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 80, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0035, 0.6501, 0.3303, 0.0046, 0.0044, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 81, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0132, 0.0074, 0.0102, 0.9387, 0.0146, 0.0060, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.018

[Epoch: 81, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0047, 0.0047, 0.0066, 0.9721, 0.0058, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 81, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0148, 0.0164, 0.0114, 0.0210, 0.3605, 0.5621, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 81, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0044, 0.0024, 0.0114, 0.9555, 0.0150, 0.0095, 0.0018],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 81, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0051, 0.5837, 0.3965, 0.0042, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 82, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0241, 0.0072, 0.0105, 0.9275, 0.0137, 0.0081, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 82, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0017, 0.0030, 0.0056, 0.9783, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.968

[Epoch: 82, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0134, 0.0164, 0.0099, 0.0228, 0.3555, 0.5693, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 82, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0026, 0.0092, 0.9559, 0.0094, 0.0133, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 82, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.6281, 0.3522, 0.0049, 0.0042, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 83, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0212, 0.0043, 0.0039, 0.9433, 0.0132, 0.0076, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 83, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0039, 0.0041, 0.0044, 0.9771, 0.0043, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.977

[Epoch: 83, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0177, 0.0103, 0.0204, 0.3565, 0.5694, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 83, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0042, 0.0093, 0.9536, 0.0156, 0.0082, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 83, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0046, 0.6193, 0.3637, 0.0036, 0.0038, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 84, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0408, 0.0106, 0.0071, 0.9127, 0.0136, 0.0094, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 84, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0027, 0.0033, 0.0050, 0.9765, 0.0038, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 84, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0127, 0.0151, 0.0109, 0.0220, 0.3400, 0.5852, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 84, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0053, 0.0027, 0.0114, 0.9560, 0.0112, 0.0101, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 84, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0031, 0.6222, 0.3608, 0.0043, 0.0048, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 85, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0119, 0.0065, 0.0044, 0.9544, 0.0079, 0.0076, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.020

[Epoch: 85, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0035, 0.0045, 0.0054, 0.9757, 0.0043, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 85, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0177, 0.0097, 0.0212, 0.3549, 0.5685, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 85, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0049, 0.0030, 0.0087, 0.9604, 0.0121, 0.0088, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 85, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0035, 0.6475, 0.3332, 0.0058, 0.0043, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 86, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0325, 0.0085, 0.0087, 0.9238, 0.0125, 0.0072, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.022

[Epoch: 86, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0027, 0.0021, 0.0044, 0.9792, 0.0045, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 86, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0150, 0.0186, 0.0140, 0.0224, 0.3547, 0.5600, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 86, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0032, 0.0088, 0.9585, 0.0115, 0.0086, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 86, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0045, 0.5551, 0.4253, 0.0040, 0.0054, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 87, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0210, 0.0092, 0.0058, 0.9392, 0.0102, 0.0079, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.019

[Epoch: 87, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0043, 0.0051, 0.0061, 0.9728, 0.0043, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.938

[Epoch: 87, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0134, 0.0144, 0.0101, 0.0174, 0.3602, 0.5718, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 87, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0049, 0.0047, 0.0130, 0.9446, 0.0186, 0.0120, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 87, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0090, 0.6549, 0.3112, 0.0071, 0.0083, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 88, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0274, 0.0063, 0.0066, 0.9369, 0.0098, 0.0046, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.016

[Epoch: 88, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0049, 0.0042, 0.0033, 0.0079, 0.9717, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 88, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0111, 0.0139, 0.0096, 0.0174, 0.3505, 0.5849, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 88, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0049, 0.0029, 0.0162, 0.9461, 0.0150, 0.0114, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 88, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0039, 0.5465, 0.4343, 0.0049, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 89, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0263, 0.0069, 0.0058, 0.9358, 0.0113, 0.0067, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 89, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0019, 0.0031, 0.0037, 0.9819, 0.0034, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 89, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0173, 0.0106, 0.0182, 0.3437, 0.5867, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 89, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0068, 0.0028, 0.0093, 0.9623, 0.0060, 0.0086, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 89, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0060, 0.7294, 0.2488, 0.0042, 0.0045, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 90, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0311, 0.0050, 0.0109, 0.9299, 0.0083, 0.0065, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 90, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0021, 0.0019, 0.0030, 0.9833, 0.0031, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.984

[Epoch: 90, batch: 129/217] total loss per batch: 0.699
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0174, 0.0115, 0.0170, 0.3493, 0.5792, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.017

[Epoch: 90, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0078, 0.0050, 0.0125, 0.9250, 0.0252, 0.0205, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 90, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 3
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0046, 0.4763, 0.5026, 0.0041, 0.0050, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.010

[Epoch: 91, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0193, 0.0093, 0.0060, 0.9421, 0.0117, 0.0068, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 91, batch: 86/217] total loss per batch: 0.703
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.0028, 0.0040, 0.9790, 0.0033, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 91, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0153, 0.0217, 0.0139, 0.0226, 0.3549, 0.5556, 0.0161],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 91, batch: 172/217] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0028, 0.0143, 0.9591, 0.0066, 0.0068, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 91, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0057, 0.6815, 0.2966, 0.0054, 0.0048, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 92, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0320, 0.0060, 0.0046, 0.9361, 0.0113, 0.0057, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 92, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0024, 0.0035, 0.0043, 0.9798, 0.0035, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 92, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0168, 0.0120, 0.0171, 0.3592, 0.5697, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.003

[Epoch: 92, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0040, 0.0104, 0.9531, 0.0090, 0.0116, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 92, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0064, 0.6197, 0.3495, 0.0064, 0.0069, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 93, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0196, 0.0063, 0.0058, 0.9447, 0.0093, 0.0073, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 93, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0041, 0.0035, 0.0043, 0.9751, 0.0047, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 93, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0162, 0.0098, 0.0210, 0.3558, 0.5705, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 93, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0049, 0.0038, 0.0118, 0.9499, 0.0127, 0.0097, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 93, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0040, 0.5965, 0.3847, 0.0043, 0.0051, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 94, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0477, 0.0071, 0.0080, 0.9110, 0.0141, 0.0077, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 94, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0050, 0.0034, 0.0032, 0.0038, 0.9770, 0.0037, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 94, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0154, 0.0214, 0.0126, 0.0218, 0.3595, 0.5560, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 94, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0083, 0.0036, 0.0123, 0.9464, 0.0107, 0.0140, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 94, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0036, 0.6282, 0.3525, 0.0049, 0.0047, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 95, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0095, 0.0058, 0.0055, 0.9544, 0.0106, 0.0083, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 95, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0017, 0.0025, 0.0035, 0.9845, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.967

[Epoch: 95, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0152, 0.0179, 0.0134, 0.0221, 0.3308, 0.5859, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 95, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0066, 0.0038, 0.0097, 0.9577, 0.0095, 0.0089, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.003

[Epoch: 95, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0031, 0.6213, 0.3611, 0.0044, 0.0037, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 96, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0415, 0.0096, 0.0114, 0.9112, 0.0133, 0.0064, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 96, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0036, 0.0044, 0.0045, 0.9751, 0.0051, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 96, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0150, 0.0167, 0.0114, 0.0197, 0.3571, 0.5657, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 96, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0022, 0.0095, 0.9596, 0.0115, 0.0085, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 96, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0055, 0.5971, 0.3783, 0.0056, 0.0069, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 97, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0214, 0.0079, 0.0059, 0.9414, 0.0108, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 97, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0043, 0.0028, 0.0045, 0.9786, 0.0024, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 97, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0103, 0.0131, 0.0082, 0.0179, 0.3656, 0.5742, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 97, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0036, 0.0115, 0.9483, 0.0154, 0.0089, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 97, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0040, 0.6405, 0.3316, 0.0069, 0.0059, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 98, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0199, 0.0062, 0.0039, 0.9525, 0.0088, 0.0050, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 98, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0084, 0.0037, 0.0052, 0.0049, 0.9663, 0.0065, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 98, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0128, 0.0147, 0.0102, 0.0179, 0.3771, 0.5566, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 98, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0104, 0.0040, 0.0125, 0.9394, 0.0141, 0.0140, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 98, batch: 215/217] total loss per batch: 0.694
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0030, 0.6183, 0.3657, 0.0028, 0.0046, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 99, batch: 43/217] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0386, 0.0076, 0.0055, 0.9203, 0.0100, 0.0095, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 99, batch: 86/217] total loss per batch: 0.705
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0011, 0.0016, 0.0018, 0.0041, 0.9839, 0.0037, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.974

[Epoch: 99, batch: 129/217] total loss per batch: 0.701
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0122, 0.0165, 0.0102, 0.0207, 0.3468, 0.5808, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.019

[Epoch: 99, batch: 172/217] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0037, 0.0021, 0.0076, 0.9673, 0.0069, 0.0098, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 99, batch: 215/217] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0020, 0.6077, 0.3748, 0.0034, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.009

[Epoch: 100, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0225, 0.0078, 0.0085, 0.9251, 0.0135, 0.0070, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.015

[Epoch: 100, batch: 86/217] total loss per batch: 0.707
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0012, 0.0031, 0.0047, 0.9787, 0.0058, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.947

[Epoch: 100, batch: 129/217] total loss per batch: 0.710
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0156, 0.0094, 0.0155, 0.3777, 0.5559, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 100, batch: 172/217] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0083, 0.0044, 0.0124, 0.9451, 0.0132, 0.0127, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.013

[Epoch: 100, batch: 215/217] total loss per batch: 0.696
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0057, 0.7003, 0.2747, 0.0059, 0.0084, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 101, batch: 43/217] total loss per batch: 0.698
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0226, 0.0156, 0.0070, 0.9241, 0.0118, 0.0069, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.013

[Epoch: 101, batch: 86/217] total loss per batch: 0.706
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0056, 0.0023, 0.0064, 0.9738, 0.0034, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.955

[Epoch: 101, batch: 129/217] total loss per batch: 0.707
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0183, 0.0112, 0.0237, 0.3489, 0.5715, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.016

[Epoch: 101, batch: 172/217] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0034, 0.0039, 0.0126, 0.9592, 0.0096, 0.0079, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.004

[Epoch: 101, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0035, 0.5430, 0.4383, 0.0039, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 102, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0219, 0.0078, 0.0060, 0.9423, 0.0075, 0.0091, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 102, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0033, 0.0030, 0.0044, 0.9774, 0.0030, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.946

[Epoch: 102, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0128, 0.0173, 0.0107, 0.0214, 0.3601, 0.5643, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 102, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0033, 0.0130, 0.9501, 0.0122, 0.0111, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.007

[Epoch: 102, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0032, 0.6189, 0.3613, 0.0043, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 103, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0312, 0.0088, 0.0083, 0.9299, 0.0084, 0.0075, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 103, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0022, 0.0029, 0.0025, 0.0041, 0.9807, 0.0026, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.964

[Epoch: 103, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0179, 0.0111, 0.0210, 0.3519, 0.5717, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 103, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0052, 0.0030, 0.0106, 0.9556, 0.0111, 0.0109, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.007

[Epoch: 103, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0037, 0.6337, 0.3453, 0.0045, 0.0053, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 104, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0239, 0.0067, 0.0063, 0.9400, 0.0095, 0.0073, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.012

[Epoch: 104, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0019, 0.0024, 0.0027, 0.0036, 0.9824, 0.0027, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.968

[Epoch: 104, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0173, 0.0107, 0.0201, 0.3490, 0.5772, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 104, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0029, 0.0107, 0.9542, 0.0126, 0.0104, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.007

[Epoch: 104, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.6133, 0.3669, 0.0044, 0.0049, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 105, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0259, 0.0069, 0.0076, 0.9375, 0.0094, 0.0065, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 105, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0036, 0.0026, 0.0038, 0.9796, 0.0029, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 105, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0124, 0.0162, 0.0102, 0.0196, 0.3511, 0.5777, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 105, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0029, 0.0111, 0.9526, 0.0137, 0.0104, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.008

[Epoch: 105, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.6397, 0.3413, 0.0040, 0.0047, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 106, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0281, 0.0063, 0.0068, 0.9351, 0.0101, 0.0074, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 106, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0033, 0.0030, 0.0037, 0.9803, 0.0029, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 106, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0124, 0.0168, 0.0098, 0.0192, 0.3500, 0.5786, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 106, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0030, 0.0108, 0.9542, 0.0125, 0.0101, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.006

[Epoch: 106, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0032, 0.6017, 0.3798, 0.0040, 0.0045, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 107, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0267, 0.0072, 0.0069, 0.9366, 0.0094, 0.0065, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 107, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0030, 0.0035, 0.9804, 0.0030, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 107, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0173, 0.0105, 0.0198, 0.3515, 0.5741, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 107, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0030, 0.0102, 0.9528, 0.0147, 0.0102, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 107, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0032, 0.6450, 0.3380, 0.0037, 0.0039, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 108, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0259, 0.0067, 0.0072, 0.9365, 0.0102, 0.0071, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.011

[Epoch: 108, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0034, 0.0030, 0.0033, 0.9814, 0.0029, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 108, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0175, 0.0104, 0.0199, 0.3546, 0.5703, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 108, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0065, 0.0032, 0.0095, 0.9553, 0.0120, 0.0103, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 108, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.6003, 0.3805, 0.0040, 0.0044, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 109, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0295, 0.0070, 0.0066, 0.9336, 0.0097, 0.0066, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 109, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0032, 0.0030, 0.0033, 0.9810, 0.0031, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 109, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0174, 0.0106, 0.0206, 0.3554, 0.5683, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 109, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0029, 0.0120, 0.9505, 0.0145, 0.0114, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 109, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.6294, 0.3516, 0.0043, 0.0045, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 110, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0229, 0.0063, 0.0062, 0.9402, 0.0106, 0.0074, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 110, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0049, 0.0038, 0.0040, 0.9758, 0.0034, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.974

[Epoch: 110, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0174, 0.0114, 0.0188, 0.3485, 0.5777, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 110, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0081, 0.0038, 0.0105, 0.9489, 0.0135, 0.0104, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 110, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0032, 0.6422, 0.3410, 0.0039, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 111, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0335, 0.0086, 0.0069, 0.9307, 0.0082, 0.0052, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 111, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0022, 0.0044, 0.0030, 0.9793, 0.0042, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 111, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0159, 0.0105, 0.0184, 0.3599, 0.5682, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 111, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0029, 0.0101, 0.9540, 0.0133, 0.0117, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 111, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.5580, 0.4226, 0.0036, 0.0041, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 112, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0179, 0.0038, 0.0098, 0.9380, 0.0116, 0.0084, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 112, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0033, 0.0036, 0.0041, 0.9816, 0.0028, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.953

[Epoch: 112, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0119, 0.0172, 0.0096, 0.0189, 0.3792, 0.5518, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 112, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0091, 0.0046, 0.0111, 0.9442, 0.0175, 0.0098, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.005

[Epoch: 112, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0041, 0.6780, 0.3032, 0.0049, 0.0042, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 113, batch: 43/217] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0342, 0.0062, 0.0040, 0.9344, 0.0100, 0.0057, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 113, batch: 86/217] total loss per batch: 0.702
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0026, 0.0027, 0.0043, 0.9804, 0.0036, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 113, batch: 129/217] total loss per batch: 0.697
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0150, 0.0193, 0.0123, 0.0206, 0.3376, 0.5814, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.015

[Epoch: 113, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0063, 0.0088, 0.9517, 0.0134, 0.0105, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 113, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0041, 0.6211, 0.3572, 0.0047, 0.0045, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 114, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0234, 0.0114, 0.0057, 0.9407, 0.0078, 0.0063, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 114, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0036, 0.0032, 0.0050, 0.9770, 0.0031, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.969

[Epoch: 114, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0143, 0.0179, 0.0127, 0.0195, 0.3572, 0.5644, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.007

[Epoch: 114, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0042, 0.0026, 0.0102, 0.9616, 0.0101, 0.0091, 0.0021],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.006

[Epoch: 114, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0036, 0.5818, 0.3988, 0.0043, 0.0041, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 115, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0212, 0.0051, 0.0099, 0.9371, 0.0135, 0.0043, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 115, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0021, 0.0030, 0.0039, 0.9806, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 115, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0121, 0.0157, 0.0102, 0.0214, 0.3553, 0.5720, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 115, batch: 172/217] total loss per batch: 0.683
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0047, 0.0042, 0.0100, 0.9581, 0.0101, 0.0097, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 115, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0040, 0.6626, 0.3177, 0.0042, 0.0039, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 116, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0335, 0.0056, 0.0060, 0.9344, 0.0085, 0.0054, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 116, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0031, 0.0032, 0.0043, 0.9788, 0.0038, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 116, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0121, 0.0164, 0.0109, 0.0196, 0.3554, 0.5728, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 116, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0076, 0.0045, 0.0120, 0.9435, 0.0163, 0.0126, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 116, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0031, 0.5799, 0.4035, 0.0036, 0.0039, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 117, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0257, 0.0062, 0.0064, 0.9389, 0.0099, 0.0068, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 117, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.0029, 0.0040, 0.9787, 0.0040, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 117, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0123, 0.0180, 0.0108, 0.0192, 0.3481, 0.5792, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 117, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0070, 0.0039, 0.0134, 0.9454, 0.0160, 0.0108, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 117, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0034, 0.6521, 0.3322, 0.0031, 0.0030, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 118, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0232, 0.0083, 0.0077, 0.9358, 0.0109, 0.0071, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 118, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0029, 0.0039, 0.0041, 0.9794, 0.0040, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 118, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0161, 0.0100, 0.0202, 0.3588, 0.5679, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 118, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0029, 0.0062, 0.9632, 0.0096, 0.0091, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.000

[Epoch: 118, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0045, 0.5807, 0.3976, 0.0050, 0.0042, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 119, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0315, 0.0066, 0.0053, 0.9341, 0.0111, 0.0053, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 119, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.0024, 0.0041, 0.9788, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 119, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0139, 0.0175, 0.0114, 0.0211, 0.3532, 0.5689, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 119, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0089, 0.0046, 0.0164, 0.9410, 0.0144, 0.0111, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 119, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0040, 0.6637, 0.3173, 0.0039, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 120, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0182, 0.0054, 0.0043, 0.9488, 0.0104, 0.0066, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 120, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0074, 0.0052, 0.0043, 0.0059, 0.9693, 0.0040, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 120, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0165, 0.0104, 0.0184, 0.3560, 0.5714, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 120, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0055, 0.0027, 0.0072, 0.9596, 0.0138, 0.0079, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 120, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0042, 0.5758, 0.4050, 0.0040, 0.0036, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 121, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0306, 0.0070, 0.0091, 0.9267, 0.0109, 0.0067, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 121, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0014, 0.0015, 0.0029, 0.0037, 0.9848, 0.0035, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 121, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0121, 0.0159, 0.0100, 0.0188, 0.3557, 0.5759, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 121, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0088, 0.0038, 0.0120, 0.9470, 0.0118, 0.0117, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 121, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.6470, 0.3358, 0.0038, 0.0031, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 122, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0328, 0.0077, 0.0068, 0.9325, 0.0095, 0.0059, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 122, batch: 86/217] total loss per batch: 0.701
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0020, 0.0019, 0.0025, 0.0023, 0.9850, 0.0035, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 122, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0179, 0.0115, 0.0199, 0.3525, 0.5689, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 122, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0072, 0.0036, 0.0102, 0.9528, 0.0128, 0.0110, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.003

[Epoch: 122, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0043, 0.6215, 0.3594, 0.0040, 0.0040, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 123, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0239, 0.0062, 0.0068, 0.9409, 0.0090, 0.0067, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 123, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0040, 0.0040, 0.0047, 0.9772, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.977

[Epoch: 123, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0157, 0.0197, 0.0133, 0.0217, 0.3463, 0.5688, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 123, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0029, 0.0074, 0.9571, 0.0141, 0.0090, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 123, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0036, 0.5939, 0.3846, 0.0049, 0.0056, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 124, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0267, 0.0081, 0.0066, 0.9331, 0.0116, 0.0061, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 124, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0060, 0.0036, 0.0037, 0.9742, 0.0039, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 124, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0127, 0.0162, 0.0103, 0.0199, 0.3542, 0.5733, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 124, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0061, 0.0040, 0.0121, 0.9517, 0.0132, 0.0089, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.004

[Epoch: 124, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0043, 0.6464, 0.3365, 0.0036, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 125, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0255, 0.0106, 0.0065, 0.9344, 0.0085, 0.0071, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 125, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0027, 0.0043, 0.0047, 0.9759, 0.0045, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 125, batch: 129/217] total loss per batch: 0.696
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0156, 0.0104, 0.0182, 0.3646, 0.5631, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 125, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0087, 0.0034, 0.0117, 0.9467, 0.0148, 0.0110, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.000

[Epoch: 125, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0030, 0.6136, 0.3713, 0.0024, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 126, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0322, 0.0061, 0.0058, 0.9330, 0.0104, 0.0062, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 126, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0028, 0.0024, 0.0039, 0.9830, 0.0034, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.968

[Epoch: 126, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0143, 0.0190, 0.0122, 0.0204, 0.3443, 0.5757, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 126, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0040, 0.0091, 0.9561, 0.0122, 0.0103, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 126, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0045, 0.6049, 0.3752, 0.0044, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 127, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0217, 0.0064, 0.0074, 0.9357, 0.0125, 0.0074, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 127, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0025, 0.0027, 0.0039, 0.9808, 0.0034, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 127, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0165, 0.0106, 0.0208, 0.3540, 0.5697, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 127, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0025, 0.0114, 0.9582, 0.0119, 0.0082, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 127, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0046, 0.6299, 0.3485, 0.0043, 0.0044, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 128, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0233, 0.0073, 0.0065, 0.9438, 0.0072, 0.0062, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 128, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0043, 0.0037, 0.0063, 0.9724, 0.0040, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.972

[Epoch: 128, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0118, 0.0139, 0.0095, 0.0204, 0.3635, 0.5678, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 128, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0034, 0.0075, 0.9557, 0.0157, 0.0089, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 128, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0032, 0.6525, 0.3318, 0.0034, 0.0037, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 129, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0286, 0.0051, 0.0055, 0.9358, 0.0116, 0.0086, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 129, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0026, 0.0033, 0.0033, 0.9804, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 129, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0109, 0.0156, 0.0087, 0.0178, 0.3422, 0.5927, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 129, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0071, 0.0037, 0.0133, 0.9498, 0.0128, 0.0087, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 129, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0032, 0.5792, 0.4042, 0.0026, 0.0043, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 130, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0284, 0.0094, 0.0080, 0.9291, 0.0090, 0.0058, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 130, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0028, 0.0031, 0.0028, 0.9802, 0.0036, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 130, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0175, 0.0104, 0.0185, 0.3570, 0.5686, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 130, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0057, 0.0038, 0.0087, 0.9549, 0.0132, 0.0109, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.001

[Epoch: 130, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0033, 0.6507, 0.3335, 0.0034, 0.0031, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 131, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0238, 0.0052, 0.0067, 0.9412, 0.0098, 0.0075, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 131, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0021, 0.0037, 0.0029, 0.0032, 0.9823, 0.0032, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.973

[Epoch: 131, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0188, 0.0124, 0.0205, 0.3512, 0.5692, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 131, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0094, 0.0022, 0.0085, 0.9508, 0.0130, 0.0118, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 -0.002

[Epoch: 131, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0046, 0.6116, 0.3625, 0.0051, 0.0057, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 132, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0320, 0.0061, 0.0058, 0.9334, 0.0114, 0.0055, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 132, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0043, 0.0027, 0.0034, 0.9774, 0.0038, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.983

[Epoch: 132, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0150, 0.0155, 0.0111, 0.0218, 0.3604, 0.5607, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 132, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0040, 0.0023, 0.0103, 0.9642, 0.0094, 0.0066, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 132, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0047, 0.5952, 0.3860, 0.0044, 0.0032, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 133, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0232, 0.0078, 0.0070, 0.9372, 0.0099, 0.0075, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 133, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0043, 0.0045, 0.0045, 0.9739, 0.0050, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 133, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0177, 0.0091, 0.0187, 0.3507, 0.5772, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 133, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0098, 0.0040, 0.0132, 0.9318, 0.0252, 0.0124, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 133, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.6516, 0.3329, 0.0031, 0.0038, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 134, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0307, 0.0070, 0.0065, 0.9322, 0.0097, 0.0060, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 134, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0016, 0.0034, 0.0024, 0.9839, 0.0033, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.970

[Epoch: 134, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0156, 0.0184, 0.0130, 0.0197, 0.3472, 0.5728, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 134, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0076, 0.0031, 0.0096, 0.9530, 0.0106, 0.0124, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 134, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0047, 0.6041, 0.3752, 0.0040, 0.0036, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 135, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0282, 0.0107, 0.0066, 0.9327, 0.0093, 0.0077, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 135, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0020, 0.0028, 0.0040, 0.9825, 0.0028, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.984

[Epoch: 135, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0145, 0.0169, 0.0117, 0.0179, 0.3582, 0.5650, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 135, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0027, 0.0086, 0.9600, 0.0110, 0.0094, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 135, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0048, 0.6155, 0.3633, 0.0042, 0.0048, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 136, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0175, 0.0055, 0.0065, 0.9446, 0.0106, 0.0092, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 136, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0058, 0.0068, 0.0037, 0.0038, 0.9716, 0.0044, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 136, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0124, 0.0147, 0.0099, 0.0203, 0.3589, 0.5703, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 136, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0044, 0.0131, 0.9494, 0.0128, 0.0096, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 136, batch: 215/217] total loss per batch: 0.692
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0038, 0.6534, 0.3294, 0.0035, 0.0037, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 137, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0433, 0.0062, 0.0074, 0.9217, 0.0089, 0.0059, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.000

[Epoch: 137, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0042, 0.0034, 0.0045, 0.9755, 0.0052, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.977

[Epoch: 137, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0121, 0.0170, 0.0101, 0.0179, 0.3631, 0.5676, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.020

[Epoch: 137, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0063, 0.0031, 0.0085, 0.9540, 0.0135, 0.0113, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 137, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0033, 0.5709, 0.4145, 0.0033, 0.0031, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 138, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0160, 0.0075, 0.0057, 0.9490, 0.0091, 0.0056, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 138, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0018, 0.0017, 0.0025, 0.0028, 0.9845, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 138, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0135, 0.0188, 0.0115, 0.0201, 0.3582, 0.5641, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 138, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0071, 0.0053, 0.0124, 0.9456, 0.0142, 0.0117, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 138, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0035, 0.6773, 0.3054, 0.0031, 0.0039, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 139, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0301, 0.0064, 0.0082, 0.9274, 0.0142, 0.0078, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 139, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0033, 0.0027, 0.0048, 0.9788, 0.0029, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 139, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0170, 0.0108, 0.0189, 0.3480, 0.5783, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.006

[Epoch: 139, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0056, 0.0018, 0.0068, 0.9618, 0.0118, 0.0098, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 139, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0043, 0.5816, 0.3990, 0.0040, 0.0042, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 140, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0255, 0.0077, 0.0058, 0.9382, 0.0091, 0.0060, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 140, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0045, 0.0052, 0.0040, 0.9725, 0.0045, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 140, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0154, 0.0099, 0.0196, 0.3597, 0.5706, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 140, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0076, 0.0041, 0.0117, 0.9474, 0.0150, 0.0101, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 140, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0030, 0.6563, 0.3272, 0.0032, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 141, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0252, 0.0063, 0.0074, 0.9351, 0.0116, 0.0081, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 141, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0024, 0.0024, 0.0034, 0.0045, 0.9803, 0.0039, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 141, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0157, 0.0091, 0.0176, 0.3483, 0.5828, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 141, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0031, 0.0069, 0.9577, 0.0130, 0.0101, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 141, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0037, 0.5693, 0.4144, 0.0030, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 142, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0319, 0.0086, 0.0069, 0.9303, 0.0093, 0.0057, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 142, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0023, 0.0032, 0.0036, 0.9823, 0.0026, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 142, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0187, 0.0117, 0.0191, 0.3502, 0.5724, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 142, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0057, 0.0034, 0.0136, 0.9518, 0.0122, 0.0103, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 142, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0052, 0.6683, 0.3109, 0.0045, 0.0044, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 143, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0222, 0.0056, 0.0071, 0.9417, 0.0099, 0.0051, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 143, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0055, 0.0047, 0.0040, 0.0039, 0.9737, 0.0046, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 143, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0147, 0.0174, 0.0118, 0.0211, 0.3553, 0.5663, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 143, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0056, 0.0036, 0.0082, 0.9565, 0.0140, 0.0090, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.001

[Epoch: 143, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0041, 0.5928, 0.3855, 0.0048, 0.0046, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 144, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0226, 0.0073, 0.0064, 0.9392, 0.0110, 0.0081, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 144, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0015, 0.0030, 0.0040, 0.9820, 0.0038, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 144, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0173, 0.0094, 0.0196, 0.3365, 0.5905, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 144, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0029, 0.0109, 0.9528, 0.0129, 0.0094, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 144, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0045, 0.6171, 0.3641, 0.0042, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 145, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0296, 0.0072, 0.0054, 0.9359, 0.0090, 0.0056, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 145, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0027, 0.0033, 0.0033, 0.9812, 0.0028, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 145, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0129, 0.0148, 0.0094, 0.0182, 0.3770, 0.5541, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 145, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0035, 0.0105, 0.9515, 0.0134, 0.0111, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 145, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0045, 0.6379, 0.3443, 0.0039, 0.0038, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.002

[Epoch: 146, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0292, 0.0069, 0.0069, 0.9330, 0.0095, 0.0070, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.002

[Epoch: 146, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0029, 0.0035, 0.0038, 0.9773, 0.0052, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 146, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0194, 0.0106, 0.0201, 0.3523, 0.5698, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 146, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0066, 0.0044, 0.0101, 0.9516, 0.0146, 0.0093, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 146, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.6137, 0.3678, 0.0040, 0.0043, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 147, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0249, 0.0055, 0.0080, 0.9403, 0.0092, 0.0068, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 147, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0022, 0.0026, 0.0036, 0.9841, 0.0026, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.975

[Epoch: 147, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0166, 0.0103, 0.0214, 0.3436, 0.5800, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 147, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0023, 0.0089, 0.9641, 0.0087, 0.0079, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 147, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0056, 0.6369, 0.3429, 0.0042, 0.0028, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 148, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0287, 0.0069, 0.0051, 0.9321, 0.0117, 0.0081, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 148, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.0040, 0.0032, 0.0037, 0.9778, 0.0036, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 148, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0128, 0.0158, 0.0092, 0.0178, 0.3761, 0.5557, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 148, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0038, 0.0104, 0.9514, 0.0146, 0.0108, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 148, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0040, 0.5936, 0.3876, 0.0043, 0.0042, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 149, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0306, 0.0074, 0.0091, 0.9277, 0.0099, 0.0071, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 149, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0044, 0.0056, 0.0048, 0.9723, 0.0048, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 149, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0186, 0.0100, 0.0216, 0.3435, 0.5796, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 149, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0081, 0.0033, 0.0130, 0.9433, 0.0155, 0.0115, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 149, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0028, 0.6698, 0.3147, 0.0026, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 150, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0193, 0.0061, 0.0035, 0.9509, 0.0088, 0.0058, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 150, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0025, 0.0020, 0.0037, 0.9820, 0.0039, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 150, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0139, 0.0179, 0.0107, 0.0193, 0.3601, 0.5652, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 150, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0070, 0.0038, 0.0125, 0.9494, 0.0132, 0.0110, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 150, batch: 215/217] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0043, 0.6168, 0.3639, 0.0027, 0.0037, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.008

[Epoch: 151, batch: 43/217] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0394, 0.0111, 0.0099, 0.9167, 0.0099, 0.0065, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.006

[Epoch: 151, batch: 86/217] total loss per batch: 0.704
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0070, 0.0022, 0.0051, 0.0021, 0.9799, 0.0027, 0.0011],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 151, batch: 129/217] total loss per batch: 0.698
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0148, 0.0172, 0.0101, 0.0211, 0.3486, 0.5718, 0.0165],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 151, batch: 172/217] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0036, 0.0085, 0.9550, 0.0089, 0.0131, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 151, batch: 215/217] total loss per batch: 0.693
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0055, 0.0033, 0.5875, 0.3930, 0.0030, 0.0043, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 152, batch: 43/217] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0231, 0.0086, 0.0066, 0.9397, 0.0092, 0.0068, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 152, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0052, 0.0045, 0.0041, 0.0033, 0.9767, 0.0036, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.987

[Epoch: 152, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0170, 0.0103, 0.0213, 0.3529, 0.5699, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 152, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0038, 0.0108, 0.9549, 0.0096, 0.0111, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 152, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0048, 0.0032, 0.6262, 0.3563, 0.0033, 0.0031, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 153, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0243, 0.0086, 0.0079, 0.9338, 0.0112, 0.0079, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 153, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0030, 0.0035, 0.0030, 0.9801, 0.0034, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.985

[Epoch: 153, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0169, 0.0104, 0.0213, 0.3531, 0.5698, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 153, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0039, 0.0108, 0.9524, 0.0110, 0.0115, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 153, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0045, 0.0035, 0.6185, 0.3639, 0.0032, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 154, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0248, 0.0075, 0.0070, 0.9370, 0.0104, 0.0075, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 154, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0033, 0.0037, 0.0034, 0.9791, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 154, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0168, 0.0102, 0.0210, 0.3532, 0.5706, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 154, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0065, 0.0036, 0.0107, 0.9529, 0.0116, 0.0109, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 154, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0043, 0.0034, 0.6274, 0.3551, 0.0031, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.007

[Epoch: 155, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0075, 0.0072, 0.9363, 0.0102, 0.0068, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 155, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0032, 0.0033, 0.0033, 0.9797, 0.0035, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 155, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0169, 0.0103, 0.0208, 0.3526, 0.5713, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 155, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0035, 0.0103, 0.9525, 0.0125, 0.0108, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 155, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0033, 0.6237, 0.3592, 0.0031, 0.0033, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 156, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0071, 0.0068, 0.9368, 0.0100, 0.0070, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 156, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0032, 0.0034, 0.0033, 0.9797, 0.0035, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 156, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0166, 0.0102, 0.0206, 0.3537, 0.5712, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 156, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0066, 0.0035, 0.0101, 0.9528, 0.0128, 0.0106, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 156, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0034, 0.6191, 0.3634, 0.0032, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 157, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0265, 0.0070, 0.0067, 0.9376, 0.0096, 0.0065, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 157, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0030, 0.0033, 0.0032, 0.9801, 0.0035, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 157, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0166, 0.0102, 0.0206, 0.3532, 0.5718, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 157, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0068, 0.0034, 0.0102, 0.9525, 0.0132, 0.0104, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 157, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.6286, 0.3545, 0.0032, 0.0033, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 158, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0266, 0.0070, 0.0069, 0.9348, 0.0105, 0.0074, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.002

[Epoch: 158, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0032, 0.0034, 0.0033, 0.9800, 0.0034, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 158, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0134, 0.0163, 0.0101, 0.0202, 0.3532, 0.5729, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 158, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0034, 0.0101, 0.9526, 0.0132, 0.0104, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 158, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0035, 0.6163, 0.3662, 0.0033, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 159, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0268, 0.0069, 0.0067, 0.9378, 0.0096, 0.0063, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 159, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0034, 0.0033, 0.9804, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 159, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0163, 0.0100, 0.0204, 0.3536, 0.5726, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 159, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0034, 0.0101, 0.9527, 0.0138, 0.0101, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 159, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0033, 0.6321, 0.3513, 0.0031, 0.0032, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 160, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0263, 0.0065, 0.0066, 0.9371, 0.0099, 0.0069, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 160, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.0034, 0.0034, 0.9794, 0.0034, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 160, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0163, 0.0099, 0.0201, 0.3474, 0.5792, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 160, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0067, 0.0033, 0.0106, 0.9526, 0.0129, 0.0103, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 160, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0035, 0.6122, 0.3707, 0.0033, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 161, batch: 43/217] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0277, 0.0074, 0.0069, 0.9333, 0.0107, 0.0071, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.001

[Epoch: 161, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0034, 0.0034, 0.0034, 0.9794, 0.0036, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 161, batch: 129/217] total loss per batch: 0.693
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0169, 0.0102, 0.0210, 0.3501, 0.5739, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 161, batch: 172/217] total loss per batch: 0.680
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0033, 0.0092, 0.9544, 0.0135, 0.0103, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 161, batch: 215/217] total loss per batch: 0.689
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0035, 0.6341, 0.3481, 0.0036, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 162, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0246, 0.0062, 0.0058, 0.9402, 0.0096, 0.0068, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 162, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0036, 0.0037, 0.0035, 0.9791, 0.0034, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.984

[Epoch: 162, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0132, 0.0169, 0.0106, 0.0211, 0.3623, 0.5624, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 162, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0090, 0.0034, 0.0117, 0.9489, 0.0116, 0.0109, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 162, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.6031, 0.3809, 0.0031, 0.0030, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 163, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0281, 0.0070, 0.0065, 0.9347, 0.0094, 0.0078, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 163, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0023, 0.0033, 0.0032, 0.9803, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.988

[Epoch: 163, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0142, 0.0178, 0.0109, 0.0213, 0.3534, 0.5679, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 163, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0039, 0.0036, 0.0091, 0.9568, 0.0152, 0.0090, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 163, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0034, 0.6553, 0.3256, 0.0041, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 164, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0298, 0.0063, 0.0073, 0.9321, 0.0109, 0.0071, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.001

[Epoch: 164, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0041, 0.0031, 0.0039, 0.9779, 0.0039, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 164, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0129, 0.0168, 0.0096, 0.0209, 0.3384, 0.5870, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 164, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0062, 0.0033, 0.0104, 0.9560, 0.0113, 0.0097, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 164, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0041, 0.5895, 0.3915, 0.0038, 0.0040, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.000

[Epoch: 165, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0089, 0.0068, 0.9363, 0.0091, 0.0065, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 165, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0023, 0.0023, 0.0024, 0.9851, 0.0027, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 165, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0180, 0.0115, 0.0208, 0.3610, 0.5608, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 165, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0058, 0.0028, 0.0078, 0.9556, 0.0131, 0.0110, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 165, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0036, 0.6228, 0.3576, 0.0041, 0.0041, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 166, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0234, 0.0052, 0.0074, 0.9370, 0.0115, 0.0076, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 166, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0034, 0.0031, 0.0053, 0.9771, 0.0039, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 166, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0174, 0.0103, 0.0188, 0.3545, 0.5721, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 166, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0075, 0.0033, 0.0106, 0.9503, 0.0151, 0.0096, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 166, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.6257, 0.3560, 0.0043, 0.0034, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 167, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0275, 0.0074, 0.0067, 0.9371, 0.0094, 0.0057, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 167, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0031, 0.0041, 0.0030, 0.9792, 0.0038, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.987

[Epoch: 167, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0154, 0.0090, 0.0197, 0.3526, 0.5781, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 167, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0063, 0.0032, 0.0103, 0.9533, 0.0133, 0.0100, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 167, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0033, 0.6281, 0.3565, 0.0029, 0.0030, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 168, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0300, 0.0062, 0.0072, 0.9304, 0.0113, 0.0077, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 168, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0035, 0.0032, 0.0038, 0.9790, 0.0033, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.983

[Epoch: 168, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0183, 0.0111, 0.0202, 0.3575, 0.5660, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 168, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0029, 0.0084, 0.9569, 0.0127, 0.0102, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 168, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0031, 0.6146, 0.3682, 0.0035, 0.0037, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 169, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0237, 0.0071, 0.0065, 0.9396, 0.0101, 0.0066, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 169, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.0033, 0.0031, 0.9815, 0.0034, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.988

[Epoch: 169, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0170, 0.0106, 0.0201, 0.3511, 0.5740, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 169, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0037, 0.0117, 0.9516, 0.0130, 0.0097, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 169, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0038, 0.6264, 0.3549, 0.0038, 0.0040, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 170, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0282, 0.0068, 0.0070, 0.9342, 0.0103, 0.0071, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 170, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0027, 0.0025, 0.0030, 0.9827, 0.0029, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.984

[Epoch: 170, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0131, 0.0160, 0.0097, 0.0200, 0.3543, 0.5731, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 170, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0045, 0.0023, 0.0073, 0.9639, 0.0104, 0.0087, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 170, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0041, 0.6295, 0.3528, 0.0034, 0.0032, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 171, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0238, 0.0055, 0.0055, 0.9431, 0.0089, 0.0063, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 171, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0042, 0.0041, 0.9761, 0.0040, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.983

[Epoch: 171, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0164, 0.0096, 0.0207, 0.3662, 0.5610, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 171, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0063, 0.0036, 0.0111, 0.9523, 0.0131, 0.0104, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 171, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0040, 0.6102, 0.3702, 0.0047, 0.0041, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 172, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0263, 0.0072, 0.0055, 0.9390, 0.0094, 0.0066, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 172, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0034, 0.0039, 0.0039, 0.9778, 0.0045, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.983

[Epoch: 172, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0127, 0.0180, 0.0096, 0.0182, 0.3399, 0.5887, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 172, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0097, 0.0045, 0.0127, 0.9431, 0.0167, 0.0101, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 172, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0023, 0.0027, 0.6266, 0.3610, 0.0022, 0.0024, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 173, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0283, 0.0084, 0.0075, 0.9325, 0.0105, 0.0059, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 173, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0017, 0.0029, 0.0031, 0.9824, 0.0036, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 173, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0124, 0.0166, 0.0092, 0.0186, 0.3610, 0.5704, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 173, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0025, 0.0092, 0.9561, 0.0127, 0.0108, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 173, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0031, 0.6394, 0.3470, 0.0027, 0.0023, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.003

[Epoch: 174, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0308, 0.0079, 0.0084, 0.9287, 0.0108, 0.0063, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 174, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0033, 0.0023, 0.0035, 0.9823, 0.0028, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 174, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0146, 0.0165, 0.0110, 0.0194, 0.3534, 0.5707, 0.0145],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 174, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0060, 0.0033, 0.0089, 0.9568, 0.0129, 0.0093, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 174, batch: 215/217] total loss per batch: 0.691
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0044, 0.6171, 0.3619, 0.0038, 0.0046, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.006

[Epoch: 175, batch: 43/217] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0223, 0.0052, 0.0048, 0.9432, 0.0097, 0.0087, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.010

[Epoch: 175, batch: 86/217] total loss per batch: 0.700
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0053, 0.0061, 0.0046, 0.0046, 0.9712, 0.0039, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 175, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0160, 0.0099, 0.0217, 0.3594, 0.5659, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 175, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0085, 0.0043, 0.0128, 0.9465, 0.0139, 0.0099, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 175, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0037, 0.6055, 0.3760, 0.0034, 0.0039, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 176, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0286, 0.0067, 0.0064, 0.9359, 0.0093, 0.0054, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 176, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0038, 0.0041, 0.0032, 0.9773, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.986

[Epoch: 176, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0184, 0.0107, 0.0195, 0.3484, 0.5746, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 176, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0078, 0.0036, 0.0097, 0.9505, 0.0140, 0.0109, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.006

[Epoch: 176, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0039, 0.6369, 0.3476, 0.0031, 0.0029, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.001

[Epoch: 177, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0265, 0.0060, 0.0087, 0.9335, 0.0114, 0.0062, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.009

[Epoch: 177, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0026, 0.0035, 0.0027, 0.9826, 0.0034, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.977

[Epoch: 177, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0192, 0.0118, 0.0215, 0.3574, 0.5615, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 177, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0033, 0.0131, 0.9483, 0.0129, 0.0125, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 177, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0037, 0.6098, 0.3708, 0.0044, 0.0038, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 178, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0270, 0.0064, 0.0071, 0.9367, 0.0107, 0.0058, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 178, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0030, 0.0027, 0.0032, 0.9821, 0.0031, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 178, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0126, 0.0160, 0.0099, 0.0191, 0.3496, 0.5807, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 178, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0032, 0.0085, 0.9573, 0.0127, 0.0085, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 178, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.6360, 0.3465, 0.0036, 0.0040, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.004

[Epoch: 179, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0268, 0.0061, 0.0052, 0.9382, 0.0095, 0.0085, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 179, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0035, 0.0044, 0.0039, 0.9753, 0.0042, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.985

[Epoch: 179, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0150, 0.0089, 0.0177, 0.3570, 0.5761, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 179, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0036, 0.0095, 0.9529, 0.0141, 0.0099, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 179, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.6236, 0.3592, 0.0037, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 180, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0260, 0.0067, 0.0073, 0.9395, 0.0086, 0.0051, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 180, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0033, 0.0037, 0.0035, 0.9800, 0.0036, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 180, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0127, 0.0185, 0.0101, 0.0188, 0.3581, 0.5693, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 180, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0026, 0.0105, 0.9591, 0.0106, 0.0088, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 180, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0032, 0.6170, 0.3676, 0.0028, 0.0034, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 181, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0287, 0.0065, 0.0076, 0.9317, 0.0109, 0.0072, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 181, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0027, 0.0028, 0.0038, 0.9809, 0.0032, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 181, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0140, 0.0194, 0.0116, 0.0196, 0.3497, 0.5728, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 181, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0076, 0.0045, 0.0110, 0.9485, 0.0145, 0.0104, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 181, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0027, 0.6237, 0.3605, 0.0034, 0.0036, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 182, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0293, 0.0072, 0.0057, 0.9346, 0.0102, 0.0063, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 182, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0036, 0.0030, 0.0030, 0.9796, 0.0035, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 182, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0164, 0.0102, 0.0192, 0.3418, 0.5842, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 182, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0052, 0.0026, 0.0082, 0.9592, 0.0131, 0.0092, 0.0026],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 182, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0039, 0.6177, 0.3665, 0.0030, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 183, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0208, 0.0072, 0.0079, 0.9401, 0.0102, 0.0072, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 183, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0038, 0.0038, 0.0040, 0.9769, 0.0042, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.982

[Epoch: 183, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0130, 0.0172, 0.0096, 0.0204, 0.3699, 0.5572, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 183, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0074, 0.0033, 0.0124, 0.9454, 0.0154, 0.0122, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.008

[Epoch: 183, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0040, 0.6176, 0.3638, 0.0041, 0.0036, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 184, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0365, 0.0066, 0.0052, 0.9292, 0.0098, 0.0067, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 184, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0032, 0.0030, 0.0037, 0.0034, 0.9805, 0.0035, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.966

[Epoch: 184, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0136, 0.0177, 0.0107, 0.0191, 0.3552, 0.5706, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 184, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0085, 0.0044, 0.0116, 0.9459, 0.0156, 0.0107, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 184, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0026, 0.0037, 0.6350, 0.3491, 0.0034, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.003

[Epoch: 185, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0216, 0.0076, 0.0092, 0.9369, 0.0112, 0.0066, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 185, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0025, 0.0028, 0.0032, 0.9816, 0.0029, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.965

[Epoch: 185, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0139, 0.0177, 0.0108, 0.0204, 0.3468, 0.5760, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 185, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0064, 0.0025, 0.0092, 0.9536, 0.0135, 0.0113, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 185, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0039, 0.0057, 0.5782, 0.3992, 0.0039, 0.0042, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 186, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0233, 0.0057, 0.0047, 0.9443, 0.0093, 0.0058, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.014

[Epoch: 186, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0041, 0.0046, 0.0031, 0.0039, 0.9773, 0.0036, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.981

[Epoch: 186, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0138, 0.0169, 0.0108, 0.0209, 0.3613, 0.5627, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 186, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0055, 0.0036, 0.0091, 0.9576, 0.0120, 0.0090, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 186, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0034, 0.0035, 0.6611, 0.3204, 0.0042, 0.0037, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 187, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0261, 0.0066, 0.0069, 0.9356, 0.0103, 0.0079, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.005

[Epoch: 187, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0027, 0.0030, 0.0029, 0.9825, 0.0033, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.974

[Epoch: 187, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0168, 0.0104, 0.0213, 0.3566, 0.5681, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 187, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0070, 0.0030, 0.0123, 0.9536, 0.0117, 0.0091, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 187, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0030, 0.0035, 0.6220, 0.3629, 0.0027, 0.0029, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.000

[Epoch: 188, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0333, 0.0075, 0.0076, 0.9257, 0.0108, 0.0074, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 188, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0028, 0.0026, 0.0042, 0.9813, 0.0028, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 188, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0137, 0.0179, 0.0111, 0.0197, 0.3484, 0.5761, 0.0132],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.013

[Epoch: 188, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0068, 0.0039, 0.0097, 0.9511, 0.0144, 0.0109, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 188, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0033, 0.6043, 0.3797, 0.0032, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 189, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0217, 0.0072, 0.0057, 0.9426, 0.0097, 0.0069, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.008

[Epoch: 189, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0044, 0.0045, 0.0038, 0.9765, 0.0036, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 189, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0133, 0.0160, 0.0098, 0.0184, 0.3558, 0.5739, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 189, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0054, 0.0028, 0.0090, 0.9575, 0.0140, 0.0084, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 189, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.6521, 0.3308, 0.0036, 0.0038, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 190, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0298, 0.0065, 0.0079, 0.9327, 0.0098, 0.0063, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 190, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0033, 0.0027, 0.0049, 0.9779, 0.0036, 0.0042],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 190, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0128, 0.0167, 0.0103, 0.0200, 0.3593, 0.5671, 0.0137],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 190, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0031, 0.0101, 0.9530, 0.0136, 0.0096, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 190, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0040, 0.6122, 0.3712, 0.0033, 0.0029, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 191, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0259, 0.0059, 0.0051, 0.9388, 0.0101, 0.0073, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 191, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0042, 0.0051, 0.0043, 0.0032, 0.9756, 0.0044, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.985

[Epoch: 191, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0144, 0.0170, 0.0110, 0.0204, 0.3539, 0.5694, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 191, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0077, 0.0030, 0.0099, 0.9557, 0.0119, 0.0089, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 191, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0033, 0.5969, 0.3856, 0.0036, 0.0034, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 192, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0289, 0.0065, 0.0074, 0.9346, 0.0086, 0.0063, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 192, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0018, 0.0028, 0.0030, 0.9829, 0.0032, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.978

[Epoch: 192, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0171, 0.0099, 0.0200, 0.3476, 0.5786, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.011

[Epoch: 192, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0055, 0.0037, 0.0108, 0.9493, 0.0163, 0.0111, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 192, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0028, 0.0042, 0.6366, 0.3472, 0.0031, 0.0032, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 0.001

[Epoch: 193, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0230, 0.0068, 0.0066, 0.9413, 0.0117, 0.0058, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.007

[Epoch: 193, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0027, 0.0020, 0.0025, 0.0031, 0.9844, 0.0030, 0.0024],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 193, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0134, 0.0172, 0.0098, 0.0201, 0.3473, 0.5791, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.008

[Epoch: 193, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0065, 0.0035, 0.0108, 0.9516, 0.0119, 0.0126, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 193, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0036, 0.0041, 0.6200, 0.3597, 0.0043, 0.0038, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 194, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0314, 0.0071, 0.0069, 0.9311, 0.0080, 0.0087, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 194, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0034, 0.0035, 0.0036, 0.9800, 0.0031, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.976

[Epoch: 194, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0144, 0.0094, 0.0197, 0.3502, 0.5815, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 194, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0078, 0.0027, 0.0104, 0.9518, 0.0152, 0.0086, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 194, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0037, 0.6247, 0.3557, 0.0039, 0.0037, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.005

[Epoch: 195, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0234, 0.0063, 0.0049, 0.9443, 0.0097, 0.0063, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.004

[Epoch: 195, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0044, 0.0040, 0.0036, 0.0026, 0.9774, 0.0033, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.974

[Epoch: 195, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0125, 0.0154, 0.0092, 0.0177, 0.3751, 0.5574, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.010

[Epoch: 195, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0059, 0.0033, 0.0098, 0.9582, 0.0109, 0.0088, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.007

[Epoch: 195, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0031, 0.0036, 0.6100, 0.3732, 0.0031, 0.0035, 0.0035],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.000

[Epoch: 196, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0281, 0.0066, 0.0098, 0.9318, 0.0092, 0.0061, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 196, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0040, 0.0040, 0.0042, 0.0054, 0.9740, 0.0039, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.980

[Epoch: 196, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0152, 0.0210, 0.0129, 0.0204, 0.3489, 0.5688, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.014

[Epoch: 196, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0075, 0.0053, 0.0120, 0.9459, 0.0139, 0.0116, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.004

[Epoch: 196, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0025, 0.6599, 0.3262, 0.0026, 0.0032, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.002

[Epoch: 197, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0262, 0.0076, 0.0060, 0.9328, 0.0112, 0.0082, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.003

[Epoch: 197, batch: 86/217] total loss per batch: 0.699
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0038, 0.0041, 0.0038, 0.0034, 0.9765, 0.0045, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.979

[Epoch: 197, batch: 129/217] total loss per batch: 0.695
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0153, 0.0200, 0.0125, 0.0199, 0.3416, 0.5757, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.012

[Epoch: 197, batch: 172/217] total loss per batch: 0.682
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0050, 0.0023, 0.0076, 0.9621, 0.0110, 0.0091, 0.0029],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.002

[Epoch: 197, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0035, 0.0031, 0.5811, 0.4028, 0.0028, 0.0029, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

[Epoch: 198, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0277, 0.0068, 0.0056, 0.9350, 0.0123, 0.0063, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 0.000

[Epoch: 198, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0025, 0.0029, 0.0026, 0.0034, 0.9822, 0.0028, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.985

[Epoch: 198, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0139, 0.0162, 0.0100, 0.0208, 0.3560, 0.5686, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 198, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0072, 0.0030, 0.0115, 0.9483, 0.0153, 0.0107, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.003

[Epoch: 198, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0033, 0.0035, 0.6335, 0.3489, 0.0044, 0.0034, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.003

[Epoch: 199, batch: 43/217] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.0267, 0.0067, 0.0067, 0.9367, 0.0100, 0.0067, 0.0067])
Policy pred: tensor([0.0267, 0.0075, 0.0064, 0.9373, 0.0083, 0.0073, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.005 -0.006

[Epoch: 199, batch: 86/217] total loss per batch: 0.698
Policy (actual, predicted): 4 4
Policy data: tensor([0.0033, 0.0033, 0.0033, 0.0033, 0.9800, 0.0033, 0.0033])
Policy pred: tensor([0.0037, 0.0046, 0.0040, 0.0035, 0.9775, 0.0036, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.979 -0.983

[Epoch: 199, batch: 129/217] total loss per batch: 0.694
Policy (actual, predicted): 5 5
Policy data: tensor([0.0133, 0.0167, 0.0100, 0.0200, 0.3533, 0.5733, 0.0133])
Policy pred: tensor([0.0129, 0.0150, 0.0092, 0.0183, 0.3539, 0.5790, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.009 0.009

[Epoch: 199, batch: 172/217] total loss per batch: 0.681
Policy (actual, predicted): 3 3
Policy data: tensor([0.0067, 0.0033, 0.0100, 0.9533, 0.0133, 0.0100, 0.0033])
Policy pred: tensor([0.0069, 0.0045, 0.0112, 0.9509, 0.0138, 0.0091, 0.0036],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.005 0.005

[Epoch: 199, batch: 215/217] total loss per batch: 0.690
Policy (actual, predicted): 2 2
Policy data: tensor([0.0033, 0.0033, 0.6233, 0.3600, 0.0033, 0.0033, 0.0033])
Policy pred: tensor([0.0029, 0.0036, 0.6257, 0.3569, 0.0035, 0.0032, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.000 -0.001

