Training set samples: 6416
Batch size: 32
[Epoch: 1, batch: 40/201] total loss per batch: 2.149
Policy (actual, predicted): 3 1
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0791, 0.1735, 0.1502, 0.1705, 0.1348, 0.1573, 0.1346],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 1, batch: 80/201] total loss per batch: 2.109
Policy (actual, predicted): 2 5
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0912, 0.0843, 0.1102, 0.2077, 0.1322, 0.2290, 0.1454],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.017

[Epoch: 1, batch: 120/201] total loss per batch: 2.122
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1180, 0.2259, 0.1560, 0.1661, 0.0690, 0.0850, 0.1801],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.013

[Epoch: 1, batch: 160/201] total loss per batch: 2.074
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1085, 0.2194, 0.1487, 0.1886, 0.1055, 0.1223, 0.1071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.018

[Epoch: 1, batch: 200/201] total loss per batch: 2.094
Policy (actual, predicted): 1 0
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([0.1713, 0.1320, 0.1325, 0.1335, 0.1331, 0.1523, 0.1453],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.026

[Epoch: 2, batch: 40/201] total loss per batch: 2.058
Policy (actual, predicted): 3 1
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1161, 0.1731, 0.1262, 0.1719, 0.1276, 0.1335, 0.1515],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 2, batch: 80/201] total loss per batch: 2.013
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.1465, 0.1022, 0.1688, 0.1300, 0.1229, 0.1188, 0.2107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.008

[Epoch: 2, batch: 120/201] total loss per batch: 2.026
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1202, 0.2249, 0.1448, 0.1679, 0.0825, 0.0812, 0.1785],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.037

[Epoch: 2, batch: 160/201] total loss per batch: 1.986
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1247, 0.1879, 0.1356, 0.1730, 0.1429, 0.1117, 0.1243],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.011

[Epoch: 2, batch: 200/201] total loss per batch: 2.020
Policy (actual, predicted): 1 2
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([0.1121, 0.2117, 0.3862, 0.0653, 0.0493, 0.0502, 0.1251],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.093

[Epoch: 3, batch: 40/201] total loss per batch: 1.974
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1029, 0.1837, 0.1169, 0.2228, 0.1157, 0.1366, 0.1213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.029

[Epoch: 3, batch: 80/201] total loss per batch: 1.938
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.1760, 0.1579, 0.1474, 0.0573, 0.1384, 0.1379, 0.1850],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.023

[Epoch: 3, batch: 120/201] total loss per batch: 1.911
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1393, 0.2267, 0.1349, 0.1417, 0.1003, 0.1084, 0.1487],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 3, batch: 160/201] total loss per batch: 1.879
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1103, 0.2056, 0.1269, 0.1722, 0.1618, 0.1056, 0.1175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.010

[Epoch: 3, batch: 200/201] total loss per batch: 1.845
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([0.0542, 0.6675, 0.2226, 0.0017, 0.0121, 0.0124, 0.0296],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.050

[Epoch: 4, batch: 40/201] total loss per batch: 1.843
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1410, 0.1869, 0.1231, 0.1968, 0.1175, 0.1260, 0.1088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.008

[Epoch: 4, batch: 80/201] total loss per batch: 1.831
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.1347, 0.1194, 0.1759, 0.0036, 0.2142, 0.1242, 0.2280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.035

[Epoch: 4, batch: 120/201] total loss per batch: 1.819
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1344, 0.2549, 0.1247, 0.1455, 0.0878, 0.0983, 0.1544],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 4, batch: 160/201] total loss per batch: 1.819
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1326, 0.2543, 0.1074, 0.1560, 0.1255, 0.0566, 0.1675],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.021

[Epoch: 4, batch: 200/201] total loss per batch: 1.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([0.0294, 0.6465, 0.2581, 0.0026, 0.0096, 0.0195, 0.0344],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.066

[Epoch: 5, batch: 40/201] total loss per batch: 1.795
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1376, 0.1682, 0.1237, 0.2228, 0.1117, 0.1550, 0.0811],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.018

[Epoch: 5, batch: 80/201] total loss per batch: 1.764
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.1056, 0.1546, 0.1713, 0.0056, 0.1962, 0.1435, 0.2232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.058

[Epoch: 5, batch: 120/201] total loss per batch: 1.763
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1293, 0.2761, 0.0995, 0.1699, 0.0990, 0.1014, 0.1248],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.025

[Epoch: 5, batch: 160/201] total loss per batch: 1.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1322, 0.2151, 0.1101, 0.1836, 0.1169, 0.0579, 0.1842],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.028

[Epoch: 5, batch: 200/201] total loss per batch: 1.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([0.0046, 0.9156, 0.0564, 0.0011, 0.0046, 0.0043, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.042

[Epoch: 6, batch: 40/201] total loss per batch: 1.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1322, 0.1505, 0.1252, 0.2677, 0.1034, 0.1519, 0.0691],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.014

[Epoch: 6, batch: 80/201] total loss per batch: 1.712
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0815, 0.1852, 0.2139, 0.0022, 0.1476, 0.1832, 0.1863],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 6, batch: 120/201] total loss per batch: 1.705
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1755, 0.3575, 0.0629, 0.1307, 0.0891, 0.0877, 0.0965],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.002

[Epoch: 6, batch: 160/201] total loss per batch: 1.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1327, 0.2244, 0.0767, 0.1977, 0.1113, 0.0612, 0.1960],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 6, batch: 200/201] total loss per batch: 1.647
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9700e-04, 9.7056e-01, 2.3032e-02, 1.9870e-04, 1.8429e-03, 2.3271e-04,
        3.3398e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.041

[Epoch: 7, batch: 40/201] total loss per batch: 1.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1175, 0.1158, 0.1116, 0.3892, 0.0995, 0.1066, 0.0599],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.028

[Epoch: 7, batch: 80/201] total loss per batch: 1.653
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0603, 0.1679, 0.1822, 0.0014, 0.1426, 0.2176, 0.2280],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.034

[Epoch: 7, batch: 120/201] total loss per batch: 1.654
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1889, 0.3085, 0.0782, 0.1484, 0.0913, 0.0933, 0.0913],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 7, batch: 160/201] total loss per batch: 1.667
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1264, 0.2318, 0.0668, 0.1813, 0.1248, 0.0796, 0.1893],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.023

[Epoch: 7, batch: 200/201] total loss per batch: 1.591
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0626e-04, 9.5515e-01, 3.9325e-02, 2.6756e-04, 3.2059e-03, 2.3505e-04,
        1.0058e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.041

[Epoch: 8, batch: 40/201] total loss per batch: 1.636
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1236, 0.1207, 0.0886, 0.4894, 0.0544, 0.0753, 0.0480],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.028

[Epoch: 8, batch: 80/201] total loss per batch: 1.609
Policy (actual, predicted): 2 6
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0295, 0.1905, 0.2320, 0.0011, 0.1051, 0.1932, 0.2487],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.008

[Epoch: 8, batch: 120/201] total loss per batch: 1.622
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1708, 0.2589, 0.0904, 0.1424, 0.1199, 0.1287, 0.0890],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.004

[Epoch: 8, batch: 160/201] total loss per batch: 1.610
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1368, 0.3029, 0.0395, 0.1761, 0.0794, 0.0596, 0.2058],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.009

[Epoch: 8, batch: 200/201] total loss per batch: 1.529
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4490e-04, 9.7101e-01, 2.7804e-02, 2.3738e-04, 6.0054e-04, 4.9167e-05,
        1.5027e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.031

[Epoch: 9, batch: 40/201] total loss per batch: 1.580
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1376, 0.1158, 0.0604, 0.5527, 0.0316, 0.0547, 0.0471],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.023

[Epoch: 9, batch: 80/201] total loss per batch: 1.545
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0313, 0.1147, 0.3042, 0.0013, 0.1343, 0.2248, 0.1893],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.027

[Epoch: 9, batch: 120/201] total loss per batch: 1.576
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2149, 0.2489, 0.0930, 0.1046, 0.1113, 0.1477, 0.0796],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 9, batch: 160/201] total loss per batch: 1.546
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1023, 0.3169, 0.0549, 0.2164, 0.0672, 0.0557, 0.1866],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.005

[Epoch: 9, batch: 200/201] total loss per batch: 1.470
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6816e-04, 9.7402e-01, 2.3715e-02, 2.3459e-04, 1.7163e-03, 1.8582e-05,
        1.3066e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.064

[Epoch: 10, batch: 40/201] total loss per batch: 1.517
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0608, 0.0470, 0.0399, 0.7854, 0.0143, 0.0253, 0.0274],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.022

[Epoch: 10, batch: 80/201] total loss per batch: 1.504
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0169, 0.0590, 0.3855, 0.0020, 0.1416, 0.2487, 0.1464],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.003

[Epoch: 10, batch: 120/201] total loss per batch: 1.544
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2005, 0.2613, 0.0851, 0.1412, 0.1093, 0.1394, 0.0632],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 10, batch: 160/201] total loss per batch: 1.488
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1249, 0.3796, 0.0406, 0.1728, 0.0494, 0.0757, 0.1568],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.008

[Epoch: 10, batch: 200/201] total loss per batch: 1.407
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9681e-05, 9.8084e-01, 1.8677e-02, 6.7704e-05, 2.8907e-04, 1.3231e-05,
        7.2559e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.063

[Epoch: 11, batch: 40/201] total loss per batch: 1.456
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1326, 0.0534, 0.0339, 0.6945, 0.0130, 0.0275, 0.0452],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.009

[Epoch: 11, batch: 80/201] total loss per batch: 1.437
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([0.0066, 0.0424, 0.6814, 0.0014, 0.0467, 0.1291, 0.0924],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.034

[Epoch: 11, batch: 120/201] total loss per batch: 1.484
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1774, 0.2936, 0.0626, 0.0912, 0.1877, 0.1419, 0.0456],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 11, batch: 160/201] total loss per batch: 1.403
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1395, 0.3416, 0.0316, 0.2162, 0.0349, 0.0597, 0.1765],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.015

[Epoch: 11, batch: 200/201] total loss per batch: 1.348
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3779e-05, 9.7921e-01, 2.0247e-02, 1.0467e-04, 2.8168e-04, 2.5957e-05,
        1.0427e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.123

[Epoch: 12, batch: 40/201] total loss per batch: 1.396
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1068, 0.0693, 0.1387, 0.6016, 0.0108, 0.0445, 0.0283],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.025

[Epoch: 12, batch: 80/201] total loss per batch: 1.379
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.4922e-03, 2.6461e-02, 8.9598e-01, 1.7726e-05, 2.1026e-02, 2.0960e-02,
        3.3068e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.070

[Epoch: 12, batch: 120/201] total loss per batch: 1.430
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3088, 0.3772, 0.0207, 0.0868, 0.0730, 0.1129, 0.0206],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 12, batch: 160/201] total loss per batch: 1.339
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1674, 0.4148, 0.0229, 0.2682, 0.0162, 0.0273, 0.0830],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.055

[Epoch: 12, batch: 200/201] total loss per batch: 1.299
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.3088e-05, 9.4567e-01, 5.2738e-02, 3.4141e-05, 1.3785e-03, 1.4887e-05,
        1.3389e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.059

[Epoch: 13, batch: 40/201] total loss per batch: 1.320
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1064, 0.0257, 0.0384, 0.7724, 0.0072, 0.0241, 0.0258],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.022

[Epoch: 13, batch: 80/201] total loss per batch: 1.307
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.7948e-03, 3.5642e-02, 8.4159e-01, 1.9752e-05, 3.3745e-02, 4.4828e-02,
        4.0378e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.105

[Epoch: 13, batch: 120/201] total loss per batch: 1.364
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.1939, 0.4455, 0.0086, 0.1228, 0.1113, 0.1060, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 13, batch: 160/201] total loss per batch: 1.259
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0824, 0.4664, 0.0090, 0.1922, 0.0371, 0.0280, 0.1850],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.075

[Epoch: 13, batch: 200/201] total loss per batch: 1.264
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4525e-06, 9.7642e-01, 2.3283e-02, 6.3923e-06, 1.3649e-04, 2.4742e-06,
        1.4399e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.246

[Epoch: 14, batch: 40/201] total loss per batch: 1.272
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.2269, 0.0399, 0.0221, 0.6236, 0.0112, 0.0501, 0.0262],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.044

[Epoch: 14, batch: 80/201] total loss per batch: 1.251
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3751e-03, 2.2354e-02, 9.0376e-01, 1.1884e-05, 3.8125e-02, 2.1898e-02,
        1.2473e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.039

[Epoch: 14, batch: 120/201] total loss per batch: 1.285
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4462, 0.3613, 0.0094, 0.0637, 0.0300, 0.0834, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.045

[Epoch: 14, batch: 160/201] total loss per batch: 1.223
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.1121, 0.7028, 0.0249, 0.0574, 0.0122, 0.0692, 0.0214],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.048

[Epoch: 14, batch: 200/201] total loss per batch: 1.195
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7064e-06, 8.3890e-01, 1.6048e-01, 3.0250e-05, 4.8316e-04, 4.7661e-06,
        9.8804e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.108

[Epoch: 15, batch: 40/201] total loss per batch: 1.205
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0501, 0.0238, 0.0863, 0.8120, 0.0060, 0.0129, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.030

[Epoch: 15, batch: 80/201] total loss per batch: 1.204
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.1670e-03, 8.0453e-02, 7.6207e-01, 4.7062e-05, 2.3843e-02, 8.5264e-02,
        4.3152e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 15, batch: 120/201] total loss per batch: 1.224
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3132, 0.2920, 0.0043, 0.1521, 0.1094, 0.1250, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.093

[Epoch: 15, batch: 160/201] total loss per batch: 1.148
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0582, 0.8384, 0.0042, 0.0148, 0.0089, 0.0216, 0.0538],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.066

[Epoch: 15, batch: 200/201] total loss per batch: 1.163
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1692e-05, 9.9116e-01, 8.2517e-03, 1.3933e-04, 2.2048e-04, 2.4011e-07,
        1.5365e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 -0.201

[Epoch: 16, batch: 40/201] total loss per batch: 1.166
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1490, 0.0429, 0.0605, 0.5375, 0.0304, 0.1649, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.007

[Epoch: 16, batch: 80/201] total loss per batch: 1.177
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.2503e-03, 4.8834e-02, 8.1694e-01, 7.1127e-06, 6.0103e-02, 4.1652e-02,
        2.8216e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.047

[Epoch: 16, batch: 120/201] total loss per batch: 1.173
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4053, 0.2612, 0.0090, 0.1239, 0.1025, 0.0907, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.088

[Epoch: 16, batch: 160/201] total loss per batch: 1.112
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0360, 0.8668, 0.0162, 0.0201, 0.0068, 0.0303, 0.0239],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.036

[Epoch: 16, batch: 200/201] total loss per batch: 1.123
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9264e-04, 8.6195e-01, 1.3621e-01, 1.5060e-04, 5.3967e-05, 7.9415e-07,
        1.4365e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.315

[Epoch: 17, batch: 40/201] total loss per batch: 1.131
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1074, 0.0388, 0.0319, 0.7186, 0.0121, 0.0263, 0.0650],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.008

[Epoch: 17, batch: 80/201] total loss per batch: 1.101
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.7577e-03, 4.0518e-02, 9.0873e-01, 2.7295e-06, 2.5622e-02, 2.0156e-02,
        3.2124e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.101

[Epoch: 17, batch: 120/201] total loss per batch: 1.122
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4209, 0.2720, 0.0036, 0.0726, 0.1835, 0.0461, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 17, batch: 160/201] total loss per batch: 1.054
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0872, 0.8043, 0.0117, 0.0078, 0.0232, 0.0383, 0.0276],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 17, batch: 200/201] total loss per batch: 1.055
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7758e-04, 9.2134e-01, 7.3088e-02, 2.1600e-04, 1.9417e-04, 6.8004e-06,
        4.9756e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.113

[Epoch: 18, batch: 40/201] total loss per batch: 1.079
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0747, 0.0499, 0.0063, 0.4351, 0.0201, 0.3710, 0.0430],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 18, batch: 80/201] total loss per batch: 1.074
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.9497e-04, 1.5788e-02, 9.6548e-01, 9.1685e-06, 1.1776e-02, 2.9154e-03,
        3.6319e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.011

[Epoch: 18, batch: 120/201] total loss per batch: 1.101
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5718, 0.1626, 0.0069, 0.0457, 0.0635, 0.1487, 0.0009],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.035

[Epoch: 18, batch: 160/201] total loss per batch: 1.021
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0065, 0.8551, 0.0072, 0.0352, 0.0070, 0.0075, 0.0815],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.086

[Epoch: 18, batch: 200/201] total loss per batch: 1.020
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.0861e-05, 9.4785e-01, 5.1463e-02, 1.0007e-04, 6.4812e-05, 2.8291e-05,
        4.0026e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.570

[Epoch: 19, batch: 40/201] total loss per batch: 1.034
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0616, 0.0088, 0.0257, 0.8078, 0.0216, 0.0486, 0.0259],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.070

[Epoch: 19, batch: 80/201] total loss per batch: 1.040
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.6557e-03, 9.9660e-02, 7.7995e-01, 2.7259e-05, 5.5509e-02, 4.3356e-02,
        1.6843e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.040

[Epoch: 19, batch: 120/201] total loss per batch: 1.035
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5663, 0.1029, 0.0080, 0.1146, 0.0855, 0.1214, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.062

[Epoch: 19, batch: 160/201] total loss per batch: 0.998
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0244, 0.7378, 0.0065, 0.0185, 0.0238, 0.0649, 0.1242],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.055

[Epoch: 19, batch: 200/201] total loss per batch: 0.965
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.5252e-04, 9.4938e-01, 4.8551e-02, 8.1356e-05, 1.4575e-04, 2.8292e-05,
        1.4615e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.428

[Epoch: 20, batch: 40/201] total loss per batch: 0.986
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1528, 0.0123, 0.0647, 0.6059, 0.0260, 0.1038, 0.0346],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.013

[Epoch: 20, batch: 80/201] total loss per batch: 1.003
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.8662e-04, 1.4538e-02, 9.5129e-01, 6.1200e-06, 2.5718e-02, 6.7443e-03,
        1.1166e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.031

[Epoch: 20, batch: 120/201] total loss per batch: 1.006
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3221, 0.2819, 0.0021, 0.1321, 0.0910, 0.1689, 0.0020],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.041

[Epoch: 20, batch: 160/201] total loss per batch: 0.970
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0605, 0.7749, 0.0299, 0.0308, 0.0132, 0.0471, 0.0436],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.090

[Epoch: 20, batch: 200/201] total loss per batch: 0.944
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.2222e-05, 9.7986e-01, 1.9859e-02, 6.5927e-05, 3.0022e-05, 9.4046e-06,
        1.3571e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.904

[Epoch: 21, batch: 40/201] total loss per batch: 0.963
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0518, 0.0462, 0.0151, 0.8026, 0.0157, 0.0485, 0.0201],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.041

[Epoch: 21, batch: 80/201] total loss per batch: 0.963
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.7006e-04, 1.6805e-02, 9.5942e-01, 2.4753e-06, 1.0409e-02, 8.5428e-03,
        3.8490e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.084

[Epoch: 21, batch: 120/201] total loss per batch: 0.988
Policy (actual, predicted): 0 1
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2805, 0.3628, 0.0018, 0.0482, 0.1503, 0.1549, 0.0014],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.036

[Epoch: 21, batch: 160/201] total loss per batch: 0.925
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0182, 0.8222, 0.0143, 0.0258, 0.0160, 0.0369, 0.0666],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.088

[Epoch: 21, batch: 200/201] total loss per batch: 0.914
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5627e-04, 7.7377e-01, 2.2514e-01, 4.4071e-05, 1.5583e-04, 1.2032e-05,
        7.1359e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.712

[Epoch: 22, batch: 40/201] total loss per batch: 0.933
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0664, 0.0168, 0.0236, 0.7259, 0.0075, 0.1468, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.041

[Epoch: 22, batch: 80/201] total loss per batch: 0.955
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5870e-03, 9.5081e-02, 8.1402e-01, 1.1439e-05, 5.0801e-02, 2.8227e-02,
        1.0278e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.000

[Epoch: 22, batch: 120/201] total loss per batch: 0.954
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([5.0232e-01, 1.0506e-01, 1.4788e-03, 1.7868e-02, 3.0682e-01, 6.5997e-02,
        4.5148e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.046

[Epoch: 22, batch: 160/201] total loss per batch: 0.904
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0230, 0.9088, 0.0103, 0.0144, 0.0095, 0.0146, 0.0193],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.096

[Epoch: 22, batch: 200/201] total loss per batch: 0.890
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2887e-05, 9.4862e-01, 5.0229e-02, 4.5571e-04, 1.4575e-04, 1.0447e-04,
        3.9242e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.919

[Epoch: 23, batch: 40/201] total loss per batch: 0.915
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0676, 0.0509, 0.0723, 0.6127, 0.0491, 0.1342, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.035

[Epoch: 23, batch: 80/201] total loss per batch: 0.930
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.0742e-03, 3.0276e-02, 9.2139e-01, 3.5895e-06, 2.6909e-02, 1.2772e-02,
        6.5806e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.061

[Epoch: 23, batch: 120/201] total loss per batch: 0.923
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4093, 0.3123, 0.0066, 0.0852, 0.0299, 0.1559, 0.0008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.062

[Epoch: 23, batch: 160/201] total loss per batch: 0.898
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0098, 0.9162, 0.0072, 0.0062, 0.0120, 0.0369, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.067

[Epoch: 23, batch: 200/201] total loss per batch: 0.870
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4980e-04, 8.9319e-01, 1.0442e-01, 9.6061e-05, 1.3269e-04, 1.1608e-04,
        1.4938e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 24, batch: 40/201] total loss per batch: 0.881
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0596, 0.0958, 0.0341, 0.6522, 0.0090, 0.1354, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.040

[Epoch: 24, batch: 80/201] total loss per batch: 0.902
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.4773e-04, 2.4314e-02, 9.2593e-01, 1.0349e-06, 3.5276e-02, 1.2006e-02,
        1.5307e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.065

[Epoch: 24, batch: 120/201] total loss per batch: 0.909
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6317, 0.0565, 0.0036, 0.0267, 0.0683, 0.2119, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.066

[Epoch: 24, batch: 160/201] total loss per batch: 0.874
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0105, 0.9228, 0.0041, 0.0121, 0.0127, 0.0123, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.086

[Epoch: 24, batch: 200/201] total loss per batch: 0.853
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0750e-04, 9.2374e-01, 6.8864e-02, 1.0997e-04, 1.3847e-03, 1.9955e-04,
        5.5957e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.910

[Epoch: 25, batch: 40/201] total loss per batch: 0.874
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0393, 0.0111, 0.0110, 0.8160, 0.0076, 0.0997, 0.0153],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 25, batch: 80/201] total loss per batch: 0.883
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.3555e-04, 4.3507e-02, 9.3336e-01, 4.1786e-06, 1.1496e-02, 7.8844e-03,
        2.9154e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.032

[Epoch: 25, batch: 120/201] total loss per batch: 0.889
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4765, 0.2031, 0.0048, 0.0327, 0.1226, 0.1592, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.101

[Epoch: 25, batch: 160/201] total loss per batch: 0.874
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0056, 0.8952, 0.0188, 0.0254, 0.0221, 0.0251, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.043

[Epoch: 25, batch: 200/201] total loss per batch: 0.839
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9777e-04, 9.1275e-01, 8.3328e-02, 2.8444e-04, 9.7434e-05, 1.2582e-04,
        3.2190e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.963

[Epoch: 26, batch: 40/201] total loss per batch: 0.855
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1277, 0.0481, 0.0250, 0.6271, 0.0285, 0.1120, 0.0315],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 26, batch: 80/201] total loss per batch: 0.875
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.6851e-03, 7.5498e-02, 8.3331e-01, 5.3343e-06, 5.2390e-02, 2.6177e-02,
        1.0931e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.030

[Epoch: 26, batch: 120/201] total loss per batch: 0.882
Policy (actual, predicted): 0 5
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2459, 0.1672, 0.0024, 0.0239, 0.1528, 0.4065, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.032

[Epoch: 26, batch: 160/201] total loss per batch: 0.847
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0080, 0.9454, 0.0051, 0.0029, 0.0028, 0.0118, 0.0240],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.094

[Epoch: 26, batch: 200/201] total loss per batch: 0.824
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3745e-05, 9.1851e-01, 7.9939e-02, 4.9968e-05, 8.5091e-04, 2.5391e-05,
        5.7633e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.973

[Epoch: 27, batch: 40/201] total loss per batch: 0.851
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0382, 0.0482, 0.0520, 0.7710, 0.0103, 0.0736, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.028

[Epoch: 27, batch: 80/201] total loss per batch: 0.869
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3637e-03, 2.1751e-02, 9.5050e-01, 2.8809e-06, 1.4964e-02, 6.8958e-03,
        4.5201e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.128

[Epoch: 27, batch: 120/201] total loss per batch: 0.861
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6412, 0.0950, 0.0077, 0.0360, 0.1609, 0.0582, 0.0010],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.027

[Epoch: 27, batch: 160/201] total loss per batch: 0.841
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0107, 0.9623, 0.0063, 0.0033, 0.0089, 0.0050, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.055

[Epoch: 27, batch: 200/201] total loss per batch: 0.819
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.6717e-04, 6.7343e-01, 3.2379e-01, 1.7582e-04, 6.0567e-04, 1.3727e-04,
        1.1910e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.692

[Epoch: 28, batch: 40/201] total loss per batch: 0.835
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0803, 0.1318, 0.0255, 0.6216, 0.0204, 0.0984, 0.0221],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 28, batch: 80/201] total loss per batch: 0.850
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.0807e-04, 5.0504e-02, 9.1463e-01, 5.1711e-06, 1.4566e-02, 1.2399e-02,
        6.9890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.100

[Epoch: 28, batch: 120/201] total loss per batch: 0.861
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4963, 0.0707, 0.0109, 0.1485, 0.1797, 0.0923, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 28, batch: 160/201] total loss per batch: 0.829
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0044, 0.9434, 0.0043, 0.0049, 0.0189, 0.0093, 0.0147],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.055

[Epoch: 28, batch: 200/201] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.5581e-04, 9.0068e-01, 9.4403e-02, 6.6791e-04, 1.5695e-03, 3.3323e-04,
        2.0955e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.995

[Epoch: 29, batch: 40/201] total loss per batch: 0.822
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0366, 0.0218, 0.1112, 0.6287, 0.0075, 0.1826, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 29, batch: 80/201] total loss per batch: 0.835
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.0413e-03, 2.5805e-02, 9.0237e-01, 6.8580e-06, 3.8468e-02, 2.0892e-02,
        1.0420e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.026

[Epoch: 29, batch: 120/201] total loss per batch: 0.835
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2576, 0.1073, 0.0027, 0.0056, 0.5280, 0.0980, 0.0008],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.038

[Epoch: 29, batch: 160/201] total loss per batch: 0.809
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0072, 0.9334, 0.0063, 0.0077, 0.0053, 0.0051, 0.0351],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.052

[Epoch: 29, batch: 200/201] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2590e-04, 8.6839e-01, 1.2889e-01, 3.2665e-04, 5.1641e-04, 4.9564e-05,
        1.5049e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.991

[Epoch: 30, batch: 40/201] total loss per batch: 0.813
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0430, 0.0405, 0.0051, 0.8752, 0.0054, 0.0257, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 30, batch: 80/201] total loss per batch: 0.829
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.2163e-03, 6.3323e-02, 8.8964e-01, 1.2655e-05, 2.1935e-02, 1.1315e-02,
        1.1561e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.026

[Epoch: 30, batch: 120/201] total loss per batch: 0.840
Policy (actual, predicted): 0 5
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3055, 0.1492, 0.0068, 0.0749, 0.0220, 0.4403, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.028

[Epoch: 30, batch: 160/201] total loss per batch: 0.803
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0140, 0.9379, 0.0063, 0.0118, 0.0096, 0.0091, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.036

[Epoch: 30, batch: 200/201] total loss per batch: 0.786
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7121e-04, 8.7492e-01, 1.2349e-01, 2.9709e-04, 4.2420e-04, 3.6467e-05,
        5.6657e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.986

[Epoch: 31, batch: 40/201] total loss per batch: 0.809
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0459, 0.0414, 0.0878, 0.4831, 0.0114, 0.3183, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 31, batch: 80/201] total loss per batch: 0.812
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.2590e-03, 5.6424e-02, 8.9009e-01, 3.3068e-06, 2.9032e-02, 1.5469e-02,
        6.7206e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.008

[Epoch: 31, batch: 120/201] total loss per batch: 0.819
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.7457, 0.0436, 0.0046, 0.0760, 0.0847, 0.0442, 0.0012],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.035

[Epoch: 31, batch: 160/201] total loss per batch: 0.810
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0081, 0.8461, 0.0050, 0.0060, 0.0082, 0.0085, 0.1181],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 31, batch: 200/201] total loss per batch: 0.793
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8761e-04, 8.1818e-01, 1.7957e-01, 6.1878e-04, 2.7363e-04, 2.2512e-04,
        9.4619e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.948

[Epoch: 32, batch: 40/201] total loss per batch: 0.801
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0256, 0.0246, 0.0099, 0.8997, 0.0125, 0.0214, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.024

[Epoch: 32, batch: 80/201] total loss per batch: 0.814
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.4227e-04, 1.4676e-02, 9.6285e-01, 2.6521e-06, 1.0317e-02, 4.4088e-03,
        6.9011e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.080

[Epoch: 32, batch: 120/201] total loss per batch: 0.818
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5083, 0.0708, 0.0099, 0.0263, 0.2088, 0.1743, 0.0016],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.059

[Epoch: 32, batch: 160/201] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0049, 0.9754, 0.0028, 0.0028, 0.0057, 0.0023, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.038

[Epoch: 32, batch: 200/201] total loss per batch: 0.768
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.4640e-04, 8.2404e-01, 1.7280e-01, 5.7347e-04, 5.6320e-04, 2.1824e-04,
        1.3644e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.973

[Epoch: 33, batch: 40/201] total loss per batch: 0.792
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0478, 0.0369, 0.0118, 0.8437, 0.0058, 0.0467, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.025

[Epoch: 33, batch: 80/201] total loss per batch: 0.811
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3271e-03, 1.0581e-02, 9.1247e-01, 4.8690e-06, 5.0613e-02, 1.5463e-02,
        9.5386e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.016

[Epoch: 33, batch: 120/201] total loss per batch: 0.814
Policy (actual, predicted): 0 5
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2151, 0.1621, 0.0067, 0.0505, 0.2812, 0.2831, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 33, batch: 160/201] total loss per batch: 0.792
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0133, 0.8636, 0.0108, 0.0160, 0.0225, 0.0237, 0.0502],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 33, batch: 200/201] total loss per batch: 0.772
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9855e-04, 7.5597e-01, 2.3651e-01, 1.9794e-03, 9.0568e-04, 3.0707e-04,
        4.0249e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.977

[Epoch: 34, batch: 40/201] total loss per batch: 0.788
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0419, 0.0647, 0.0565, 0.7673, 0.0262, 0.0344, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 34, batch: 80/201] total loss per batch: 0.794
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.0171e-03, 5.3660e-02, 8.5729e-01, 4.4783e-06, 5.6979e-02, 1.5799e-02,
        1.4247e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.025

[Epoch: 34, batch: 120/201] total loss per batch: 0.800
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6714, 0.0356, 0.0118, 0.0628, 0.1173, 0.0984, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.046

[Epoch: 34, batch: 160/201] total loss per batch: 0.782
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0153, 0.9560, 0.0050, 0.0081, 0.0049, 0.0037, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.057

[Epoch: 34, batch: 200/201] total loss per batch: 0.769
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1908e-04, 9.0946e-01, 8.9127e-02, 3.3716e-04, 2.0027e-04, 1.5858e-04,
        2.9715e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 35, batch: 40/201] total loss per batch: 0.778
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0700, 0.0403, 0.0425, 0.6841, 0.0223, 0.1343, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 35, batch: 80/201] total loss per batch: 0.792
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.1340e-03, 2.9977e-02, 9.1549e-01, 1.8981e-06, 2.7819e-02, 8.9138e-03,
        1.5666e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 35, batch: 120/201] total loss per batch: 0.795
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3061, 0.1477, 0.0049, 0.0342, 0.3301, 0.1757, 0.0013],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.047

[Epoch: 35, batch: 160/201] total loss per batch: 0.777
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0045, 0.9481, 0.0044, 0.0081, 0.0076, 0.0060, 0.0213],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 35, batch: 200/201] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8724e-04, 6.8442e-01, 3.1199e-01, 1.4484e-04, 8.7587e-04, 2.0180e-04,
        2.1769e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.987

[Epoch: 36, batch: 40/201] total loss per batch: 0.774
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0361, 0.0233, 0.0244, 0.8348, 0.0081, 0.0669, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 36, batch: 80/201] total loss per batch: 0.783
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.7840e-03, 1.4400e-01, 6.6876e-01, 1.5751e-05, 8.9049e-02, 5.3410e-02,
        3.6978e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.025

[Epoch: 36, batch: 120/201] total loss per batch: 0.791
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5761, 0.0706, 0.0065, 0.0516, 0.1646, 0.1274, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 36, batch: 160/201] total loss per batch: 0.774
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0060, 0.9124, 0.0169, 0.0141, 0.0048, 0.0102, 0.0355],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.033

[Epoch: 36, batch: 200/201] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3095e-04, 8.7076e-01, 1.2744e-01, 5.6885e-04, 3.9815e-04, 9.4942e-05,
        6.0778e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.989

[Epoch: 37, batch: 40/201] total loss per batch: 0.771
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1871, 0.0591, 0.0448, 0.5702, 0.0267, 0.0970, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 37, batch: 80/201] total loss per batch: 0.789
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1048e-03, 5.2598e-03, 9.7737e-01, 1.2764e-06, 9.9485e-03, 2.3742e-03,
        3.9369e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 37, batch: 120/201] total loss per batch: 0.785
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6459, 0.0503, 0.0056, 0.0671, 0.1613, 0.0675, 0.0022],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.028

[Epoch: 37, batch: 160/201] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0045, 0.9790, 0.0011, 0.0017, 0.0022, 0.0050, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.051

[Epoch: 37, batch: 200/201] total loss per batch: 0.752
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.6125e-04, 8.1841e-01, 1.7832e-01, 6.9317e-04, 7.5115e-04, 1.1541e-04,
        1.3488e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.973

[Epoch: 38, batch: 40/201] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0132, 0.0083, 0.0070, 0.9185, 0.0049, 0.0409, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 38, batch: 80/201] total loss per batch: 0.781
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.5951e-04, 2.5313e-02, 9.2029e-01, 3.0313e-06, 3.2489e-02, 1.0306e-02,
        1.0841e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 38, batch: 120/201] total loss per batch: 0.780
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5044, 0.0694, 0.0086, 0.0511, 0.1974, 0.1666, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 38, batch: 160/201] total loss per batch: 0.767
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0143, 0.9188, 0.0110, 0.0248, 0.0092, 0.0048, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 38, batch: 200/201] total loss per batch: 0.753
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3638e-04, 8.4083e-01, 1.5805e-01, 1.7411e-04, 3.3243e-04, 1.1172e-04,
        2.6587e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.943

[Epoch: 39, batch: 40/201] total loss per batch: 0.764
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0675, 0.0653, 0.0280, 0.6710, 0.0290, 0.1177, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 39, batch: 80/201] total loss per batch: 0.776
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.6168e-04, 1.5349e-02, 9.6667e-01, 1.2249e-06, 4.9503e-03, 7.6144e-03,
        4.4502e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.025

[Epoch: 39, batch: 120/201] total loss per batch: 0.781
Policy (actual, predicted): 0 5
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2874, 0.0587, 0.0073, 0.0651, 0.2692, 0.3076, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 39, batch: 160/201] total loss per batch: 0.766
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0110, 0.9266, 0.0069, 0.0046, 0.0127, 0.0157, 0.0225],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.039

[Epoch: 39, batch: 200/201] total loss per batch: 0.748
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9271e-04, 7.8560e-01, 2.0618e-01, 2.6837e-04, 1.0513e-03, 3.8029e-04,
        6.1304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.976

[Epoch: 40, batch: 40/201] total loss per batch: 0.763
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0808, 0.0441, 0.0345, 0.7004, 0.0147, 0.1182, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 40, batch: 80/201] total loss per batch: 0.780
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.0221e-04, 1.8847e-02, 9.5725e-01, 1.4612e-06, 1.3986e-02, 3.0197e-03,
        6.1902e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 40, batch: 120/201] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6206, 0.0629, 0.0078, 0.0687, 0.1241, 0.1125, 0.0033],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.049

[Epoch: 40, batch: 160/201] total loss per batch: 0.762
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0054, 0.9359, 0.0058, 0.0086, 0.0110, 0.0117, 0.0215],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.046

[Epoch: 40, batch: 200/201] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1601e-04, 8.3527e-01, 1.6389e-01, 6.1675e-05, 2.9014e-04, 5.6441e-05,
        2.1685e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.994

[Epoch: 41, batch: 40/201] total loss per batch: 0.760
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0630, 0.0358, 0.0102, 0.8138, 0.0232, 0.0397, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.007

[Epoch: 41, batch: 80/201] total loss per batch: 0.777
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.5768e-03, 4.6193e-02, 8.6761e-01, 1.9532e-06, 3.2196e-02, 1.8101e-02,
        3.2319e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 41, batch: 120/201] total loss per batch: 0.774
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4945, 0.0398, 0.0080, 0.1126, 0.2214, 0.1159, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 41, batch: 160/201] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0132, 0.8523, 0.0145, 0.0408, 0.0269, 0.0199, 0.0324],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 41, batch: 200/201] total loss per batch: 0.741
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7629e-04, 8.6559e-01, 1.3287e-01, 1.1623e-04, 7.3917e-04, 7.0959e-05,
        4.4121e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.978

[Epoch: 42, batch: 40/201] total loss per batch: 0.758
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0328, 0.0234, 0.0254, 0.8396, 0.0200, 0.0490, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 42, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.5240e-04, 1.4841e-02, 9.6658e-01, 8.0081e-07, 8.8038e-03, 4.9821e-03,
        4.1444e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 42, batch: 120/201] total loss per batch: 0.777
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4801, 0.1584, 0.0211, 0.0693, 0.1208, 0.1475, 0.0028],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 42, batch: 160/201] total loss per batch: 0.763
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0059, 0.9830, 0.0022, 0.0022, 0.0016, 0.0021, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.041

[Epoch: 42, batch: 200/201] total loss per batch: 0.738
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.2092e-04, 7.4106e-01, 2.5691e-01, 1.6670e-04, 3.5319e-04, 1.0187e-04,
        9.8360e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.955

[Epoch: 43, batch: 40/201] total loss per batch: 0.755
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1212, 0.0586, 0.0102, 0.7084, 0.0231, 0.0660, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 43, batch: 80/201] total loss per batch: 0.775
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.4938e-04, 9.8164e-03, 9.6395e-01, 1.4774e-06, 1.6969e-02, 4.3473e-03,
        4.1623e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 43, batch: 120/201] total loss per batch: 0.766
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3462, 0.0592, 0.0094, 0.1839, 0.1997, 0.1916, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.040

[Epoch: 43, batch: 160/201] total loss per batch: 0.754
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0158, 0.9255, 0.0171, 0.0098, 0.0080, 0.0082, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.011

[Epoch: 43, batch: 200/201] total loss per batch: 0.737
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0331e-04, 9.0762e-01, 9.1634e-02, 7.0684e-05, 1.5893e-04, 5.0363e-05,
        2.5864e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.981

[Epoch: 44, batch: 40/201] total loss per batch: 0.749
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0355, 0.0374, 0.0510, 0.7555, 0.0199, 0.0942, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.019

[Epoch: 44, batch: 80/201] total loss per batch: 0.767
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.6575e-03, 4.7465e-02, 8.6822e-01, 4.6136e-06, 3.8389e-02, 2.5572e-02,
        1.6696e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.041

[Epoch: 44, batch: 120/201] total loss per batch: 0.771
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3626, 0.0419, 0.0045, 0.0178, 0.4950, 0.0760, 0.0023],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.035

[Epoch: 44, batch: 160/201] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0177, 0.8969, 0.0016, 0.0322, 0.0136, 0.0158, 0.0222],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.050

[Epoch: 44, batch: 200/201] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.8863e-04, 6.5593e-01, 3.4260e-01, 1.5918e-04, 6.0929e-04, 4.2539e-05,
        4.6513e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 45, batch: 40/201] total loss per batch: 0.752
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0843, 0.0157, 0.0120, 0.7666, 0.0118, 0.0934, 0.0163],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 45, batch: 80/201] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.9831e-04, 6.6193e-03, 9.7643e-01, 5.9356e-07, 7.7940e-03, 4.6624e-03,
        3.8959e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 45, batch: 120/201] total loss per batch: 0.767
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.7092, 0.0330, 0.0048, 0.0685, 0.0691, 0.1102, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 45, batch: 160/201] total loss per batch: 0.751
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0062, 0.9696, 0.0036, 0.0048, 0.0025, 0.0042, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.015

[Epoch: 45, batch: 200/201] total loss per batch: 0.736
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7782e-04, 8.1776e-01, 1.8077e-01, 1.3761e-04, 3.5786e-04, 1.0152e-04,
        5.9833e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.966

[Epoch: 46, batch: 40/201] total loss per batch: 0.751
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1120, 0.0871, 0.0297, 0.6063, 0.0329, 0.1190, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 46, batch: 80/201] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.7892e-03, 5.9397e-02, 8.5725e-01, 6.6212e-06, 5.0555e-02, 1.6177e-02,
        1.4822e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 46, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5134, 0.0508, 0.0087, 0.0444, 0.2320, 0.1467, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.022

[Epoch: 46, batch: 160/201] total loss per batch: 0.749
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0358, 0.8932, 0.0088, 0.0278, 0.0119, 0.0123, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 46, batch: 200/201] total loss per batch: 0.730
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7949e-04, 8.5439e-01, 1.4439e-01, 7.9762e-05, 2.5901e-04, 8.4771e-05,
        2.1535e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.959

[Epoch: 47, batch: 40/201] total loss per batch: 0.745
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0254, 0.0085, 0.0152, 0.9102, 0.0048, 0.0323, 0.0034],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.000

[Epoch: 47, batch: 80/201] total loss per batch: 0.756
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2519e-03, 8.3077e-03, 9.6833e-01, 1.0242e-06, 1.0990e-02, 4.7393e-03,
        6.3772e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 47, batch: 120/201] total loss per batch: 0.759
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3577, 0.1424, 0.0096, 0.0336, 0.2225, 0.2257, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 47, batch: 160/201] total loss per batch: 0.746
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0102, 0.8886, 0.0152, 0.0249, 0.0128, 0.0109, 0.0373],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.049

[Epoch: 47, batch: 200/201] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7721e-04, 6.7883e-01, 3.2045e-01, 1.2677e-04, 1.2177e-04, 6.7513e-05,
        2.2024e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.959

[Epoch: 48, batch: 40/201] total loss per batch: 0.743
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1589, 0.0937, 0.0286, 0.5478, 0.0162, 0.1331, 0.0217],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 48, batch: 80/201] total loss per batch: 0.757
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.1810e-04, 3.4270e-02, 9.3862e-01, 4.6278e-06, 1.2561e-02, 8.3824e-03,
        5.3462e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.075

[Epoch: 48, batch: 120/201] total loss per batch: 0.757
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4957, 0.0188, 0.0046, 0.1074, 0.2923, 0.0736, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.031

[Epoch: 48, batch: 160/201] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0119, 0.9582, 0.0056, 0.0075, 0.0071, 0.0065, 0.0032],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.018

[Epoch: 48, batch: 200/201] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0824e-04, 8.7472e-01, 1.2443e-01, 4.8185e-05, 4.3365e-04, 2.2451e-05,
        2.2879e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.946

[Epoch: 49, batch: 40/201] total loss per batch: 0.747
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0097, 0.0061, 0.0057, 0.9365, 0.0066, 0.0324, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 49, batch: 80/201] total loss per batch: 0.760
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.7677e-03, 1.5118e-02, 9.3634e-01, 1.0288e-06, 2.5575e-02, 1.3253e-02,
        6.9456e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.016

[Epoch: 49, batch: 120/201] total loss per batch: 0.758
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4021, 0.1370, 0.0042, 0.0098, 0.2597, 0.1820, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 49, batch: 160/201] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0094, 0.9310, 0.0088, 0.0130, 0.0118, 0.0065, 0.0196],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.036

[Epoch: 49, batch: 200/201] total loss per batch: 0.729
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1615e-04, 7.2759e-01, 2.6681e-01, 1.8166e-04, 1.6919e-03, 2.0885e-04,
        3.0933e-03], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.976

[Epoch: 50, batch: 40/201] total loss per batch: 0.744
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0630, 0.0291, 0.0681, 0.7131, 0.0150, 0.1040, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.003

[Epoch: 50, batch: 80/201] total loss per batch: 0.769
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.7191e-04, 2.8086e-02, 9.0650e-01, 5.0122e-06, 3.9624e-02, 1.3033e-02,
        1.1976e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.032

[Epoch: 50, batch: 120/201] total loss per batch: 0.762
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6335, 0.0310, 0.0066, 0.1071, 0.1460, 0.0669, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 50, batch: 160/201] total loss per batch: 0.745
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0173, 0.9169, 0.0079, 0.0259, 0.0030, 0.0174, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.045

[Epoch: 50, batch: 200/201] total loss per batch: 0.727
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0378e-04, 7.9792e-01, 2.0078e-01, 1.2992e-04, 1.4450e-04, 8.5323e-05,
        3.4368e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.980

[Epoch: 51, batch: 40/201] total loss per batch: 0.737
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0939, 0.0536, 0.0368, 0.6754, 0.0158, 0.1110, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 51, batch: 80/201] total loss per batch: 0.753
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.8003e-03, 9.2066e-03, 9.7034e-01, 1.4882e-06, 7.8178e-03, 5.6787e-03,
        5.1584e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.067

[Epoch: 51, batch: 120/201] total loss per batch: 0.747
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5028, 0.0389, 0.0102, 0.0553, 0.2100, 0.1755, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 51, batch: 160/201] total loss per batch: 0.732
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0073, 0.9572, 0.0099, 0.0068, 0.0049, 0.0076, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 51, batch: 200/201] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0233e-04, 8.4351e-01, 1.5546e-01, 5.5865e-05, 3.0308e-04, 3.3048e-05,
        4.3269e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.976

[Epoch: 52, batch: 40/201] total loss per batch: 0.721
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0422, 0.0209, 0.0127, 0.8552, 0.0085, 0.0536, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 52, batch: 80/201] total loss per batch: 0.732
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3656e-03, 1.9189e-02, 9.2951e-01, 5.4972e-06, 2.7010e-02, 8.3314e-03,
        1.4592e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.046

[Epoch: 52, batch: 120/201] total loss per batch: 0.725
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4276, 0.0521, 0.0126, 0.0927, 0.2788, 0.1250, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.027

[Epoch: 52, batch: 160/201] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0198, 0.9163, 0.0086, 0.0183, 0.0077, 0.0133, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.037

[Epoch: 52, batch: 200/201] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.7156e-04, 7.8749e-01, 2.1148e-01, 5.0617e-05, 1.2589e-04, 1.4111e-04,
        3.4098e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 53, batch: 40/201] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1280, 0.0363, 0.0436, 0.4950, 0.0201, 0.2696, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 53, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.3667e-03, 3.2312e-02, 8.9474e-01, 4.3638e-06, 3.7039e-02, 1.6182e-02,
        1.6358e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.026

[Epoch: 53, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5224, 0.0421, 0.0116, 0.0641, 0.2137, 0.1373, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.022

[Epoch: 53, batch: 160/201] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0109, 0.9388, 0.0115, 0.0188, 0.0039, 0.0067, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.038

[Epoch: 53, batch: 200/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1890e-04, 7.9859e-01, 2.0076e-01, 6.3731e-05, 1.1838e-04, 6.5996e-05,
        1.8293e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.953

[Epoch: 54, batch: 40/201] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0394, 0.0173, 0.0097, 0.8998, 0.0084, 0.0215, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.004

[Epoch: 54, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.5515e-03, 1.9081e-02, 9.3320e-01, 3.0556e-06, 2.1973e-02, 8.2702e-03,
        1.4920e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.043

[Epoch: 54, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3516, 0.0910, 0.0068, 0.0274, 0.3072, 0.2065, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 54, batch: 160/201] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0119, 0.9442, 0.0080, 0.0115, 0.0047, 0.0079, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.039

[Epoch: 54, batch: 200/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9616e-04, 7.8388e-01, 2.1524e-01, 4.7742e-05, 2.2369e-04, 4.3069e-05,
        2.6720e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 55, batch: 40/201] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1113, 0.0333, 0.0345, 0.6372, 0.0207, 0.1509, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 55, batch: 80/201] total loss per batch: 0.728
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.5760e-03, 3.4027e-02, 9.1279e-01, 3.7915e-06, 2.1738e-02, 1.6895e-02,
        1.0966e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 55, batch: 120/201] total loss per batch: 0.724
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6605, 0.0135, 0.0095, 0.0781, 0.1299, 0.1035, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.038

[Epoch: 55, batch: 160/201] total loss per batch: 0.716
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0205, 0.9202, 0.0066, 0.0283, 0.0043, 0.0109, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.042

[Epoch: 55, batch: 200/201] total loss per batch: 0.699
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.1507e-04, 8.4283e-01, 1.5660e-01, 6.5659e-05, 1.2015e-04, 5.9053e-05,
        1.0911e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.907

[Epoch: 56, batch: 40/201] total loss per batch: 0.719
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0649, 0.0202, 0.0148, 0.8564, 0.0162, 0.0250, 0.0025],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.001

[Epoch: 56, batch: 80/201] total loss per batch: 0.735
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.0354e-03, 2.4743e-02, 8.4777e-01, 2.8892e-06, 8.0848e-02, 1.1673e-02,
        3.0923e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 56, batch: 120/201] total loss per batch: 0.730
Policy (actual, predicted): 0 5
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2865, 0.1155, 0.0070, 0.0185, 0.2627, 0.3012, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 56, batch: 160/201] total loss per batch: 0.721
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0095, 0.9338, 0.0110, 0.0116, 0.0059, 0.0082, 0.0199],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 56, batch: 200/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.3709e-04, 8.1672e-01, 1.8238e-01, 4.0724e-05, 2.5868e-04, 3.5830e-05,
        3.3014e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 57, batch: 40/201] total loss per batch: 0.723
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1210, 0.0296, 0.0155, 0.6653, 0.0150, 0.1360, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 57, batch: 80/201] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1929e-03, 1.2534e-02, 9.6757e-01, 1.5686e-06, 5.9653e-03, 7.2285e-03,
        5.5103e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 57, batch: 120/201] total loss per batch: 0.731
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6077, 0.0152, 0.0082, 0.0798, 0.2117, 0.0674, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.033

[Epoch: 57, batch: 160/201] total loss per batch: 0.722
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0143, 0.9575, 0.0029, 0.0112, 0.0045, 0.0070, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.041

[Epoch: 57, batch: 200/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9781e-04, 7.5413e-01, 2.4508e-01, 1.1610e-04, 1.0705e-04, 9.9645e-05,
        1.7083e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.953

[Epoch: 58, batch: 40/201] total loss per batch: 0.723
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0284, 0.0163, 0.0091, 0.8862, 0.0263, 0.0310, 0.0027],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.002

[Epoch: 58, batch: 80/201] total loss per batch: 0.737
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.9076e-03, 5.2761e-02, 8.2500e-01, 5.4717e-06, 6.9967e-02, 2.2068e-02,
        2.4288e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.024

[Epoch: 58, batch: 120/201] total loss per batch: 0.729
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4089, 0.0451, 0.0135, 0.0520, 0.2571, 0.2165, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 58, batch: 160/201] total loss per batch: 0.721
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0163, 0.9061, 0.0127, 0.0182, 0.0081, 0.0101, 0.0285],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.038

[Epoch: 58, batch: 200/201] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9540e-04, 8.5208e-01, 1.4676e-01, 7.6758e-05, 4.5462e-04, 5.9754e-05,
        3.7542e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 59, batch: 40/201] total loss per batch: 0.721
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1713, 0.0786, 0.0215, 0.5143, 0.0146, 0.1656, 0.0341],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 59, batch: 80/201] total loss per batch: 0.733
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.1506e-04, 5.6226e-03, 9.8083e-01, 2.5833e-06, 4.4706e-03, 3.9233e-03,
        4.3379e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.065

[Epoch: 59, batch: 120/201] total loss per batch: 0.728
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5350, 0.0418, 0.0126, 0.0695, 0.2493, 0.0821, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 59, batch: 160/201] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0137, 0.9444, 0.0111, 0.0122, 0.0055, 0.0065, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.049

[Epoch: 59, batch: 200/201] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2039e-04, 7.8785e-01, 2.1134e-01, 1.1284e-04, 1.9135e-04, 5.2580e-05,
        3.2669e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.988

[Epoch: 60, batch: 40/201] total loss per batch: 0.719
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0179, 0.0073, 0.0142, 0.9140, 0.0184, 0.0252, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.004

[Epoch: 60, batch: 80/201] total loss per batch: 0.731
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.3026e-03, 1.0649e-02, 9.6216e-01, 1.2547e-06, 1.3634e-02, 6.7608e-03,
        4.4884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.037

[Epoch: 60, batch: 120/201] total loss per batch: 0.730
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5901, 0.0291, 0.0075, 0.0579, 0.1837, 0.1214, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 60, batch: 160/201] total loss per batch: 0.722
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0151, 0.9091, 0.0110, 0.0319, 0.0065, 0.0096, 0.0168],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.036

[Epoch: 60, batch: 200/201] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9730e-04, 7.8223e-01, 2.1718e-01, 3.4487e-05, 1.2642e-04, 3.1328e-05,
        1.9065e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.953

[Epoch: 61, batch: 40/201] total loss per batch: 0.720
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1707, 0.0805, 0.0367, 0.5515, 0.0242, 0.1208, 0.0156],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 61, batch: 80/201] total loss per batch: 0.731
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.8428e-03, 1.2611e-02, 9.6524e-01, 3.8880e-06, 6.6978e-03, 4.4296e-03,
        8.1790e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.012

[Epoch: 61, batch: 120/201] total loss per batch: 0.731
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3829, 0.0621, 0.0072, 0.0420, 0.3520, 0.1486, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 61, batch: 160/201] total loss per batch: 0.719
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0244, 0.9291, 0.0097, 0.0108, 0.0056, 0.0053, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.045

[Epoch: 61, batch: 200/201] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3663e-04, 7.3800e-01, 2.6091e-01, 9.5986e-05, 3.8218e-04, 4.6533e-05,
        4.3120e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.976

[Epoch: 62, batch: 40/201] total loss per batch: 0.719
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0408, 0.0144, 0.0126, 0.8693, 0.0153, 0.0423, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 62, batch: 80/201] total loss per batch: 0.730
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.9744e-03, 1.3587e-02, 9.6164e-01, 7.9096e-07, 8.9847e-03, 8.8980e-03,
        4.9148e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.039

[Epoch: 62, batch: 120/201] total loss per batch: 0.729
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.7304, 0.0260, 0.0062, 0.0387, 0.0768, 0.1125, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.019

[Epoch: 62, batch: 160/201] total loss per batch: 0.717
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0143, 0.9268, 0.0084, 0.0187, 0.0080, 0.0159, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.026

[Epoch: 62, batch: 200/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9228e-04, 7.4432e-01, 2.5460e-01, 1.0859e-04, 3.1098e-04, 1.0132e-04,
        3.6394e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.960

[Epoch: 63, batch: 40/201] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1527, 0.0475, 0.0205, 0.6826, 0.0232, 0.0657, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.001

[Epoch: 63, batch: 80/201] total loss per batch: 0.734
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.2044e-03, 3.8540e-02, 9.1097e-01, 3.2890e-06, 2.1509e-02, 1.5089e-02,
        8.6855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 63, batch: 120/201] total loss per batch: 0.731
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2592, 0.0616, 0.0091, 0.0423, 0.3330, 0.2881, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.029

[Epoch: 63, batch: 160/201] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0176, 0.9206, 0.0105, 0.0274, 0.0060, 0.0053, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.046

[Epoch: 63, batch: 200/201] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4661e-04, 9.2310e-01, 7.6579e-02, 2.2943e-05, 7.7318e-05, 9.2531e-06,
        6.0190e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.960

[Epoch: 64, batch: 40/201] total loss per batch: 0.718
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0364, 0.0068, 0.0090, 0.8520, 0.0090, 0.0830, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 64, batch: 80/201] total loss per batch: 0.735
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.0446e-03, 2.0648e-02, 9.2510e-01, 3.8274e-06, 2.1270e-02, 1.2011e-02,
        1.3924e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.030

[Epoch: 64, batch: 120/201] total loss per batch: 0.729
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6132, 0.0205, 0.0073, 0.0613, 0.2050, 0.0858, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 64, batch: 160/201] total loss per batch: 0.717
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0147, 0.9573, 0.0044, 0.0069, 0.0030, 0.0075, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.034

[Epoch: 64, batch: 200/201] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9032e-04, 7.0426e-01, 2.9351e-01, 3.1776e-04, 5.5701e-04, 1.8092e-04,
        6.8388e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 65, batch: 40/201] total loss per batch: 0.717
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1256, 0.0374, 0.0237, 0.6653, 0.0266, 0.1146, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 65, batch: 80/201] total loss per batch: 0.730
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.6143e-03, 1.9876e-02, 9.3697e-01, 2.3014e-06, 2.1697e-02, 1.2245e-02,
        6.5918e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.019

[Epoch: 65, batch: 120/201] total loss per batch: 0.727
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4162, 0.0484, 0.0109, 0.0472, 0.2070, 0.2594, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 65, batch: 160/201] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0186, 0.9315, 0.0082, 0.0147, 0.0063, 0.0093, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 65, batch: 200/201] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4420e-04, 8.1471e-01, 1.8488e-01, 1.9570e-05, 1.0725e-04, 1.5823e-05,
        1.1987e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.950

[Epoch: 66, batch: 40/201] total loss per batch: 0.715
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1050, 0.0097, 0.0087, 0.8173, 0.0057, 0.0506, 0.0030],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 66, batch: 80/201] total loss per batch: 0.726
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.0003e-03, 4.4517e-02, 8.5564e-01, 3.0836e-06, 4.9892e-02, 1.2090e-02,
        3.3858e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 66, batch: 120/201] total loss per batch: 0.722
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4291, 0.0291, 0.0087, 0.0698, 0.3557, 0.1011, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.029

[Epoch: 66, batch: 160/201] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0101, 0.9394, 0.0130, 0.0106, 0.0079, 0.0080, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.030

[Epoch: 66, batch: 200/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5422e-04, 8.4064e-01, 1.5842e-01, 2.6915e-04, 2.3942e-04, 1.0331e-04,
        1.7444e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.963

[Epoch: 67, batch: 40/201] total loss per batch: 0.714
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0770, 0.0170, 0.0136, 0.7659, 0.0142, 0.1060, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 67, batch: 80/201] total loss per batch: 0.726
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3257e-03, 1.2836e-02, 9.6682e-01, 5.8709e-07, 9.5904e-03, 6.7370e-03,
        2.6925e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.035

[Epoch: 67, batch: 120/201] total loss per batch: 0.722
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5183, 0.0277, 0.0100, 0.0646, 0.1881, 0.1801, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.036

[Epoch: 67, batch: 160/201] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0236, 0.9338, 0.0077, 0.0119, 0.0035, 0.0112, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.043

[Epoch: 67, batch: 200/201] total loss per batch: 0.699
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9613e-04, 6.7329e-01, 3.2522e-01, 6.2583e-05, 4.7298e-04, 8.4616e-05,
        5.7676e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.979

[Epoch: 68, batch: 40/201] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1263, 0.0185, 0.0142, 0.6923, 0.0121, 0.1275, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 68, batch: 80/201] total loss per batch: 0.734
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.8146e-03, 2.6065e-02, 9.0102e-01, 2.7671e-06, 3.6543e-02, 1.3653e-02,
        1.7901e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 68, batch: 120/201] total loss per batch: 0.727
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4925, 0.0419, 0.0060, 0.0325, 0.2551, 0.1643, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 68, batch: 160/201] total loss per batch: 0.721
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0125, 0.9502, 0.0074, 0.0083, 0.0081, 0.0071, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.033

[Epoch: 68, batch: 200/201] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2333e-04, 8.7279e-01, 1.2584e-01, 2.1359e-04, 1.6339e-04, 1.5507e-04,
        5.1041e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.932

[Epoch: 69, batch: 40/201] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0648, 0.0168, 0.0081, 0.8449, 0.0198, 0.0409, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 69, batch: 80/201] total loss per batch: 0.742
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.9264e-03, 2.5351e-02, 9.2342e-01, 3.0339e-06, 2.7413e-02, 7.2729e-03,
        9.6095e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.016

[Epoch: 69, batch: 120/201] total loss per batch: 0.733
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6388, 0.0231, 0.0054, 0.0296, 0.1433, 0.1533, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.027

[Epoch: 69, batch: 160/201] total loss per batch: 0.718
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0133, 0.9163, 0.0089, 0.0190, 0.0066, 0.0104, 0.0255],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.049

[Epoch: 69, batch: 200/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3858e-05, 8.1576e-01, 1.8288e-01, 1.0893e-04, 3.6305e-04, 8.7341e-05,
        7.2348e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.930

[Epoch: 70, batch: 40/201] total loss per batch: 0.716
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0564, 0.0132, 0.0064, 0.7988, 0.0141, 0.1064, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.001

[Epoch: 70, batch: 80/201] total loss per batch: 0.731
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.9870e-03, 1.6131e-02, 9.5922e-01, 4.5942e-06, 8.9189e-03, 8.0428e-03,
        4.6964e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.026

[Epoch: 70, batch: 120/201] total loss per batch: 0.727
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.2706, 0.0205, 0.0041, 0.0304, 0.5049, 0.1635, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.012

[Epoch: 70, batch: 160/201] total loss per batch: 0.715
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0249, 0.9302, 0.0056, 0.0186, 0.0047, 0.0071, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 70, batch: 200/201] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1998e-04, 7.7922e-01, 2.2041e-01, 2.5183e-05, 7.5401e-05, 3.5529e-05,
        1.1567e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.889

[Epoch: 71, batch: 40/201] total loss per batch: 0.712
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1287, 0.0525, 0.0093, 0.7040, 0.0213, 0.0761, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 71, batch: 80/201] total loss per batch: 0.728
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.6449e-03, 1.8105e-02, 9.4169e-01, 3.3396e-06, 1.4513e-02, 7.0868e-03,
        1.2954e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 71, batch: 120/201] total loss per batch: 0.725
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6483, 0.0352, 0.0073, 0.0338, 0.1378, 0.1217, 0.0160],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.045

[Epoch: 71, batch: 160/201] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0147, 0.9172, 0.0131, 0.0137, 0.0044, 0.0116, 0.0253],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.041

[Epoch: 71, batch: 200/201] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0426e-05, 8.3073e-01, 1.6854e-01, 5.3618e-05, 2.3742e-04, 5.1848e-05,
        3.0845e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.946

[Epoch: 72, batch: 40/201] total loss per batch: 0.713
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1586, 0.0195, 0.0121, 0.6909, 0.0270, 0.0835, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 72, batch: 80/201] total loss per batch: 0.725
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.1194e-03, 2.4901e-02, 9.3323e-01, 5.9033e-06, 2.1473e-02, 8.3437e-03,
        7.9242e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.005

[Epoch: 72, batch: 120/201] total loss per batch: 0.724
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3822, 0.0335, 0.0139, 0.0752, 0.2287, 0.2582, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.015

[Epoch: 72, batch: 160/201] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0118, 0.9467, 0.0090, 0.0159, 0.0045, 0.0070, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 72, batch: 200/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6539e-04, 7.9758e-01, 2.0124e-01, 6.2105e-05, 1.1145e-04, 8.5046e-05,
        6.5736e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.950

[Epoch: 73, batch: 40/201] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0395, 0.0150, 0.0064, 0.8367, 0.0113, 0.0849, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 73, batch: 80/201] total loss per batch: 0.725
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.9734e-03, 1.4295e-02, 9.6181e-01, 3.1280e-06, 3.4535e-03, 9.7217e-03,
        4.7425e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.004

[Epoch: 73, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6373, 0.0234, 0.0100, 0.0758, 0.1505, 0.0865, 0.0164],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 73, batch: 160/201] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0245, 0.9080, 0.0148, 0.0153, 0.0156, 0.0071, 0.0149],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.032

[Epoch: 73, batch: 200/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5671e-04, 8.2809e-01, 1.7144e-01, 2.9195e-05, 8.5800e-05, 5.3146e-05,
        1.5387e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.947

[Epoch: 74, batch: 40/201] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1549, 0.0387, 0.0148, 0.6633, 0.0292, 0.0891, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 74, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.9036e-03, 3.3537e-02, 8.9989e-01, 2.0138e-05, 3.3999e-02, 1.2418e-02,
        1.2233e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 74, batch: 120/201] total loss per batch: 0.721
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4042, 0.0413, 0.0062, 0.0260, 0.2934, 0.2220, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 74, batch: 160/201] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0092, 0.9306, 0.0104, 0.0210, 0.0077, 0.0097, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 74, batch: 200/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5054e-04, 8.0635e-01, 1.9298e-01, 6.8662e-05, 6.2991e-05, 4.8038e-05,
        3.3897e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 75, batch: 40/201] total loss per batch: 0.710
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0473, 0.0097, 0.0046, 0.8743, 0.0146, 0.0440, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 75, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.8471e-03, 1.4762e-02, 9.5130e-01, 3.7536e-06, 8.8146e-03, 1.1978e-02,
        8.2899e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 75, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6502, 0.0093, 0.0075, 0.0672, 0.1655, 0.0921, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.004

[Epoch: 75, batch: 160/201] total loss per batch: 0.713
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0197, 0.9091, 0.0322, 0.0102, 0.0059, 0.0102, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 75, batch: 200/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.7155e-04, 7.8265e-01, 2.1695e-01, 1.7035e-05, 8.1511e-05, 3.7772e-05,
        9.4251e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.945

[Epoch: 76, batch: 40/201] total loss per batch: 0.711
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0803, 0.0314, 0.0064, 0.6955, 0.0144, 0.1652, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 76, batch: 80/201] total loss per batch: 0.725
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.6705e-03, 2.2892e-02, 9.3074e-01, 9.9307e-06, 1.7527e-02, 1.3211e-02,
        6.9487e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.047

[Epoch: 76, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3420, 0.0227, 0.0058, 0.0302, 0.2873, 0.3028, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 76, batch: 160/201] total loss per batch: 0.711
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0079, 0.9468, 0.0030, 0.0140, 0.0081, 0.0060, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.047

[Epoch: 76, batch: 200/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8434e-05, 7.9687e-01, 2.0261e-01, 5.8500e-05, 8.3439e-05, 6.0801e-05,
        2.3723e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.948

[Epoch: 77, batch: 40/201] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1217, 0.0191, 0.0172, 0.7208, 0.0256, 0.0869, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.003

[Epoch: 77, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.9002e-03, 1.7516e-02, 9.2742e-01, 4.7251e-06, 2.2299e-02, 1.5320e-02,
        1.0541e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 77, batch: 120/201] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5189, 0.0210, 0.0064, 0.0586, 0.3143, 0.0733, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 77, batch: 160/201] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0133, 0.9271, 0.0112, 0.0163, 0.0117, 0.0105, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.034

[Epoch: 77, batch: 200/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.9994e-05, 8.3162e-01, 1.6799e-01, 1.0852e-05, 1.2005e-04, 2.0898e-05,
        1.3716e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 78, batch: 40/201] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0827, 0.0154, 0.0076, 0.7662, 0.0107, 0.1114, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 78, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.2095e-03, 3.7985e-02, 9.0519e-01, 7.9006e-06, 1.8210e-02, 1.7753e-02,
        1.3645e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.024

[Epoch: 78, batch: 120/201] total loss per batch: 0.716
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4442, 0.0178, 0.0169, 0.0576, 0.2233, 0.2280, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.034

[Epoch: 78, batch: 160/201] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0126, 0.9496, 0.0028, 0.0083, 0.0065, 0.0087, 0.0116],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.025

[Epoch: 78, batch: 200/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.6732e-04, 6.9104e-01, 3.0814e-01, 1.9100e-04, 9.2971e-05, 6.7838e-05,
        1.9400e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 79, batch: 40/201] total loss per batch: 0.710
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0984, 0.0161, 0.0093, 0.7924, 0.0138, 0.0648, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 79, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.7444e-03, 7.9635e-03, 9.6774e-01, 4.6205e-06, 9.1775e-03, 6.2123e-03,
        5.1577e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.015

[Epoch: 79, batch: 120/201] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5677, 0.0282, 0.0043, 0.0620, 0.2068, 0.1246, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 79, batch: 160/201] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0130, 0.9099, 0.0090, 0.0337, 0.0084, 0.0074, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.062

[Epoch: 79, batch: 200/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.5392e-05, 8.9418e-01, 1.0540e-01, 2.3776e-05, 8.4806e-05, 5.3339e-05,
        1.6495e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 80, batch: 40/201] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0935, 0.0296, 0.0191, 0.6913, 0.0233, 0.1320, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.002

[Epoch: 80, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0167e-02, 4.2329e-02, 8.8241e-01, 4.2592e-06, 3.5625e-02, 1.6008e-02,
        1.3457e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 80, batch: 120/201] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4469, 0.0204, 0.0128, 0.0261, 0.2152, 0.2637, 0.0150],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.035

[Epoch: 80, batch: 160/201] total loss per batch: 0.712
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0108, 0.9627, 0.0060, 0.0063, 0.0055, 0.0050, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.027

[Epoch: 80, batch: 200/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.0566e-04, 7.6022e-01, 2.3908e-01, 4.6361e-05, 1.5794e-04, 4.8407e-05,
        2.4976e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.944

[Epoch: 81, batch: 40/201] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0950, 0.0363, 0.0080, 0.7420, 0.0171, 0.0911, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 81, batch: 80/201] total loss per batch: 0.722
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.5935e-03, 9.4789e-03, 9.6223e-01, 4.7923e-06, 1.0206e-02, 6.9877e-03,
        7.5025e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.036

[Epoch: 81, batch: 120/201] total loss per batch: 0.719
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5685, 0.0093, 0.0030, 0.0521, 0.2917, 0.0690, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 81, batch: 160/201] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0096, 0.9095, 0.0120, 0.0273, 0.0163, 0.0081, 0.0173],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.055

[Epoch: 81, batch: 200/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0689e-04, 7.3035e-01, 2.6902e-01, 5.7310e-05, 1.5851e-04, 5.3134e-05,
        2.5175e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.950

[Epoch: 82, batch: 40/201] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1071, 0.0224, 0.0159, 0.7599, 0.0151, 0.0727, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.009

[Epoch: 82, batch: 80/201] total loss per batch: 0.719
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0060e-02, 3.1789e-02, 9.2396e-01, 1.1271e-05, 1.6011e-02, 8.2919e-03,
        9.8718e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.004

[Epoch: 82, batch: 120/201] total loss per batch: 0.718
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3803, 0.0389, 0.0117, 0.0524, 0.3037, 0.2013, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 82, batch: 160/201] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0117, 0.9464, 0.0053, 0.0173, 0.0047, 0.0083, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.028

[Epoch: 82, batch: 200/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.5372e-05, 8.4096e-01, 1.5864e-01, 4.1630e-05, 5.5481e-05, 3.5285e-05,
        1.8471e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.913

[Epoch: 83, batch: 40/201] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1180, 0.0273, 0.0122, 0.7172, 0.0242, 0.0911, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 83, batch: 80/201] total loss per batch: 0.721
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.7209e-03, 1.2656e-02, 9.5854e-01, 3.3920e-06, 8.8913e-03, 1.1783e-02,
        4.4088e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.024

[Epoch: 83, batch: 120/201] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6088, 0.0150, 0.0065, 0.0713, 0.2016, 0.0864, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 83, batch: 160/201] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0107, 0.9477, 0.0104, 0.0128, 0.0059, 0.0061, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.044

[Epoch: 83, batch: 200/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1546e-04, 8.1617e-01, 1.8319e-01, 3.1404e-05, 2.6994e-04, 2.7244e-05,
        2.0103e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.965

[Epoch: 84, batch: 40/201] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0372, 0.0152, 0.0040, 0.8616, 0.0109, 0.0661, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 84, batch: 80/201] total loss per batch: 0.721
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3492e-02, 4.8316e-02, 8.7790e-01, 6.7034e-06, 2.2624e-02, 1.6025e-02,
        2.1639e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.003

[Epoch: 84, batch: 120/201] total loss per batch: 0.715
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3442, 0.0202, 0.0092, 0.0305, 0.3523, 0.2374, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.013

[Epoch: 84, batch: 160/201] total loss per batch: 0.710
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0153, 0.9198, 0.0061, 0.0201, 0.0116, 0.0096, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.040

[Epoch: 84, batch: 200/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.9861e-04, 8.4531e-01, 1.5395e-01, 2.8869e-05, 1.3819e-04, 7.3323e-05,
        3.0932e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.933

[Epoch: 85, batch: 40/201] total loss per batch: 0.707
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1611, 0.0206, 0.0298, 0.6310, 0.0180, 0.1303, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 85, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.3189e-03, 1.4698e-02, 9.1844e-01, 8.5381e-06, 2.8578e-02, 1.9550e-02,
        1.0402e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.006

[Epoch: 85, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5446, 0.0255, 0.0084, 0.0747, 0.1786, 0.1564, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 85, batch: 160/201] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0058, 0.9597, 0.0124, 0.0087, 0.0035, 0.0049, 0.0050],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.034

[Epoch: 85, batch: 200/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1822e-05, 7.7406e-01, 2.2531e-01, 4.1535e-05, 2.9651e-04, 3.0987e-05,
        2.0231e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.960

[Epoch: 86, batch: 40/201] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0627, 0.0272, 0.0141, 0.7368, 0.0233, 0.1223, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.000

[Epoch: 86, batch: 80/201] total loss per batch: 0.724
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2424e-02, 2.9163e-02, 9.0944e-01, 1.3176e-05, 2.0498e-02, 1.4462e-02,
        1.4002e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.033

[Epoch: 86, batch: 120/201] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5405, 0.0201, 0.0052, 0.0354, 0.2476, 0.1367, 0.0146],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.016

[Epoch: 86, batch: 160/201] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0129, 0.9362, 0.0062, 0.0142, 0.0063, 0.0076, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.046

[Epoch: 86, batch: 200/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.7841e-04, 8.1871e-01, 1.8051e-01, 7.6426e-05, 1.4269e-04, 1.0542e-04,
        1.8243e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.954

[Epoch: 87, batch: 40/201] total loss per batch: 0.709
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0933, 0.0080, 0.0063, 0.8365, 0.0082, 0.0426, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.008

[Epoch: 87, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.8404e-03, 1.3747e-02, 9.4494e-01, 7.0846e-06, 1.8536e-02, 7.5283e-03,
        8.3979e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 87, batch: 120/201] total loss per batch: 0.717
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5499, 0.0189, 0.0053, 0.0402, 0.2601, 0.1191, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 87, batch: 160/201] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0109, 0.9160, 0.0079, 0.0340, 0.0075, 0.0111, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.035

[Epoch: 87, batch: 200/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6039e-04, 8.3651e-01, 1.6290e-01, 6.3667e-05, 1.5573e-04, 3.6532e-05,
        1.7812e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.932

[Epoch: 88, batch: 40/201] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1354, 0.0516, 0.0168, 0.6402, 0.0224, 0.1210, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 88, batch: 80/201] total loss per batch: 0.725
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.3203e-03, 2.3579e-02, 9.3713e-01, 1.4339e-05, 9.9775e-03, 1.6712e-02,
        6.2661e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 88, batch: 120/201] total loss per batch: 0.716
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4523, 0.0174, 0.0072, 0.0672, 0.2130, 0.2344, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 88, batch: 160/201] total loss per batch: 0.708
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0180, 0.9353, 0.0070, 0.0160, 0.0072, 0.0100, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.039

[Epoch: 88, batch: 200/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0142e-04, 7.8460e-01, 2.1490e-01, 8.2816e-05, 1.0045e-04, 2.7077e-05,
        1.8775e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 89, batch: 40/201] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1396, 0.0173, 0.0098, 0.7385, 0.0178, 0.0682, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 89, batch: 80/201] total loss per batch: 0.723
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0667e-02, 1.5688e-02, 9.3369e-01, 1.4826e-05, 1.9547e-02, 6.6697e-03,
        1.3719e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 89, batch: 120/201] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5031, 0.0252, 0.0063, 0.0360, 0.2881, 0.1353, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.010

[Epoch: 89, batch: 160/201] total loss per batch: 0.709
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0082, 0.8938, 0.0062, 0.0394, 0.0074, 0.0093, 0.0358],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.045

[Epoch: 89, batch: 200/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.9555e-04, 7.1688e-01, 2.8218e-01, 1.4041e-04, 1.9641e-04, 1.2172e-04,
        1.9110e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.945

[Epoch: 90, batch: 40/201] total loss per batch: 0.708
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0473, 0.0131, 0.0034, 0.8683, 0.0075, 0.0550, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 90, batch: 80/201] total loss per batch: 0.722
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.1644e-03, 1.4946e-02, 9.4823e-01, 1.2653e-05, 1.2083e-02, 8.1798e-03,
        8.3882e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 90, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4834, 0.0189, 0.0073, 0.0457, 0.2620, 0.1750, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 90, batch: 160/201] total loss per batch: 0.707
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0149, 0.9472, 0.0050, 0.0104, 0.0063, 0.0075, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 90, batch: 200/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.3558e-04, 8.9829e-01, 1.0120e-01, 2.2208e-05, 1.1945e-04, 1.1184e-04,
        1.2767e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.939

[Epoch: 91, batch: 40/201] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1822, 0.0368, 0.0105, 0.6673, 0.0175, 0.0762, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 91, batch: 80/201] total loss per batch: 0.721
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0509e-02, 1.2607e-02, 9.1507e-01, 1.1894e-05, 3.3132e-02, 1.7660e-02,
        1.1012e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.015

[Epoch: 91, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4213, 0.0254, 0.0063, 0.0342, 0.3209, 0.1840, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.013

[Epoch: 91, batch: 160/201] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0072, 0.9569, 0.0073, 0.0074, 0.0053, 0.0066, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.041

[Epoch: 91, batch: 200/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.4612e-04, 7.6114e-01, 2.3773e-01, 2.5511e-04, 1.1981e-04, 1.4788e-04,
        3.6412e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.954

[Epoch: 92, batch: 40/201] total loss per batch: 0.704
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0628, 0.0105, 0.0107, 0.8141, 0.0116, 0.0832, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 92, batch: 80/201] total loss per batch: 0.720
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.1673e-03, 2.5067e-02, 9.3239e-01, 7.6010e-06, 1.1485e-02, 1.1342e-02,
        1.1543e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.021

[Epoch: 92, batch: 120/201] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5886, 0.0333, 0.0052, 0.0262, 0.1823, 0.1553, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 92, batch: 160/201] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0105, 0.9578, 0.0035, 0.0091, 0.0042, 0.0062, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.026

[Epoch: 92, batch: 200/201] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([2.2760e-04, 7.9171e-01, 2.0755e-01, 5.9002e-05, 1.6697e-04, 7.1983e-05,
        2.0850e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.933

[Epoch: 93, batch: 40/201] total loss per batch: 0.704
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0919, 0.0206, 0.0079, 0.7623, 0.0189, 0.0901, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.005

[Epoch: 93, batch: 80/201] total loss per batch: 0.719
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.2870e-03, 1.9832e-02, 9.3221e-01, 8.2793e-06, 2.1868e-02, 1.1340e-02,
        8.4541e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.015

[Epoch: 93, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5283, 0.0105, 0.0070, 0.0645, 0.2569, 0.1262, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 93, batch: 160/201] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0132, 0.9432, 0.0087, 0.0167, 0.0048, 0.0065, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.042

[Epoch: 93, batch: 200/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6081e-05, 7.8161e-01, 2.1800e-01, 5.2735e-05, 5.1999e-05, 6.6851e-05,
        1.3499e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.982

[Epoch: 94, batch: 40/201] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1098, 0.0308, 0.0164, 0.6967, 0.0177, 0.1225, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 94, batch: 80/201] total loss per batch: 0.718
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0966e-02, 1.6649e-02, 9.3922e-01, 5.0765e-06, 1.3705e-02, 1.2429e-02,
        7.0302e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.018

[Epoch: 94, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4778, 0.0330, 0.0059, 0.0325, 0.2063, 0.2370, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 94, batch: 160/201] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0081, 0.9431, 0.0066, 0.0100, 0.0053, 0.0081, 0.0187],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.027

[Epoch: 94, batch: 200/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2729e-04, 8.0842e-01, 1.9109e-01, 7.4933e-05, 1.3034e-04, 2.9053e-05,
        1.2169e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.954

[Epoch: 95, batch: 40/201] total loss per batch: 0.705
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1263, 0.0197, 0.0072, 0.7660, 0.0154, 0.0554, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 95, batch: 80/201] total loss per batch: 0.718
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.6916e-03, 2.6064e-02, 8.9759e-01, 7.4498e-06, 3.6682e-02, 1.7093e-02,
        1.4868e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.020

[Epoch: 95, batch: 120/201] total loss per batch: 0.714
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5602, 0.0145, 0.0066, 0.0442, 0.2236, 0.1421, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 95, batch: 160/201] total loss per batch: 0.706
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0116, 0.9438, 0.0109, 0.0127, 0.0088, 0.0050, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.021

[Epoch: 95, batch: 200/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4018e-04, 7.3557e-01, 2.6387e-01, 7.5099e-05, 1.1513e-04, 3.8874e-05,
        1.8979e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.942

[Epoch: 96, batch: 40/201] total loss per batch: 0.706
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0406, 0.0091, 0.0060, 0.8431, 0.0173, 0.0799, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 96, batch: 80/201] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.2021e-03, 2.4963e-02, 9.3301e-01, 1.3212e-05, 1.4144e-02, 9.8835e-03,
        9.7855e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 96, batch: 120/201] total loss per batch: 0.713
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4379, 0.0155, 0.0072, 0.0437, 0.3393, 0.1517, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 96, batch: 160/201] total loss per batch: 0.703
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0130, 0.8728, 0.0170, 0.0419, 0.0112, 0.0123, 0.0319],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.052

[Epoch: 96, batch: 200/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2313e-04, 8.5610e-01, 1.4332e-01, 1.0041e-04, 1.2796e-04, 7.1912e-05,
        1.5705e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.945

[Epoch: 97, batch: 40/201] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1739, 0.0278, 0.0110, 0.6152, 0.0171, 0.1432, 0.0117],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 97, batch: 80/201] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([3.0642e-03, 5.8521e-03, 9.6827e-01, 3.8566e-06, 1.1909e-02, 6.4872e-03,
        4.4135e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.004

[Epoch: 97, batch: 120/201] total loss per batch: 0.713
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5827, 0.0240, 0.0077, 0.0308, 0.1526, 0.1964, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 97, batch: 160/201] total loss per batch: 0.702
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0163, 0.9483, 0.0051, 0.0062, 0.0051, 0.0089, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.022

[Epoch: 97, batch: 200/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1208e-04, 7.3768e-01, 2.6192e-01, 3.0255e-05, 7.0055e-05, 4.3234e-05,
        1.5027e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 98, batch: 40/201] total loss per batch: 0.703
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0628, 0.0111, 0.0046, 0.8536, 0.0142, 0.0497, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 98, batch: 80/201] total loss per batch: 0.717
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3160e-02, 4.0233e-02, 8.5736e-01, 1.2239e-05, 3.1304e-02, 3.4290e-02,
        2.3642e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 98, batch: 120/201] total loss per batch: 0.711
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4488, 0.0115, 0.0055, 0.0373, 0.3016, 0.1894, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 98, batch: 160/201] total loss per batch: 0.704
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0089, 0.9335, 0.0086, 0.0179, 0.0089, 0.0055, 0.0166],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.044

[Epoch: 98, batch: 200/201] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5267e-04, 8.1608e-01, 1.8308e-01, 2.6496e-04, 1.1379e-04, 1.0820e-04,
        2.0256e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.947

[Epoch: 99, batch: 40/201] total loss per batch: 0.701
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0804, 0.0199, 0.0078, 0.7395, 0.0150, 0.1252, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 99, batch: 80/201] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.2167e-03, 6.6190e-03, 9.7512e-01, 4.1070e-06, 6.0361e-03, 3.0587e-03,
        4.9436e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 99, batch: 120/201] total loss per batch: 0.712
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5870, 0.0182, 0.0089, 0.0950, 0.1606, 0.1224, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 99, batch: 160/201] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0073, 0.9622, 0.0054, 0.0082, 0.0042, 0.0043, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.029

[Epoch: 99, batch: 200/201] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.9288e-05, 8.1221e-01, 1.8705e-01, 1.6081e-04, 1.6952e-04, 2.8946e-05,
        2.8779e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.979

[Epoch: 100, batch: 40/201] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0786, 0.0129, 0.0084, 0.8348, 0.0090, 0.0510, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 100, batch: 80/201] total loss per batch: 0.716
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.1466e-03, 2.7948e-02, 9.1783e-01, 6.7988e-06, 1.6445e-02, 1.8575e-02,
        1.0051e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 100, batch: 120/201] total loss per batch: 0.715
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4321, 0.0128, 0.0064, 0.0236, 0.3520, 0.1670, 0.0060],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.008

[Epoch: 100, batch: 160/201] total loss per batch: 0.705
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0104, 0.9507, 0.0097, 0.0095, 0.0044, 0.0059, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.031

[Epoch: 100, batch: 200/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1963e-05, 8.2375e-01, 1.7591e-01, 5.2868e-05, 3.7508e-05, 5.0151e-05,
        1.3954e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 101, batch: 40/201] total loss per batch: 0.702
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1256, 0.0267, 0.0124, 0.6870, 0.0267, 0.1111, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 101, batch: 80/201] total loss per batch: 0.715
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3393e-02, 1.5204e-02, 9.1748e-01, 1.0927e-05, 2.8972e-02, 7.6467e-03,
        1.7293e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 101, batch: 120/201] total loss per batch: 0.711
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4763, 0.0244, 0.0092, 0.0556, 0.1799, 0.2488, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 101, batch: 160/201] total loss per batch: 0.701
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0136, 0.9252, 0.0073, 0.0193, 0.0098, 0.0072, 0.0176],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.024

[Epoch: 101, batch: 200/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6844e-04, 7.2757e-01, 2.7161e-01, 1.0196e-04, 1.8785e-04, 8.7719e-05,
        2.7289e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.965

[Epoch: 102, batch: 40/201] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0848, 0.0215, 0.0117, 0.7375, 0.0155, 0.1211, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 102, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.6282e-03, 1.1739e-02, 9.5615e-01, 6.6125e-06, 8.8235e-03, 1.0439e-02,
        6.2100e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 102, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5662, 0.0075, 0.0056, 0.0408, 0.2365, 0.1358, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 102, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0099, 0.9578, 0.0055, 0.0112, 0.0038, 0.0049, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.022

[Epoch: 102, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7783e-05, 8.3437e-01, 1.6519e-01, 4.9625e-05, 1.0770e-04, 3.5527e-05,
        1.5663e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 103, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1185, 0.0192, 0.0132, 0.7328, 0.0201, 0.0878, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 103, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.7411e-03, 1.5525e-02, 9.2493e-01, 9.7188e-06, 2.7094e-02, 1.0547e-02,
        1.3151e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.012

[Epoch: 103, batch: 120/201] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4352, 0.0169, 0.0067, 0.0327, 0.2635, 0.2378, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.007

[Epoch: 103, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0116, 0.9456, 0.0072, 0.0144, 0.0060, 0.0052, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.016

[Epoch: 103, batch: 200/201] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0152e-04, 7.8879e-01, 2.1080e-01, 8.0214e-05, 6.9152e-05, 5.6633e-05,
        1.0499e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.966

[Epoch: 104, batch: 40/201] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0783, 0.0195, 0.0093, 0.7862, 0.0157, 0.0844, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.001

[Epoch: 104, batch: 80/201] total loss per batch: 0.702
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.0365e-03, 1.6494e-02, 9.4207e-01, 7.0414e-06, 1.0351e-02, 1.3522e-02,
        8.5169e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 104, batch: 120/201] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5743, 0.0143, 0.0060, 0.0329, 0.2371, 0.1262, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 104, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0084, 0.9605, 0.0058, 0.0088, 0.0044, 0.0042, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.022

[Epoch: 104, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4366e-05, 7.9107e-01, 2.0853e-01, 6.7996e-05, 9.0637e-05, 3.4182e-05,
        1.4428e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.954

[Epoch: 105, batch: 40/201] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1435, 0.0234, 0.0123, 0.6640, 0.0217, 0.1243, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 105, batch: 80/201] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.7073e-03, 1.3800e-02, 9.3342e-01, 8.1363e-06, 2.0605e-02, 1.2476e-02,
        1.0981e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 105, batch: 120/201] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5135, 0.0118, 0.0064, 0.0414, 0.2590, 0.1614, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 105, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0097, 0.9560, 0.0044, 0.0116, 0.0040, 0.0052, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.026

[Epoch: 105, batch: 200/201] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7889e-05, 8.0354e-01, 1.9613e-01, 5.5150e-05, 7.3075e-05, 3.3529e-05,
        9.5219e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 106, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0613, 0.0128, 0.0086, 0.8471, 0.0094, 0.0543, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.007

[Epoch: 106, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3016e-02, 1.3033e-02, 9.3167e-01, 6.6376e-06, 1.6602e-02, 1.4292e-02,
        1.1384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.013

[Epoch: 106, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5457, 0.0133, 0.0046, 0.0286, 0.2182, 0.1815, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 106, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0120, 0.9355, 0.0078, 0.0199, 0.0073, 0.0054, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.022

[Epoch: 106, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1425e-05, 8.0951e-01, 1.9006e-01, 4.9907e-05, 1.1699e-04, 3.5459e-05,
        1.6628e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.940

[Epoch: 107, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1288, 0.0205, 0.0070, 0.7041, 0.0173, 0.1147, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 107, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.5285e-03, 1.4437e-02, 9.3695e-01, 4.5469e-06, 1.5575e-02, 1.5664e-02,
        9.8443e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.008

[Epoch: 107, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3939, 0.0199, 0.0094, 0.0569, 0.3057, 0.2043, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.002

[Epoch: 107, batch: 160/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0118, 0.9455, 0.0073, 0.0124, 0.0036, 0.0074, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.026

[Epoch: 107, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9757e-05, 8.0575e-01, 1.9402e-01, 2.9100e-05, 3.5042e-05, 2.5395e-05,
        9.2898e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 108, batch: 40/201] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0848, 0.0186, 0.0110, 0.7917, 0.0137, 0.0734, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 108, batch: 80/201] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1566e-02, 1.6433e-02, 9.1731e-01, 8.1299e-06, 2.9445e-02, 1.1110e-02,
        1.4128e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.008

[Epoch: 108, batch: 120/201] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5717, 0.0100, 0.0042, 0.0297, 0.2445, 0.1351, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 108, batch: 160/201] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0107, 0.9406, 0.0044, 0.0162, 0.0087, 0.0073, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.020

[Epoch: 108, batch: 200/201] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5683e-05, 7.9105e-01, 2.0857e-01, 7.8822e-05, 6.4892e-05, 2.5582e-05,
        1.5180e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.955

[Epoch: 109, batch: 40/201] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1012, 0.0233, 0.0069, 0.7589, 0.0162, 0.0857, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 109, batch: 80/201] total loss per batch: 0.710
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.6818e-03, 1.3134e-02, 9.6238e-01, 3.7970e-06, 6.1393e-03, 8.3700e-03,
        5.2884e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.005

[Epoch: 109, batch: 120/201] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5961, 0.0219, 0.0053, 0.0276, 0.1798, 0.1637, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 109, batch: 160/201] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0110, 0.9401, 0.0074, 0.0166, 0.0049, 0.0076, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.026

[Epoch: 109, batch: 200/201] total loss per batch: 0.682
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7816e-05, 8.3374e-01, 1.6599e-01, 4.4892e-05, 2.6958e-05, 4.8070e-05,
        8.9405e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.982

[Epoch: 110, batch: 40/201] total loss per batch: 0.697
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1217, 0.0175, 0.0083, 0.6926, 0.0266, 0.1265, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 110, batch: 80/201] total loss per batch: 0.711
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2899e-02, 1.8647e-02, 9.1034e-01, 1.4600e-05, 2.7566e-02, 1.5391e-02,
        1.5142e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.019

[Epoch: 110, batch: 120/201] total loss per batch: 0.705
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3636, 0.0154, 0.0050, 0.0530, 0.3299, 0.2248, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 110, batch: 160/201] total loss per batch: 0.698
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0149, 0.9505, 0.0060, 0.0087, 0.0039, 0.0077, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.021

[Epoch: 110, batch: 200/201] total loss per batch: 0.681
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8072e-05, 8.1190e-01, 1.8780e-01, 4.3437e-05, 7.6930e-05, 2.9442e-05,
        9.5651e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.953

[Epoch: 111, batch: 40/201] total loss per batch: 0.696
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0801, 0.0138, 0.0057, 0.8187, 0.0097, 0.0650, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 111, batch: 80/201] total loss per batch: 0.711
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.1363e-03, 1.0431e-02, 9.5464e-01, 5.5203e-06, 8.7195e-03, 1.1828e-02,
        7.2440e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 111, batch: 120/201] total loss per batch: 0.704
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6391, 0.0115, 0.0045, 0.0284, 0.1849, 0.1271, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 111, batch: 160/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0093, 0.9199, 0.0071, 0.0186, 0.0074, 0.0076, 0.0301],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.033

[Epoch: 111, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.1477e-04, 7.3241e-01, 2.6718e-01, 5.4480e-05, 7.4667e-05, 2.2218e-05,
        1.4372e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.979

[Epoch: 112, batch: 40/201] total loss per batch: 0.695
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1180, 0.0153, 0.0072, 0.6813, 0.0236, 0.1456, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 112, batch: 80/201] total loss per batch: 0.709
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1983e-02, 2.3993e-02, 9.2213e-01, 1.4152e-05, 1.9364e-02, 1.0877e-02,
        1.1638e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.006

[Epoch: 112, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3800, 0.0179, 0.0047, 0.0319, 0.3161, 0.2437, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 112, batch: 160/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0114, 0.9548, 0.0048, 0.0098, 0.0038, 0.0068, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.015

[Epoch: 112, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.9095e-05, 8.6708e-01, 1.3267e-01, 4.8329e-05, 3.3102e-05, 2.5490e-05,
        1.0285e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 113, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1069, 0.0207, 0.0061, 0.7693, 0.0125, 0.0768, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 113, batch: 80/201] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.8061e-03, 2.0613e-02, 9.2817e-01, 7.9704e-06, 1.5015e-02, 1.6208e-02,
        1.0177e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 113, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6230, 0.0085, 0.0053, 0.0364, 0.2176, 0.1034, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 113, batch: 160/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0082, 0.9304, 0.0079, 0.0202, 0.0064, 0.0055, 0.0214],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.019

[Epoch: 113, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.6713e-04, 7.3735e-01, 2.6216e-01, 8.5608e-05, 8.1283e-05, 4.0700e-05,
        1.1342e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.950

[Epoch: 114, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1201, 0.0137, 0.0097, 0.6996, 0.0172, 0.1282, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 114, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.4585e-03, 1.9379e-02, 9.4749e-01, 1.0174e-05, 7.9618e-03, 9.2092e-03,
        7.4917e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 114, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4813, 0.0092, 0.0029, 0.0226, 0.2783, 0.2019, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 114, batch: 160/201] total loss per batch: 0.696
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0117, 0.9420, 0.0106, 0.0140, 0.0043, 0.0064, 0.0111],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.022

[Epoch: 114, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8476e-05, 8.1237e-01, 1.8717e-01, 5.6927e-05, 4.6585e-05, 3.9568e-05,
        2.3137e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 115, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0807, 0.0161, 0.0065, 0.8176, 0.0156, 0.0580, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 115, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1912e-02, 1.6598e-02, 9.1274e-01, 1.2388e-05, 2.4555e-02, 1.8827e-02,
        1.5358e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 115, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5242, 0.0200, 0.0074, 0.0376, 0.2109, 0.1893, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 115, batch: 160/201] total loss per batch: 0.697
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0157, 0.9339, 0.0047, 0.0194, 0.0043, 0.0065, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.018

[Epoch: 115, batch: 200/201] total loss per batch: 0.680
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2917e-04, 7.7591e-01, 2.2377e-01, 6.5231e-05, 4.5404e-05, 3.2559e-05,
        5.3947e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.940

[Epoch: 116, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1050, 0.0227, 0.0085, 0.7402, 0.0143, 0.0978, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 116, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2786e-02, 1.6390e-02, 9.3936e-01, 8.4271e-06, 7.8661e-03, 1.3240e-02,
        1.0348e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 116, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5411, 0.0112, 0.0057, 0.0343, 0.2134, 0.1873, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.022

[Epoch: 116, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0082, 0.9447, 0.0093, 0.0128, 0.0076, 0.0058, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.012

[Epoch: 116, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6948e-05, 8.1134e-01, 1.8830e-01, 6.4309e-05, 3.9396e-05, 2.7422e-05,
        1.4464e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.943

[Epoch: 117, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1138, 0.0198, 0.0110, 0.7370, 0.0261, 0.0833, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 117, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.1922e-03, 1.2072e-02, 9.4838e-01, 1.3306e-05, 1.4090e-02, 1.0323e-02,
        7.9281e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.002

[Epoch: 117, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4954, 0.0105, 0.0067, 0.0307, 0.2892, 0.1600, 0.0075],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 117, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0100, 0.9464, 0.0058, 0.0116, 0.0045, 0.0075, 0.0143],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.029

[Epoch: 117, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.5925e-04, 7.8435e-01, 2.1534e-01, 3.1408e-05, 2.9985e-05, 2.9740e-05,
        6.4874e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.965

[Epoch: 118, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0768, 0.0194, 0.0089, 0.7791, 0.0137, 0.0882, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 118, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1047e-02, 2.1601e-02, 9.3091e-01, 1.0728e-05, 1.0099e-02, 1.6554e-02,
        9.7726e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.029

[Epoch: 118, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4687, 0.0129, 0.0092, 0.0372, 0.2634, 0.2000, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 118, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0102, 0.9376, 0.0079, 0.0189, 0.0058, 0.0054, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.017

[Epoch: 118, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.2426e-05, 7.7786e-01, 2.2179e-01, 5.1093e-05, 4.3796e-05, 2.3676e-05,
        1.5144e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.930

[Epoch: 119, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1109, 0.0198, 0.0109, 0.7451, 0.0194, 0.0849, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 119, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1988e-02, 1.6253e-02, 9.3225e-01, 1.2818e-05, 1.8691e-02, 1.2451e-02,
        8.3548e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 119, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5917, 0.0105, 0.0039, 0.0283, 0.1860, 0.1741, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 119, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0097, 0.9512, 0.0063, 0.0114, 0.0053, 0.0062, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.019

[Epoch: 119, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1180e-05, 8.1391e-01, 1.8585e-01, 5.6079e-05, 2.9627e-05, 2.5427e-05,
        5.6419e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 120, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1353, 0.0302, 0.0064, 0.6986, 0.0120, 0.1037, 0.0138],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 120, batch: 80/201] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.6527e-02, 2.7953e-02, 8.9211e-01, 1.0586e-05, 2.5650e-02, 2.1516e-02,
        1.6229e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.012

[Epoch: 120, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4955, 0.0114, 0.0078, 0.0491, 0.2892, 0.1396, 0.0074],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 120, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0099, 0.9250, 0.0098, 0.0222, 0.0071, 0.0075, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.009

[Epoch: 120, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3115e-05, 7.8956e-01, 2.1016e-01, 3.6699e-05, 5.1129e-05, 1.4878e-05,
        1.3552e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.957

[Epoch: 121, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0340, 0.0159, 0.0089, 0.8045, 0.0192, 0.1069, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 121, batch: 80/201] total loss per batch: 0.708
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2459e-02, 9.9865e-03, 9.4810e-01, 1.6874e-05, 1.0113e-02, 1.0593e-02,
        8.7299e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.007

[Epoch: 121, batch: 120/201] total loss per batch: 0.703
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4914, 0.0132, 0.0049, 0.0365, 0.2013, 0.2464, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 121, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0090, 0.9513, 0.0065, 0.0147, 0.0063, 0.0058, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.014

[Epoch: 121, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4069e-05, 7.9348e-01, 2.0606e-01, 8.0332e-05, 1.0218e-04, 3.5802e-05,
        1.6028e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.957

[Epoch: 122, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1549, 0.0294, 0.0063, 0.6985, 0.0101, 0.0924, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 122, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.3798e-03, 1.1349e-02, 9.5665e-01, 1.3895e-05, 5.4978e-03, 1.1464e-02,
        5.6426e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 122, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5214, 0.0122, 0.0058, 0.0415, 0.2879, 0.1263, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.008

[Epoch: 122, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0093, 0.9220, 0.0091, 0.0206, 0.0090, 0.0082, 0.0219],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.013

[Epoch: 122, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.7014e-05, 7.5718e-01, 2.4241e-01, 3.5622e-05, 1.0333e-04, 2.6128e-05,
        1.6132e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.948

[Epoch: 123, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0731, 0.0339, 0.0088, 0.7949, 0.0105, 0.0683, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 123, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.6150e-02, 1.8384e-02, 9.1524e-01, 8.9749e-06, 2.2767e-02, 1.4734e-02,
        1.2717e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.003

[Epoch: 123, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4524, 0.0108, 0.0041, 0.0275, 0.3033, 0.1940, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 123, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0089, 0.9514, 0.0068, 0.0133, 0.0042, 0.0076, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.008

[Epoch: 123, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2879e-05, 7.9902e-01, 2.0029e-01, 7.0304e-05, 1.3483e-04, 4.1242e-05,
        3.7922e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.921

[Epoch: 124, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1172, 0.0214, 0.0073, 0.6961, 0.0128, 0.1365, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 124, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.4548e-03, 8.4536e-03, 9.6437e-01, 4.0013e-06, 6.4270e-03, 9.7274e-03,
        5.5590e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 124, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5720, 0.0142, 0.0069, 0.0418, 0.1906, 0.1688, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 124, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0093, 0.9479, 0.0071, 0.0173, 0.0046, 0.0043, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.027

[Epoch: 124, batch: 200/201] total loss per batch: 0.679
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.9730e-05, 8.0145e-01, 1.9802e-01, 9.0549e-05, 1.6908e-04, 6.6589e-05,
        1.1434e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.966

[Epoch: 125, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0828, 0.0146, 0.0046, 0.8245, 0.0075, 0.0569, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 125, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.0354e-02, 2.4030e-02, 9.0539e-01, 1.1462e-05, 1.8082e-02, 1.8807e-02,
        1.3325e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 125, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5177, 0.0128, 0.0046, 0.0431, 0.2268, 0.1856, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.024

[Epoch: 125, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0101, 0.9488, 0.0057, 0.0139, 0.0043, 0.0070, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.009

[Epoch: 125, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6231e-05, 8.0444e-01, 1.9516e-01, 3.1437e-05, 3.4090e-05, 2.0183e-05,
        2.3317e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.924

[Epoch: 126, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1330, 0.0293, 0.0087, 0.6680, 0.0130, 0.1403, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 126, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([4.7065e-03, 1.0766e-02, 9.5866e-01, 2.8306e-06, 7.2117e-03, 9.0637e-03,
        9.5890e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 126, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5150, 0.0105, 0.0080, 0.0346, 0.2549, 0.1701, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 126, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0076, 0.9430, 0.0072, 0.0146, 0.0075, 0.0068, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.016

[Epoch: 126, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.8044e-05, 8.0052e-01, 1.9913e-01, 5.6379e-05, 8.9651e-05, 2.7406e-05,
        9.6146e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.977

[Epoch: 127, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0965, 0.0095, 0.0050, 0.8246, 0.0075, 0.0485, 0.0084],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.001

[Epoch: 127, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0257e-02, 1.6689e-02, 9.3946e-01, 9.6621e-06, 1.5740e-02, 8.2932e-03,
        9.5493e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 127, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4787, 0.0119, 0.0037, 0.0331, 0.3051, 0.1612, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.018

[Epoch: 127, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0116, 0.9334, 0.0112, 0.0220, 0.0036, 0.0072, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.006

[Epoch: 127, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7614e-05, 7.7100e-01, 2.2870e-01, 4.7543e-05, 5.1964e-05, 3.8702e-05,
        9.6624e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 128, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1056, 0.0248, 0.0071, 0.7575, 0.0112, 0.0839, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 128, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2762e-02, 1.7380e-02, 9.2444e-01, 6.1551e-06, 1.5954e-02, 1.8097e-02,
        1.1360e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.019

[Epoch: 128, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5357, 0.0170, 0.0062, 0.0344, 0.1982, 0.2003, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 128, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0070, 0.9524, 0.0067, 0.0128, 0.0058, 0.0052, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.010

[Epoch: 128, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8082e-05, 8.4154e-01, 1.5823e-01, 2.9794e-05, 4.7181e-05, 1.1736e-05,
        8.7421e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 129, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1421, 0.0177, 0.0101, 0.6989, 0.0164, 0.1050, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.003

[Epoch: 129, batch: 80/201] total loss per batch: 0.707
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.0484e-03, 1.0572e-02, 9.4888e-01, 5.3034e-06, 1.6256e-02, 7.9854e-03,
        9.2532e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 129, batch: 120/201] total loss per batch: 0.702
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5213, 0.0115, 0.0026, 0.0186, 0.2689, 0.1740, 0.0031],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 129, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0080, 0.9476, 0.0074, 0.0131, 0.0042, 0.0065, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 129, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4576e-05, 7.5484e-01, 2.4467e-01, 6.0553e-05, 9.9649e-05, 2.4952e-05,
        2.5027e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 130, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0699, 0.0180, 0.0091, 0.8094, 0.0107, 0.0727, 0.0102],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.006

[Epoch: 130, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.6913e-02, 2.4897e-02, 9.0792e-01, 1.8548e-05, 1.8120e-02, 1.8869e-02,
        1.3260e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 130, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5299, 0.0092, 0.0062, 0.0598, 0.1969, 0.1907, 0.0073],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 130, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0086, 0.9465, 0.0101, 0.0152, 0.0040, 0.0054, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.013

[Epoch: 130, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0321e-04, 7.8869e-01, 2.1084e-01, 9.4352e-05, 1.0160e-04, 7.6212e-05,
        9.8450e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.955

[Epoch: 131, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1009, 0.0116, 0.0061, 0.7707, 0.0112, 0.0868, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 131, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.1256e-03, 1.4726e-02, 9.4746e-01, 4.7249e-06, 1.3077e-02, 9.8293e-03,
        8.7730e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 131, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4875, 0.0116, 0.0064, 0.0303, 0.2954, 0.1616, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.004

[Epoch: 131, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0076, 0.9503, 0.0060, 0.0138, 0.0061, 0.0063, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.001

[Epoch: 131, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1319e-05, 8.1697e-01, 1.8272e-01, 2.8301e-05, 6.8143e-05, 1.7932e-05,
        1.5924e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.961

[Epoch: 132, batch: 40/201] total loss per batch: 0.694
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1064, 0.0193, 0.0084, 0.7747, 0.0124, 0.0683, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 132, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5063e-02, 1.5705e-02, 9.3511e-01, 1.1963e-05, 9.6621e-03, 1.4087e-02,
        1.0359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 132, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5526, 0.0077, 0.0032, 0.0338, 0.2372, 0.1601, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 132, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0087, 0.9406, 0.0064, 0.0181, 0.0050, 0.0049, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.010

[Epoch: 132, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4961e-05, 8.0565e-01, 1.9391e-01, 5.2170e-05, 4.2216e-05, 3.4876e-05,
        2.1670e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.886

[Epoch: 133, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1432, 0.0259, 0.0078, 0.6679, 0.0135, 0.1255, 0.0162],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.003

[Epoch: 133, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3783e-02, 2.0574e-02, 8.7914e-01, 6.7293e-06, 3.9555e-02, 2.5907e-02,
        2.1036e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.002

[Epoch: 133, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4625, 0.0137, 0.0084, 0.0477, 0.2791, 0.1807, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 133, batch: 160/201] total loss per batch: 0.695
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0074, 0.9474, 0.0078, 0.0189, 0.0042, 0.0063, 0.0079],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.005

[Epoch: 133, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2995e-05, 7.9337e-01, 2.0638e-01, 2.3497e-05, 5.0357e-05, 2.9358e-05,
        7.9834e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.989

[Epoch: 134, batch: 40/201] total loss per batch: 0.693
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1057, 0.0197, 0.0051, 0.7761, 0.0143, 0.0670, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 134, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.9688e-03, 7.5141e-03, 9.6767e-01, 9.5962e-06, 5.9689e-03, 6.7230e-03,
        5.1433e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.007

[Epoch: 134, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4821, 0.0097, 0.0042, 0.0285, 0.2492, 0.2210, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 134, batch: 160/201] total loss per batch: 0.694
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0085, 0.9416, 0.0065, 0.0209, 0.0054, 0.0070, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.012

[Epoch: 134, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.9617e-05, 8.1923e-01, 1.8049e-01, 1.8058e-05, 2.6344e-05, 2.3216e-05,
        1.0738e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 135, batch: 40/201] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0764, 0.0134, 0.0051, 0.7945, 0.0117, 0.0889, 0.0100],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.003

[Epoch: 135, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1315e-02, 1.4644e-02, 9.2914e-01, 7.3873e-06, 1.3848e-02, 1.6430e-02,
        1.4613e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 135, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6086, 0.0083, 0.0028, 0.0259, 0.2232, 0.1269, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 135, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0105, 0.9486, 0.0080, 0.0118, 0.0061, 0.0063, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 135, batch: 200/201] total loss per batch: 0.678
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8278e-05, 7.9787e-01, 2.0191e-01, 2.9464e-05, 6.0180e-05, 1.3111e-05,
        6.5045e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.965

[Epoch: 136, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1730, 0.0450, 0.0083, 0.6204, 0.0245, 0.1087, 0.0202],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 136, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.5918e-03, 1.4964e-02, 9.3546e-01, 4.3349e-06, 1.2751e-02, 1.7072e-02,
        1.0157e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 136, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3886, 0.0101, 0.0047, 0.0273, 0.2545, 0.3077, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 136, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0060, 0.9431, 0.0048, 0.0208, 0.0049, 0.0082, 0.0122],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.008

[Epoch: 136, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.4025e-04, 7.9065e-01, 2.0897e-01, 3.4636e-05, 6.3320e-05, 3.6076e-05,
        1.0909e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 137, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0457, 0.0068, 0.0040, 0.8490, 0.0067, 0.0790, 0.0087],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 137, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0252e-02, 1.9056e-02, 9.3252e-01, 5.8602e-06, 1.6079e-02, 1.0623e-02,
        1.1469e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 137, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6533, 0.0072, 0.0029, 0.0284, 0.2110, 0.0932, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.010

[Epoch: 137, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0084, 0.9377, 0.0100, 0.0224, 0.0075, 0.0062, 0.0077],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 137, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7732e-05, 7.9932e-01, 2.0041e-01, 4.7942e-05, 4.8493e-05, 1.6177e-05,
        7.9435e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 138, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1022, 0.0148, 0.0041, 0.7654, 0.0097, 0.0908, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.024

[Epoch: 138, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([2.0695e-02, 1.3534e-02, 8.9993e-01, 8.2564e-06, 2.5404e-02, 2.1233e-02,
        1.9196e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.010

[Epoch: 138, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 4
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3400, 0.0148, 0.0048, 0.0335, 0.3574, 0.2399, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 138, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0069, 0.9667, 0.0028, 0.0083, 0.0029, 0.0056, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.002

[Epoch: 138, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.3728e-05, 7.9746e-01, 2.0228e-01, 4.9996e-05, 4.0873e-05, 2.5183e-05,
        6.7334e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.946

[Epoch: 139, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1412, 0.0344, 0.0074, 0.6600, 0.0133, 0.1302, 0.0136],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 139, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.0431e-03, 7.5784e-03, 9.6785e-01, 4.1690e-06, 7.6378e-03, 6.1966e-03,
        5.6887e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.025

[Epoch: 139, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6472, 0.0081, 0.0038, 0.0423, 0.1694, 0.1246, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 139, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0098, 0.9360, 0.0100, 0.0231, 0.0054, 0.0059, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 139, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6580e-05, 7.9917e-01, 2.0052e-01, 3.7232e-05, 6.1379e-05, 2.0932e-05,
        1.1043e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.976

[Epoch: 140, batch: 40/201] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0761, 0.0093, 0.0051, 0.8310, 0.0061, 0.0638, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 140, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4962e-02, 1.7234e-02, 9.1811e-01, 1.7239e-05, 2.3317e-02, 1.2082e-02,
        1.4276e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 140, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3918, 0.0098, 0.0058, 0.0427, 0.3537, 0.1898, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 140, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0064, 0.9428, 0.0059, 0.0223, 0.0062, 0.0072, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.005

[Epoch: 140, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0633e-05, 8.4755e-01, 1.5222e-01, 4.4604e-05, 6.1808e-05, 1.8368e-05,
        4.8843e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 141, batch: 40/201] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1269, 0.0288, 0.0068, 0.7235, 0.0144, 0.0844, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 141, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0856e-02, 9.9731e-03, 9.4070e-01, 7.1396e-06, 1.3859e-02, 1.4524e-02,
        1.0085e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 141, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5961, 0.0112, 0.0057, 0.0244, 0.1662, 0.1911, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 141, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0128, 0.9474, 0.0063, 0.0164, 0.0039, 0.0063, 0.0069],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.004

[Epoch: 141, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.7996e-05, 7.4346e-01, 2.5626e-01, 4.1042e-05, 3.2123e-05, 3.0912e-05,
        1.0867e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.960

[Epoch: 142, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1070, 0.0164, 0.0067, 0.7636, 0.0075, 0.0912, 0.0076],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 142, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1402e-02, 1.7115e-02, 9.3714e-01, 7.1334e-06, 1.1817e-02, 1.3920e-02,
        8.5950e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 142, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5015, 0.0077, 0.0052, 0.0653, 0.2409, 0.1713, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 142, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0047, 0.9483, 0.0072, 0.0143, 0.0046, 0.0054, 0.0155],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 142, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2705e-04, 8.0879e-01, 1.9075e-01, 3.5660e-05, 1.1993e-04, 1.6990e-05,
        1.5671e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.971

[Epoch: 143, batch: 40/201] total loss per batch: 0.692
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0999, 0.0209, 0.0055, 0.7486, 0.0106, 0.0975, 0.0171],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 143, batch: 80/201] total loss per batch: 0.706
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2627e-02, 2.5817e-02, 8.8470e-01, 1.6870e-05, 2.8898e-02, 2.4743e-02,
        2.3195e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.009

[Epoch: 143, batch: 120/201] total loss per batch: 0.701
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4842, 0.0127, 0.0040, 0.0239, 0.2898, 0.1810, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.005

[Epoch: 143, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0089, 0.9451, 0.0063, 0.0197, 0.0047, 0.0050, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.002

[Epoch: 143, batch: 200/201] total loss per batch: 0.677
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7777e-05, 7.8209e-01, 2.1764e-01, 3.5517e-05, 3.0619e-05, 2.9094e-05,
        1.0195e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.954

[Epoch: 144, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0884, 0.0185, 0.0047, 0.7976, 0.0081, 0.0734, 0.0093],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.004

[Epoch: 144, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.1223e-03, 6.6358e-03, 9.6168e-01, 5.7515e-06, 8.3833e-03, 9.7389e-03,
        5.4355e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.019

[Epoch: 144, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4880, 0.0075, 0.0058, 0.0578, 0.2499, 0.1838, 0.0072],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.003

[Epoch: 144, batch: 160/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0068, 0.9440, 0.0080, 0.0205, 0.0052, 0.0061, 0.0094],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.010

[Epoch: 144, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.0574e-05, 8.1970e-01, 1.7995e-01, 6.3039e-05, 7.7802e-05, 2.3592e-05,
        1.0355e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.963

[Epoch: 145, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1417, 0.0318, 0.0078, 0.6650, 0.0186, 0.1199, 0.0152],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 145, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0492e-02, 2.4100e-02, 9.0650e-01, 1.4642e-05, 1.8618e-02, 2.0557e-02,
        1.9722e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 145, batch: 120/201] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5170, 0.0121, 0.0041, 0.0314, 0.2805, 0.1495, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 145, batch: 160/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0072, 0.9394, 0.0104, 0.0191, 0.0036, 0.0058, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.002

[Epoch: 145, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.4778e-05, 7.9780e-01, 2.0189e-01, 2.5244e-05, 3.5460e-05, 1.1804e-05,
        1.5351e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 146, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0608, 0.0127, 0.0051, 0.8260, 0.0102, 0.0739, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 146, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2227e-02, 1.1776e-02, 9.4644e-01, 1.3596e-05, 9.2970e-03, 1.0602e-02,
        9.6430e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 146, batch: 120/201] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5175, 0.0074, 0.0075, 0.0545, 0.1969, 0.2109, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 146, batch: 160/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0091, 0.9513, 0.0048, 0.0152, 0.0041, 0.0046, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 146, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.0415e-04, 7.8665e-01, 2.1303e-01, 4.7645e-05, 5.2328e-05, 2.0593e-05,
        1.0167e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.971

[Epoch: 147, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1370, 0.0214, 0.0055, 0.7070, 0.0147, 0.1041, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 147, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([8.8327e-03, 1.7439e-02, 9.3932e-01, 2.2394e-05, 1.1377e-02, 1.3447e-02,
        9.5641e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 147, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5195, 0.0115, 0.0042, 0.0403, 0.2635, 0.1550, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 147, batch: 160/201] total loss per batch: 0.692
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0072, 0.9555, 0.0076, 0.0143, 0.0023, 0.0061, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.007

[Epoch: 147, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.2557e-05, 7.8301e-01, 2.1671e-01, 3.3971e-05, 4.1688e-05, 9.6527e-06,
        1.1984e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 148, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1022, 0.0203, 0.0070, 0.7731, 0.0073, 0.0804, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 148, batch: 80/201] total loss per batch: 0.705
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3088e-02, 1.1412e-02, 9.2613e-01, 8.0119e-06, 2.3610e-02, 1.0456e-02,
        1.5295e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 148, batch: 120/201] total loss per batch: 0.700
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4971, 0.0087, 0.0062, 0.0474, 0.2455, 0.1890, 0.0062],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.012

[Epoch: 148, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0142, 0.9279, 0.0082, 0.0219, 0.0066, 0.0082, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 148, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4750e-05, 8.3380e-01, 1.6591e-01, 5.1988e-05, 5.1845e-05, 1.9953e-05,
        9.4342e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 149, batch: 40/201] total loss per batch: 0.691
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1168, 0.0223, 0.0054, 0.7193, 0.0131, 0.1135, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 149, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.7588e-03, 1.8074e-02, 9.4135e-01, 1.5504e-05, 9.1741e-03, 1.5254e-02,
        8.3768e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 149, batch: 120/201] total loss per batch: 0.699
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5267, 0.0122, 0.0056, 0.0406, 0.2417, 0.1679, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.020

[Epoch: 149, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0070, 0.9550, 0.0053, 0.0139, 0.0032, 0.0061, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.011

[Epoch: 149, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.1785e-05, 7.6007e-01, 2.3971e-01, 3.8163e-05, 2.1166e-05, 1.7976e-05,
        6.5321e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 150, batch: 40/201] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1001, 0.0162, 0.0076, 0.7581, 0.0105, 0.0948, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 150, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.8860e-02, 2.0137e-02, 8.9764e-01, 1.4523e-05, 2.8574e-02, 1.6678e-02,
        1.8100e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.030

[Epoch: 150, batch: 120/201] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4829, 0.0121, 0.0044, 0.0454, 0.2339, 0.2156, 0.0057],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 150, batch: 160/201] total loss per batch: 0.693
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0064, 0.9605, 0.0063, 0.0144, 0.0029, 0.0044, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 150, batch: 200/201] total loss per batch: 0.676
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.1785e-05, 8.4208e-01, 1.5758e-01, 7.4123e-05, 3.4413e-05, 2.7076e-05,
        1.5992e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.960

[Epoch: 151, batch: 40/201] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1124, 0.0267, 0.0050, 0.7472, 0.0135, 0.0861, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 151, batch: 80/201] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([6.2860e-03, 6.4682e-03, 9.7041e-01, 9.3393e-06, 4.3379e-03, 7.7063e-03,
        4.7821e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.005

[Epoch: 151, batch: 120/201] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5583, 0.0074, 0.0036, 0.0305, 0.2479, 0.1475, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.016

[Epoch: 151, batch: 160/201] total loss per batch: 0.691
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0079, 0.9425, 0.0042, 0.0176, 0.0034, 0.0065, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.007

[Epoch: 151, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.6064e-05, 8.0028e-01, 1.9946e-01, 4.2125e-05, 2.7642e-05, 1.3339e-05,
        9.3156e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.957

[Epoch: 152, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0719, 0.0200, 0.0056, 0.7759, 0.0108, 0.1050, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 152, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2048e-02, 1.2081e-02, 9.4079e-01, 1.5034e-05, 1.2776e-02, 1.2910e-02,
        9.3788e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.018

[Epoch: 152, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4480, 0.0088, 0.0067, 0.0460, 0.2626, 0.2221, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 152, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0066, 0.9451, 0.0082, 0.0188, 0.0045, 0.0067, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.002

[Epoch: 152, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.8501e-05, 7.7850e-01, 2.2124e-01, 6.1039e-05, 3.4886e-05, 2.0956e-05,
        7.9851e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.956

[Epoch: 153, batch: 40/201] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1223, 0.0210, 0.0060, 0.7287, 0.0136, 0.0977, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 153, batch: 80/201] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0957e-02, 1.1023e-02, 9.4568e-01, 8.2138e-06, 1.0837e-02, 1.2324e-02,
        9.1698e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 153, batch: 120/201] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5067, 0.0084, 0.0047, 0.0361, 0.2551, 0.1843, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.031

[Epoch: 153, batch: 160/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0071, 0.9510, 0.0058, 0.0162, 0.0037, 0.0054, 0.0108],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 153, batch: 200/201] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6107e-05, 8.0654e-01, 1.9324e-01, 3.1710e-05, 2.0463e-05, 1.4103e-05,
        9.7912e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.952

[Epoch: 154, batch: 40/201] total loss per batch: 0.684
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0839, 0.0179, 0.0057, 0.7886, 0.0107, 0.0828, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 154, batch: 80/201] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3217e-02, 1.4126e-02, 9.3756e-01, 1.2014e-05, 1.2749e-02, 1.2400e-02,
        9.9378e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.016

[Epoch: 154, batch: 120/201] total loss per batch: 0.692
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5254, 0.0075, 0.0045, 0.0336, 0.2408, 0.1835, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 154, batch: 160/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0060, 0.9512, 0.0063, 0.0183, 0.0033, 0.0058, 0.0092],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.002

[Epoch: 154, batch: 200/201] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5688e-05, 8.0976e-01, 1.9010e-01, 2.4033e-05, 1.8039e-05, 8.4830e-06,
        4.5085e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 155, batch: 40/201] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1541, 0.0229, 0.0066, 0.6845, 0.0147, 0.1028, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 155, batch: 80/201] total loss per batch: 0.697
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2531e-02, 1.3813e-02, 9.3173e-01, 1.1834e-05, 1.3099e-02, 1.6468e-02,
        1.2348e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 155, batch: 120/201] total loss per batch: 0.693
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5168, 0.0058, 0.0038, 0.0441, 0.2572, 0.1678, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.023

[Epoch: 155, batch: 160/201] total loss per batch: 0.686
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0071, 0.9542, 0.0067, 0.0139, 0.0040, 0.0059, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 155, batch: 200/201] total loss per batch: 0.670
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.1626e-05, 7.8153e-01, 2.1824e-01, 4.2934e-05, 2.6178e-05, 1.3646e-05,
        9.1576e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 156, batch: 40/201] total loss per batch: 0.685
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0601, 0.0135, 0.0049, 0.8340, 0.0094, 0.0700, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 156, batch: 80/201] total loss per batch: 0.698
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3812e-02, 1.6021e-02, 9.2769e-01, 8.3347e-06, 1.4752e-02, 1.4947e-02,
        1.2774e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 156, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5310, 0.0113, 0.0048, 0.0246, 0.2518, 0.1717, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 156, batch: 160/201] total loss per batch: 0.687
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0071, 0.9457, 0.0062, 0.0200, 0.0036, 0.0052, 0.0123],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 156, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.2884e-05, 8.1105e-01, 1.8880e-01, 2.3409e-05, 2.6624e-05, 7.8656e-06,
        5.1252e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.961

[Epoch: 157, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1417, 0.0231, 0.0045, 0.7032, 0.0130, 0.1005, 0.0140],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 157, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0124e-02, 8.8878e-03, 9.5585e-01, 1.5641e-05, 9.5623e-03, 7.9124e-03,
        7.6524e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 157, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5071, 0.0048, 0.0044, 0.0502, 0.2292, 0.1986, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 157, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0070, 0.9545, 0.0076, 0.0129, 0.0032, 0.0065, 0.0083],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 157, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.9475e-05, 7.8460e-01, 2.1518e-01, 4.1288e-05, 2.8405e-05, 1.4899e-05,
        6.8711e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 158, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0827, 0.0180, 0.0049, 0.7664, 0.0117, 0.1048, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 158, batch: 80/201] total loss per batch: 0.702
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.9030e-02, 2.7041e-02, 8.7703e-01, 9.7324e-06, 3.0431e-02, 2.4589e-02,
        2.1870e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 158, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5339, 0.0099, 0.0051, 0.0289, 0.2706, 0.1470, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 158, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0071, 0.9475, 0.0060, 0.0194, 0.0034, 0.0058, 0.0107],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.019

[Epoch: 158, batch: 200/201] total loss per batch: 0.675
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.2574e-05, 7.9950e-01, 2.0040e-01, 1.4497e-05, 1.6120e-05, 4.7606e-06,
        3.2078e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 159, batch: 40/201] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1087, 0.0191, 0.0047, 0.7603, 0.0103, 0.0848, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 159, batch: 80/201] total loss per batch: 0.704
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([5.9328e-03, 4.3311e-03, 9.7554e-01, 6.9764e-06, 4.1902e-03, 5.4691e-03,
        4.5304e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 159, batch: 120/201] total loss per batch: 0.697
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4619, 0.0076, 0.0042, 0.0430, 0.2245, 0.2539, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.002

[Epoch: 159, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0084, 0.9510, 0.0082, 0.0133, 0.0025, 0.0062, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.006

[Epoch: 159, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([1.2139e-04, 8.0108e-01, 1.9860e-01, 7.0133e-05, 5.3952e-05, 1.9664e-05,
        6.2764e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 160, batch: 40/201] total loss per batch: 0.690
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1146, 0.0186, 0.0070, 0.7132, 0.0168, 0.1183, 0.0115],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 160, batch: 80/201] total loss per batch: 0.703
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.9524e-02, 2.7324e-02, 8.9620e-01, 1.2726e-05, 2.4195e-02, 1.6881e-02,
        1.5860e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.013

[Epoch: 160, batch: 120/201] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5763, 0.0122, 0.0055, 0.0379, 0.2457, 0.1164, 0.0061],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.011

[Epoch: 160, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0050, 0.9474, 0.0066, 0.0186, 0.0036, 0.0068, 0.0121],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.013

[Epoch: 160, batch: 200/201] total loss per batch: 0.674
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4514e-05, 8.3678e-01, 1.6306e-01, 2.2411e-05, 2.3333e-05, 6.9430e-06,
        6.7224e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.955

[Epoch: 161, batch: 40/201] total loss per batch: 0.689
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0839, 0.0195, 0.0056, 0.7940, 0.0106, 0.0763, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 161, batch: 80/201] total loss per batch: 0.702
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([7.5554e-03, 5.8799e-03, 9.6254e-01, 4.9073e-06, 7.4298e-03, 8.6514e-03,
        7.9336e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 161, batch: 120/201] total loss per batch: 0.698
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4018, 0.0094, 0.0036, 0.0267, 0.3147, 0.2382, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 161, batch: 160/201] total loss per batch: 0.690
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0090, 0.9502, 0.0074, 0.0147, 0.0034, 0.0063, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.009

[Epoch: 161, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.4318e-05, 7.6758e-01, 2.3215e-01, 4.5885e-05, 4.2985e-05, 1.5251e-05,
        7.1236e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.959

[Epoch: 162, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1397, 0.0259, 0.0069, 0.6775, 0.0143, 0.1175, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.018

[Epoch: 162, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2370e-02, 1.4386e-02, 9.3463e-01, 1.0527e-05, 1.2652e-02, 1.5814e-02,
        1.0142e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 162, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6539, 0.0086, 0.0050, 0.0502, 0.1447, 0.1318, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 162, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0054, 0.9459, 0.0050, 0.0210, 0.0035, 0.0050, 0.0141],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 162, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9339e-05, 8.0621e-01, 1.9360e-01, 2.0279e-05, 2.7175e-05, 9.3744e-06,
        8.6043e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 163, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0836, 0.0190, 0.0053, 0.7975, 0.0100, 0.0746, 0.0101],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 163, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3812e-02, 1.2220e-02, 9.3097e-01, 1.0564e-05, 1.5402e-02, 1.2414e-02,
        1.5172e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.009

[Epoch: 163, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.3852, 0.0080, 0.0038, 0.0294, 0.3589, 0.2094, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 163, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0068, 0.9547, 0.0083, 0.0127, 0.0039, 0.0066, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 163, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.1297e-05, 7.7102e-01, 2.2876e-01, 3.3934e-05, 4.8366e-05, 1.5473e-05,
        5.2226e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 164, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1260, 0.0175, 0.0053, 0.7338, 0.0111, 0.0893, 0.0170],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.024

[Epoch: 164, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0376e-02, 1.2621e-02, 9.4140e-01, 7.0176e-06, 1.0380e-02, 1.5244e-02,
        9.9677e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 164, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5718, 0.0131, 0.0043, 0.0377, 0.1803, 0.1871, 0.0059],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.003

[Epoch: 164, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0059, 0.9336, 0.0061, 0.0237, 0.0040, 0.0074, 0.0194],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.014

[Epoch: 164, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8052e-05, 8.1917e-01, 1.8061e-01, 3.2743e-05, 3.3438e-05, 1.2107e-05,
        8.0838e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 165, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0939, 0.0169, 0.0058, 0.7571, 0.0101, 0.1053, 0.0110],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.001

[Epoch: 165, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.1894e-03, 1.2340e-02, 9.4591e-01, 1.3045e-05, 1.3456e-02, 9.8917e-03,
        9.2012e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 165, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5333, 0.0055, 0.0034, 0.0351, 0.2589, 0.1587, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.006

[Epoch: 165, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0074, 0.9561, 0.0077, 0.0129, 0.0036, 0.0061, 0.0063],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.010

[Epoch: 165, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6559e-05, 7.7232e-01, 2.2749e-01, 4.3636e-05, 3.5444e-05, 1.1006e-05,
        5.1379e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 166, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0750, 0.0152, 0.0044, 0.8197, 0.0086, 0.0666, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.022

[Epoch: 166, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4933e-02, 1.9232e-02, 9.1796e-01, 9.1241e-06, 1.3443e-02, 2.0156e-02,
        1.4266e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 166, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4656, 0.0127, 0.0049, 0.0422, 0.2279, 0.2404, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 166, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0068, 0.9427, 0.0061, 0.0182, 0.0035, 0.0069, 0.0158],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 166, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7035e-05, 8.1407e-01, 1.8575e-01, 2.6433e-05, 2.1925e-05, 6.8965e-06,
        6.6681e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 167, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1382, 0.0252, 0.0053, 0.6673, 0.0158, 0.1347, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 167, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4713e-02, 1.3630e-02, 9.3589e-01, 1.0224e-05, 1.4043e-02, 1.1074e-02,
        1.0644e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 167, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5318, 0.0057, 0.0029, 0.0294, 0.2716, 0.1548, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.015

[Epoch: 167, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0075, 0.9563, 0.0076, 0.0111, 0.0032, 0.0060, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.009

[Epoch: 167, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4894e-05, 7.6349e-01, 2.3628e-01, 3.7826e-05, 3.1193e-05, 5.8119e-06,
        8.2108e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 168, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0752, 0.0212, 0.0048, 0.8148, 0.0094, 0.0647, 0.0099],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 168, batch: 80/201] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1848e-02, 1.4472e-02, 9.3705e-01, 1.2035e-05, 1.2750e-02, 1.1158e-02,
        1.2707e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.010

[Epoch: 168, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5462, 0.0068, 0.0049, 0.0381, 0.2381, 0.1592, 0.0066],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 168, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0056, 0.9526, 0.0054, 0.0170, 0.0031, 0.0067, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 168, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.0162e-05, 8.0218e-01, 1.9764e-01, 3.7892e-05, 2.8969e-05, 1.4515e-05,
        5.4685e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.962

[Epoch: 169, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1154, 0.0186, 0.0038, 0.7283, 0.0127, 0.1069, 0.0144],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 169, batch: 80/201] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([9.8722e-03, 1.2436e-02, 9.4012e-01, 8.6923e-06, 1.3106e-02, 1.4657e-02,
        9.7979e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 169, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4322, 0.0081, 0.0038, 0.0289, 0.2872, 0.2354, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.005

[Epoch: 169, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0056, 0.9497, 0.0081, 0.0165, 0.0036, 0.0076, 0.0089],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.016

[Epoch: 169, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([3.4947e-05, 8.1993e-01, 1.7994e-01, 1.9862e-05, 1.9363e-05, 4.4159e-06,
        4.4622e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.970

[Epoch: 170, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1302, 0.0260, 0.0076, 0.7225, 0.0144, 0.0860, 0.0133],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 170, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1094e-02, 1.0904e-02, 9.4425e-01, 9.1051e-06, 1.3511e-02, 8.6377e-03,
        1.1591e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.015

[Epoch: 170, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5594, 0.0060, 0.0044, 0.0411, 0.2483, 0.1351, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 170, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0055, 0.9507, 0.0063, 0.0156, 0.0031, 0.0074, 0.0114],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 170, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.9052e-05, 7.7733e-01, 2.2242e-01, 3.5420e-05, 3.8950e-05, 1.8172e-05,
        1.0572e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 171, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0807, 0.0133, 0.0036, 0.7982, 0.0086, 0.0829, 0.0128],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.020

[Epoch: 171, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3539e-02, 1.5128e-02, 9.2571e-01, 1.7200e-05, 1.4673e-02, 1.7080e-02,
        1.3854e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.012

[Epoch: 171, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5372, 0.0095, 0.0038, 0.0276, 0.2274, 0.1906, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.011

[Epoch: 171, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0055, 0.9422, 0.0077, 0.0226, 0.0036, 0.0077, 0.0106],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 171, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4189e-05, 7.9092e-01, 2.0894e-01, 2.4380e-05, 1.8570e-05, 6.9791e-06,
        2.7145e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 172, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1130, 0.0194, 0.0056, 0.7445, 0.0120, 0.0950, 0.0105],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 172, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4173e-02, 1.1807e-02, 9.3988e-01, 7.0698e-06, 1.0892e-02, 1.2543e-02,
        1.0703e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.008

[Epoch: 172, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4630, 0.0092, 0.0047, 0.0418, 0.2771, 0.1975, 0.0067],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.033

[Epoch: 172, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0051, 0.9581, 0.0071, 0.0127, 0.0030, 0.0060, 0.0081],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.005

[Epoch: 172, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.2700e-05, 8.2545e-01, 1.7438e-01, 2.3027e-05, 2.3143e-05, 1.0580e-05,
        5.5528e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.965

[Epoch: 173, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1150, 0.0184, 0.0041, 0.7392, 0.0119, 0.0934, 0.0180],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.013

[Epoch: 173, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4999e-02, 1.2682e-02, 9.3430e-01, 1.4198e-05, 1.2199e-02, 1.2031e-02,
        1.3776e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.005

[Epoch: 173, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5577, 0.0054, 0.0034, 0.0293, 0.2480, 0.1516, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.011

[Epoch: 173, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0055, 0.9488, 0.0074, 0.0163, 0.0035, 0.0064, 0.0120],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.009

[Epoch: 173, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.6625e-05, 7.8431e-01, 2.1551e-01, 3.0533e-05, 1.9515e-05, 8.0954e-06,
        5.6871e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.972

[Epoch: 174, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0920, 0.0130, 0.0056, 0.7904, 0.0109, 0.0784, 0.0098],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 174, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0446e-02, 1.6750e-02, 9.3011e-01, 5.3815e-06, 1.6329e-02, 1.2841e-02,
        1.3515e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 174, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4616, 0.0093, 0.0047, 0.0381, 0.2730, 0.2074, 0.0058],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.010

[Epoch: 174, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0056, 0.9433, 0.0087, 0.0184, 0.0057, 0.0079, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.012

[Epoch: 174, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.1680e-05, 8.0112e-01, 1.9868e-01, 3.2660e-05, 2.6894e-05, 1.1412e-05,
        7.2554e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 175, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1330, 0.0331, 0.0057, 0.6918, 0.0165, 0.1025, 0.0175],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 175, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4800e-02, 1.1086e-02, 9.3384e-01, 1.2854e-05, 1.3420e-02, 1.6039e-02,
        1.0798e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.001

[Epoch: 175, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5494, 0.0057, 0.0050, 0.0320, 0.2465, 0.1565, 0.0049],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.001

[Epoch: 175, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0050, 0.9591, 0.0048, 0.0127, 0.0026, 0.0055, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.001

[Epoch: 175, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4834e-05, 8.1052e-01, 1.8928e-01, 3.3231e-05, 2.4786e-05, 6.8141e-06,
        6.8285e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 176, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0861, 0.0119, 0.0034, 0.8157, 0.0080, 0.0664, 0.0085],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 0.001

[Epoch: 176, batch: 80/201] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1095e-02, 1.4719e-02, 9.2600e-01, 9.0511e-06, 1.7481e-02, 1.4805e-02,
        1.5887e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.012

[Epoch: 176, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4772, 0.0075, 0.0029, 0.0260, 0.2588, 0.2232, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.014

[Epoch: 176, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0067, 0.9380, 0.0070, 0.0216, 0.0049, 0.0082, 0.0135],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.013

[Epoch: 176, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.5493e-05, 7.3828e-01, 2.6147e-01, 2.2554e-05, 5.1432e-05, 6.7360e-06,
        1.1915e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.968

[Epoch: 177, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1275, 0.0266, 0.0066, 0.6713, 0.0152, 0.1297, 0.0232],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.021

[Epoch: 177, batch: 80/201] total loss per batch: 0.701
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5449e-02, 1.1883e-02, 9.4088e-01, 1.2362e-05, 7.5747e-03, 1.5853e-02,
        8.3469e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 177, batch: 120/201] total loss per batch: 0.696
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5741, 0.0061, 0.0048, 0.0372, 0.2044, 0.1685, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.013

[Epoch: 177, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0052, 0.9530, 0.0075, 0.0144, 0.0031, 0.0070, 0.0097],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 177, batch: 200/201] total loss per batch: 0.673
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.5783e-05, 8.3179e-01, 1.6799e-01, 5.3997e-05, 2.2818e-05, 1.3704e-05,
        6.3179e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 178, batch: 40/201] total loss per batch: 0.688
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0613, 0.0117, 0.0022, 0.8594, 0.0057, 0.0528, 0.0070],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.002

[Epoch: 178, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2307e-02, 1.8717e-02, 9.1900e-01, 1.0185e-05, 1.9055e-02, 1.5078e-02,
        1.5837e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.022

[Epoch: 178, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4698, 0.0056, 0.0027, 0.0327, 0.2920, 0.1931, 0.0040],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 178, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0072, 0.9425, 0.0066, 0.0187, 0.0043, 0.0068, 0.0139],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 0.003

[Epoch: 178, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.4854e-05, 8.1829e-01, 1.8152e-01, 3.9820e-05, 2.9204e-05, 8.0596e-06,
        6.0649e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.970

[Epoch: 179, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1706, 0.0284, 0.0041, 0.6080, 0.0144, 0.1521, 0.0224],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 179, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2557e-02, 1.2609e-02, 9.3955e-01, 1.0238e-05, 1.1944e-02, 1.2907e-02,
        1.0428e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 179, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4966, 0.0066, 0.0050, 0.0436, 0.2752, 0.1666, 0.0064],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.001

[Epoch: 179, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0042, 0.9578, 0.0070, 0.0131, 0.0028, 0.0066, 0.0086],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.015

[Epoch: 179, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([9.1051e-05, 7.6098e-01, 2.3872e-01, 5.1995e-05, 4.1650e-05, 1.4267e-05,
        9.2903e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.966

[Epoch: 180, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0635, 0.0155, 0.0035, 0.8437, 0.0097, 0.0570, 0.0071],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.010

[Epoch: 180, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5176e-02, 1.5030e-02, 9.2559e-01, 9.2294e-06, 1.6439e-02, 1.2205e-02,
        1.5549e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.017

[Epoch: 180, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5834, 0.0083, 0.0038, 0.0289, 0.1613, 0.2100, 0.0043],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.011

[Epoch: 180, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0055, 0.9497, 0.0050, 0.0168, 0.0043, 0.0063, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 180, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.8120e-05, 8.2939e-01, 1.7042e-01, 4.0438e-05, 1.4833e-05, 6.9214e-06,
        8.1085e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

[Epoch: 181, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1491, 0.0303, 0.0054, 0.6475, 0.0146, 0.1349, 0.0183],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 181, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4382e-02, 1.4004e-02, 9.3376e-01, 1.3973e-05, 1.1977e-02, 1.3539e-02,
        1.2319e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.011

[Epoch: 181, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4315, 0.0055, 0.0042, 0.0241, 0.3865, 0.1429, 0.0053],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 181, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0051, 0.9502, 0.0097, 0.0150, 0.0041, 0.0065, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.010

[Epoch: 181, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.6664e-05, 7.6606e-01, 2.3370e-01, 3.4256e-05, 3.6995e-05, 1.3000e-05,
        7.2300e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 182, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0680, 0.0114, 0.0050, 0.8436, 0.0076, 0.0556, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 182, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5580e-02, 1.3990e-02, 9.2439e-01, 7.0885e-06, 1.9408e-02, 1.5234e-02,
        1.1395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.027

[Epoch: 182, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5617, 0.0086, 0.0048, 0.0517, 0.1305, 0.2372, 0.0054],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 182, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0055, 0.9488, 0.0050, 0.0180, 0.0039, 0.0059, 0.0130],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.018

[Epoch: 182, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.9137e-05, 8.0101e-01, 1.9873e-01, 5.0022e-05, 3.4167e-05, 1.4354e-05,
        8.3710e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.955

[Epoch: 183, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1249, 0.0276, 0.0056, 0.7074, 0.0112, 0.1105, 0.0127],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 183, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0908e-02, 1.2890e-02, 9.4112e-01, 1.3622e-05, 1.4023e-02, 9.6641e-03,
        1.1384e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.009

[Epoch: 183, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4752, 0.0051, 0.0030, 0.0302, 0.3150, 0.1667, 0.0048],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.011

[Epoch: 183, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0044, 0.9594, 0.0065, 0.0139, 0.0031, 0.0059, 0.0068],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.019

[Epoch: 183, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.3536e-05, 8.1924e-01, 1.8061e-01, 2.6980e-05, 2.0773e-05, 5.7964e-06,
        4.7866e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.974

[Epoch: 184, batch: 40/201] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1021, 0.0162, 0.0049, 0.7855, 0.0125, 0.0686, 0.0104],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 184, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4997e-02, 1.4760e-02, 9.2749e-01, 7.7133e-06, 1.3535e-02, 1.4949e-02,
        1.4258e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.007

[Epoch: 184, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4964, 0.0056, 0.0044, 0.0346, 0.2603, 0.1937, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.019

[Epoch: 184, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0058, 0.9520, 0.0074, 0.0121, 0.0039, 0.0061, 0.0126],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 184, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([8.8021e-05, 7.6407e-01, 2.3570e-01, 3.1054e-05, 2.3563e-05, 1.1559e-05,
        7.1237e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.953

[Epoch: 185, batch: 40/201] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1150, 0.0156, 0.0043, 0.7443, 0.0087, 0.0987, 0.0134],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.015

[Epoch: 185, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3471e-02, 1.8271e-02, 9.2830e-01, 8.4485e-06, 1.4982e-02, 1.2142e-02,
        1.2828e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 185, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4953, 0.0056, 0.0042, 0.0386, 0.2732, 0.1779, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 185, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0049, 0.9535, 0.0069, 0.0151, 0.0044, 0.0062, 0.0090],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 185, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([4.5533e-05, 8.0985e-01, 1.8995e-01, 4.5326e-05, 3.2091e-05, 6.9162e-06,
        7.2984e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.973

[Epoch: 186, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0959, 0.0197, 0.0034, 0.7678, 0.0126, 0.0903, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 186, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3453e-02, 1.4299e-02, 9.3247e-01, 6.9677e-06, 1.3699e-02, 1.4419e-02,
        1.1656e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.004

[Epoch: 186, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5611, 0.0075, 0.0048, 0.0382, 0.2061, 0.1767, 0.0055],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 186, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0069, 0.9495, 0.0068, 0.0140, 0.0035, 0.0070, 0.0124],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 186, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.7995e-05, 7.9754e-01, 2.0221e-01, 6.2495e-05, 4.1780e-05, 1.4840e-05,
        5.3479e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.950

[Epoch: 187, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1188, 0.0193, 0.0050, 0.7366, 0.0102, 0.0915, 0.0186],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.017

[Epoch: 187, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1861e-02, 1.5451e-02, 9.3288e-01, 7.0937e-06, 1.4246e-02, 1.2194e-02,
        1.3359e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.019

[Epoch: 187, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4761, 0.0063, 0.0039, 0.0330, 0.2824, 0.1937, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.000

[Epoch: 187, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0043, 0.9472, 0.0056, 0.0190, 0.0053, 0.0074, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.022

[Epoch: 187, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.2845e-05, 8.1620e-01, 1.8356e-01, 3.1697e-05, 3.2660e-05, 4.8536e-06,
        1.0991e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 188, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1014, 0.0160, 0.0041, 0.7777, 0.0104, 0.0800, 0.0103],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 188, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.2066e-02, 1.5018e-02, 9.2563e-01, 9.0571e-06, 1.9062e-02, 1.4659e-02,
        1.3557e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 188, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5616, 0.0047, 0.0028, 0.0271, 0.2401, 0.1602, 0.0037],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.024

[Epoch: 188, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0066, 0.9499, 0.0091, 0.0137, 0.0035, 0.0054, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.006

[Epoch: 188, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.3911e-05, 7.8861e-01, 2.1109e-01, 7.0046e-05, 3.9337e-05, 1.8122e-05,
        1.0210e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.948

[Epoch: 189, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1082, 0.0284, 0.0055, 0.7011, 0.0166, 0.1228, 0.0174],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.005

[Epoch: 189, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0417e-02, 1.3471e-02, 9.4225e-01, 1.1548e-05, 1.1732e-02, 9.2395e-03,
        1.2880e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.014

[Epoch: 189, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4966, 0.0063, 0.0037, 0.0365, 0.2774, 0.1739, 0.0056],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 189, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0036, 0.9582, 0.0042, 0.0161, 0.0029, 0.0060, 0.0091],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.021

[Epoch: 189, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.4636e-05, 8.0159e-01, 1.9824e-01, 2.4370e-05, 2.5945e-05, 4.5386e-06,
        4.9102e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.970

[Epoch: 190, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0927, 0.0176, 0.0036, 0.7922, 0.0088, 0.0733, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.023

[Epoch: 190, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4640e-02, 1.3277e-02, 9.3106e-01, 5.0220e-06, 1.5098e-02, 1.3434e-02,
        1.2491e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.013

[Epoch: 190, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5168, 0.0084, 0.0032, 0.0283, 0.1912, 0.2469, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.029

[Epoch: 190, batch: 160/201] total loss per batch: 0.689
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0069, 0.9544, 0.0050, 0.0151, 0.0039, 0.0060, 0.0088],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.004

[Epoch: 190, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.0374e-05, 8.0746e-01, 1.9229e-01, 5.3732e-05, 3.4215e-05, 1.1234e-05,
        8.7460e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.969

[Epoch: 191, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0946, 0.0144, 0.0038, 0.7697, 0.0115, 0.0928, 0.0131],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 191, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0509e-02, 1.7111e-02, 9.3531e-01, 5.7093e-06, 1.2826e-02, 1.3031e-02,
        1.1211e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.006

[Epoch: 191, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5008, 0.0036, 0.0047, 0.0399, 0.3191, 0.1273, 0.0045],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.009

[Epoch: 191, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0043, 0.9552, 0.0065, 0.0142, 0.0030, 0.0058, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 191, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8678e-05, 7.9706e-01, 2.0280e-01, 2.8606e-05, 1.5435e-05, 4.0204e-06,
        4.1002e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.958

[Epoch: 192, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1307, 0.0269, 0.0062, 0.6870, 0.0123, 0.1218, 0.0151],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.019

[Epoch: 192, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4969e-02, 1.0290e-02, 9.3761e-01, 1.2339e-05, 1.4378e-02, 1.1077e-02,
        1.1668e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.002

[Epoch: 192, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5752, 0.0070, 0.0036, 0.0294, 0.1834, 0.1970, 0.0044],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.006

[Epoch: 192, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0053, 0.9508, 0.0061, 0.0198, 0.0043, 0.0057, 0.0080],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 192, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.8694e-05, 8.0070e-01, 1.9910e-01, 4.4165e-05, 3.7060e-05, 7.9155e-06,
        5.3616e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.975

[Epoch: 193, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0915, 0.0136, 0.0034, 0.8016, 0.0095, 0.0684, 0.0119],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.008

[Epoch: 193, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0937e-02, 1.6650e-02, 9.4077e-01, 7.8451e-06, 1.0156e-02, 1.2085e-02,
        9.3941e-03], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 193, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4430, 0.0058, 0.0051, 0.0377, 0.2614, 0.2405, 0.0065],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.002

[Epoch: 193, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0060, 0.9423, 0.0105, 0.0164, 0.0041, 0.0083, 0.0125],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.009

[Epoch: 193, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4905e-05, 7.8119e-01, 2.1860e-01, 3.3048e-05, 3.1037e-05, 4.4482e-06,
        7.0700e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.946

[Epoch: 194, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1022, 0.0252, 0.0057, 0.7421, 0.0139, 0.0967, 0.0142],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.011

[Epoch: 194, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5693e-02, 1.6275e-02, 9.3043e-01, 1.4002e-05, 1.3617e-02, 1.1349e-02,
        1.2625e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.002

[Epoch: 194, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5231, 0.0087, 0.0035, 0.0266, 0.2792, 0.1551, 0.0038],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.010

[Epoch: 194, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0049, 0.9519, 0.0049, 0.0177, 0.0042, 0.0051, 0.0112],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.012

[Epoch: 194, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.0310e-05, 7.9637e-01, 2.0343e-01, 4.7380e-05, 2.9201e-05, 4.5387e-06,
        5.1738e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.959

[Epoch: 195, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1110, 0.0227, 0.0037, 0.7487, 0.0111, 0.0915, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.012

[Epoch: 195, batch: 80/201] total loss per batch: 0.700
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3010e-02, 2.0628e-02, 9.2164e-01, 1.0653e-05, 1.3315e-02, 1.9505e-02,
        1.1893e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.003

[Epoch: 195, batch: 120/201] total loss per batch: 0.695
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5759, 0.0051, 0.0056, 0.0417, 0.1993, 0.1673, 0.0051],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.017

[Epoch: 195, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0051, 0.9469, 0.0075, 0.0193, 0.0030, 0.0065, 0.0118],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.003

[Epoch: 195, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.2549e-05, 7.8921e-01, 2.1050e-01, 4.7095e-05, 4.6906e-05, 7.6339e-06,
        1.1536e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.959

[Epoch: 196, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0725, 0.0176, 0.0040, 0.8248, 0.0098, 0.0616, 0.0096],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.014

[Epoch: 196, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.3767e-02, 1.1136e-02, 9.4104e-01, 1.0076e-05, 1.2391e-02, 9.9045e-03,
        1.1755e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.021

[Epoch: 196, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4732, 0.0100, 0.0036, 0.0301, 0.2941, 0.1851, 0.0039],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.000

[Epoch: 196, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0057, 0.9459, 0.0049, 0.0214, 0.0036, 0.0073, 0.0113],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.024

[Epoch: 196, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.5159e-05, 8.0224e-01, 1.9757e-01, 4.3442e-05, 2.2478e-05, 6.7770e-06,
        3.9788e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.978

[Epoch: 197, batch: 40/201] total loss per batch: 0.687
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1420, 0.0236, 0.0044, 0.6485, 0.0123, 0.1489, 0.0203],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.007

[Epoch: 197, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.5991e-02, 1.5370e-02, 9.2768e-01, 1.1170e-05, 1.2351e-02, 1.4115e-02,
        1.4484e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 197, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.5645, 0.0042, 0.0039, 0.0408, 0.2157, 0.1657, 0.0052],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.007

[Epoch: 197, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0048, 0.9560, 0.0078, 0.0124, 0.0036, 0.0061, 0.0095],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.009

[Epoch: 197, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.3727e-05, 8.0995e-01, 1.8975e-01, 6.1615e-05, 4.7578e-05, 5.5297e-06,
        1.2611e-04], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.951

[Epoch: 198, batch: 40/201] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0855, 0.0119, 0.0035, 0.8202, 0.0092, 0.0615, 0.0082],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.016

[Epoch: 198, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.1488e-02, 1.2148e-02, 9.3626e-01, 8.7919e-06, 1.2149e-02, 1.3553e-02,
        1.4395e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.020

[Epoch: 198, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4028, 0.0066, 0.0050, 0.0404, 0.2891, 0.2514, 0.0047],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.004

[Epoch: 198, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0054, 0.9316, 0.0069, 0.0287, 0.0041, 0.0074, 0.0159],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.015

[Epoch: 198, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([7.4989e-05, 7.7438e-01, 2.2544e-01, 2.4173e-05, 2.3277e-05, 7.9793e-06,
        4.7031e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.964

[Epoch: 199, batch: 40/201] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.1290, 0.0201, 0.0042, 0.7022, 0.0111, 0.1177, 0.0157],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 199, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.4926e-02, 1.4567e-02, 9.3179e-01, 1.0963e-05, 1.2343e-02, 1.3760e-02,
        1.2607e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 0.000

[Epoch: 199, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.6056, 0.0047, 0.0033, 0.0289, 0.2422, 0.1112, 0.0041],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 0.021

[Epoch: 199, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0058, 0.9582, 0.0063, 0.0133, 0.0030, 0.0056, 0.0078],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.011

[Epoch: 199, batch: 200/201] total loss per batch: 0.672
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([5.7587e-05, 8.3061e-01, 1.6921e-01, 4.2089e-05, 2.3299e-05, 6.0672e-06,
        5.8351e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.951

[Epoch: 200, batch: 40/201] total loss per batch: 0.686
Policy (actual, predicted): 3 3
Policy data: tensor([0.1067, 0.0200, 0.0033, 0.7533, 0.0100, 0.0933, 0.0133])
Policy pred: tensor([0.0972, 0.0129, 0.0042, 0.7883, 0.0106, 0.0759, 0.0109],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.014 -0.009

[Epoch: 200, batch: 80/201] total loss per batch: 0.699
Policy (actual, predicted): 2 2
Policy data: tensor([0.0133, 0.0133, 0.9333, 0.0000, 0.0133, 0.0133, 0.0133])
Policy pred: tensor([1.0644e-02, 1.4008e-02, 9.3660e-01, 6.8694e-06, 1.3242e-02, 1.3053e-02,
        1.2448e-02], grad_fn=<SelectBackward>)Value (actual, predicted): -0.006 -0.001

[Epoch: 200, batch: 120/201] total loss per batch: 0.694
Policy (actual, predicted): 0 0
Policy data: tensor([0.5167, 0.0033, 0.0033, 0.0333, 0.2533, 0.1867, 0.0033])
Policy pred: tensor([0.4351, 0.0046, 0.0042, 0.0325, 0.2354, 0.2836, 0.0046],
       grad_fn=<SelectBackward>)Value (actual, predicted): 0.015 -0.009

[Epoch: 200, batch: 160/201] total loss per batch: 0.688
Policy (actual, predicted): 1 1
Policy data: tensor([0.0033, 0.9533, 0.0067, 0.0167, 0.0033, 0.0067, 0.0100])
Policy pred: tensor([0.0046, 0.9490, 0.0075, 0.0150, 0.0034, 0.0076, 0.0129],
       grad_fn=<SelectBackward>)Value (actual, predicted): -0.029 -0.008

[Epoch: 200, batch: 200/201] total loss per batch: 0.671
Policy (actual, predicted): 1 1
Policy data: tensor([0.0000, 0.8000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000])
Policy pred: tensor([6.3350e-05, 7.5919e-01, 2.4065e-01, 3.3102e-05, 1.9285e-05, 5.5456e-06,
        3.8350e-05], grad_fn=<SelectBackward>)Value (actual, predicted): 0.962 0.967

